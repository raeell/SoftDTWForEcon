{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rnKo-COcSoR"
      },
      "source": [
        "# Setup installation and imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sHG1Fk9bxxT"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/google-research/soft-dtw-divergences.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKPEaTcBb8Wd"
      },
      "outputs": [],
      "source": [
        "%cd soft-dtw-divergences\n",
        "!make cython\n",
        "!python setup.py build\n",
        "!sudo python setup.py install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vglxTkZDb-2q"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CC4AAM9cE5g"
      },
      "outputs": [],
      "source": [
        "pip install tslearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJSniUqOgV3g"
      },
      "outputs": [],
      "source": [
        "from sdtw_div.numba_ops import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwuUBVyhb-mF"
      },
      "outputs": [],
      "source": [
        "%cd -"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YvGtxrFcvrs"
      },
      "source": [
        "# Callable metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KoXAnCEsFU4"
      },
      "outputs": [],
      "source": [
        "class SoftDTW():\n",
        "  def __init__(self, gamma):\n",
        "    super(SoftDTW, self).__init__()\n",
        "    self.gamma = gamma\n",
        "\n",
        "  def __call__(self, x, y):\n",
        "    return sdtw(x, y, gamma=self.gamma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MG6KhJ4TsHv4"
      },
      "outputs": [],
      "source": [
        "class SoftDTWValueAndGrad():\n",
        "  def __init__(self, gamma):\n",
        "    super(SoftDTWValueAndGrad, self).__init__()\n",
        "    self.gamma = gamma\n",
        "\n",
        "  def __call__(self, x, y):\n",
        "    return sdtw_value_and_grad(x, y, gamma=self.gamma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yj8RL1QMsJJk"
      },
      "outputs": [],
      "source": [
        "class SharpSoftDTW():\n",
        "  def __init__(self, gamma):\n",
        "    super(SharpSoftDTW, self).__init__()\n",
        "    self.gamma = gamma\n",
        "\n",
        "  def __call__(self, x, y):\n",
        "    return sharp_sdtw(x, y, gamma=self.gamma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UHfb85-sLIV"
      },
      "outputs": [],
      "source": [
        "class SharpSoftDTWValueAndGrad():\n",
        "  def __init__(self, gamma):\n",
        "    super(SharpSoftDTWValueAndGrad, self).__init__()\n",
        "    self.gamma = gamma\n",
        "\n",
        "  def __call__(self, x, y):\n",
        "    return sharp_sdtw_value_and_grad(x, y, gamma=self.gamma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4E6WymHcHNH"
      },
      "outputs": [],
      "source": [
        "class SoftDTWDiv():\n",
        "  def __init__(self, gamma):\n",
        "    super(SoftDTWDiv, self).__init__()\n",
        "    self.gamma = gamma\n",
        "\n",
        "  def __call__(self, x, y):\n",
        "    return sdtw_div(x, y, gamma=self.gamma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnXk0ao3cLs9"
      },
      "outputs": [],
      "source": [
        "class SoftDTWDivValueAndGrad():\n",
        "  def __init__(self, gamma):\n",
        "    super(SoftDTWDivValueAndGrad, self).__init__()\n",
        "    self.gamma = gamma\n",
        "\n",
        "  def __call__(self, x, y):\n",
        "    return sdtw_div_value_and_grad(x, y, gamma=self.gamma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fQTSIcQcNue"
      },
      "outputs": [],
      "source": [
        "class SharpSoftDTWDiv():\n",
        "  def __init__(self, gamma):\n",
        "    super(SharpSoftDTWDiv, self).__init__()\n",
        "    self.gamma = gamma\n",
        "\n",
        "  def __call__(self, x, y):\n",
        "    return sharp_sdtw_div(x, y, gamma=self.gamma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYnxjfG9f6X4"
      },
      "outputs": [],
      "source": [
        "class SharpSoftDTWDivValueAndGrad():\n",
        "  def __init__(self, gamma):\n",
        "    super(SharpSoftDTWDivValueAndGrad, self).__init__()\n",
        "    self.gamma = gamma\n",
        "\n",
        "  def __call__(self, x, y):\n",
        "    return sharp_sdtw_div_value_and_grad(x, y, gamma=self.gamma)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-YG5K3wdP_Z"
      },
      "source": [
        "# Setting up the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhMk-LJGc3wf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JguD6IeSc5Fy"
      },
      "outputs": [],
      "source": [
        "# Import the times series through Google drive\n",
        "# The time series are available at this link https://www.cs.ucr.edu/%7Eeamonn/time_series_data_2018/\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "data_location = '/content/drive/MyDrive/'\n",
        "data_dir = data_location + 'UCRArchive_2018'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzC7i4HDdB1O"
      },
      "outputs": [],
      "source": [
        "# Create training and test sets\n",
        "\n",
        "catgs = os.listdir(data_dir)\n",
        "# remove Missing_value_and_variable_length_datasets_adjusted from catgs\n",
        "catgs.remove('Missing_value_and_variable_length_datasets_adjusted')\n",
        "catgs_missing = os.listdir(data_dir + '/Missing_value_and_variable_length_datasets_adjusted')\n",
        "catgs_missing.remove('REAME.md')\n",
        "catgs_missing.remove('missing_value_and_variable_length_datasets_info.csv')\n",
        "trainings = []\n",
        "tests = []\n",
        "\n",
        "for folder in catgs:\n",
        "  file = pd.read_csv(data_dir + '/' + folder + '/' + folder + '_TRAIN.tsv', sep='\\t', header=None)\n",
        "  trainings.append(file)\n",
        "  file = pd.read_csv(data_dir + '/' + folder + '/' + folder + '_TEST.tsv', sep='\\t', header=None)\n",
        "  tests.append(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9huTJ2gRdPAJ"
      },
      "outputs": [],
      "source": [
        "idxs = []\n",
        "for i, cat in enumerate(catgs):\n",
        "  # if trainings[i].shape[1] > 100 and trainings[i].shape[0] > 100 and tests[i].shape[0] > 400: #sufficient data size and samples\n",
        "  print(i, cat)\n",
        "  idxs.append(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2Dp6Kkogdkf"
      },
      "source": [
        "# Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
        "import scipy as sp\n"
      ],
      "metadata": {
        "id": "opWnO-4sYvSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kbdw5WKf3jh"
      },
      "outputs": [],
      "source": [
        "# Missing values detection\n",
        "\n",
        "idxs = [116, 119, 99, 80, 22, 16]\n",
        "\n",
        "# Here we look at which time series have missing values\n",
        "\n",
        "for i in idxs:\n",
        "  if trainings[i].isna().values.any():\n",
        "      print(catgs[i] + \" has \" + str(trainings[i].isna().values.sum()) + \" missing values in trainings\")\n",
        "      trainings[i].interpolate(method='linear', inplace=True)\n",
        "  else:\n",
        "      print(catgs[i] + \" has no missing values in trainings\")\n",
        "\n",
        "  if tests[i].isna().values.any():\n",
        "      print(catgs[i] + \" has \" + str(tests[i].isna().values.sum()) + \" missing values in tests\")\n",
        "      tests[i].interpolate(method='linear', inplace=True)\n",
        "  else:\n",
        "      print(catgs[i] + \" has no missing values in tests\")\n",
        "  print(\"--------------------------------\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for outliers\n",
        "\n",
        "def fig_ax(figsize=(15, 5)):\n",
        "    return plt.subplots(figsize=figsize)\n",
        "\n",
        "def outlier_detection(data):\n",
        "  quantile_threshold_low, quantile_threshold_high = 0.01, 0.997\n",
        "\n",
        "  fig, ax = fig_ax()\n",
        "  _ = ax.hist(data, 20)\n",
        "\n",
        "  threshold_low, threshold_high = np.quantile(\n",
        "      data, [quantile_threshold_low, quantile_threshold_high]\n",
        "  )\n",
        "\n",
        "  _ = ax.axvline(threshold_low, ls=\"--\", color=\"k\")\n",
        "  _ = ax.axvline(threshold_high, ls=\"--\", color=\"k\")\n",
        "\n",
        "  fig, ax = fig_ax()\n",
        "  ax.plot(data, \"*-\")\n",
        "\n",
        "  outlier_mask = (data < threshold_low) | (\n",
        "      data > threshold_high\n",
        "  )\n",
        "\n",
        "  ax.plot(\n",
        "      data[outlier_mask],\n",
        "      \"*\",\n",
        "      label=\"Outliers\",\n",
        "  )\n",
        "\n",
        "  plt.legend()"
      ],
      "metadata": {
        "id": "OKVlAgZSMmIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outlier_detection(trainings[80][1])"
      ],
      "metadata": {
        "id": "UKOXhhMqZSNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUBXuTNhmQj3"
      },
      "source": [
        "# Time series averaging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSCiu0rynKf_"
      },
      "source": [
        "Experimental setup:\n",
        "- in the original paper: DTW, SDTW, SHARP, MEANCOST with Euclidean mean initialization and $\\gamma = 1$. Divergences with their \"biased counterpart\" as initialization and $\\gamma = 10$.\n",
        "- here: all discrepancies are computed with $\\gamma$ in $\\{0.01, 0.1, 1, 10, 100\\}$. We keep the same initializations as in the original paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMKtImCQo16m"
      },
      "outputs": [],
      "source": [
        "from scipy.optimize import minimize\n",
        "from tslearn.barycenters import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idxs = [116, 119, 99, 80, 22, 16]"
      ],
      "metadata": {
        "id": "J_tWOAjPsXdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxNNslF6k5-1"
      },
      "outputs": [],
      "source": [
        "# DTW barycenters\n",
        "# We use the DBA algorithm to compute the barycenters\n",
        "\n",
        "dtw_barycenters = []\n",
        "\n",
        "for i in idxs:\n",
        "  eucl_mean = np.mean(trainings_array[i][:10], axis=0) # we average on 10 samples from the dataset\n",
        "  print(\"DTW barycenters on the \" + catgs[i] + \" dataset\")\n",
        "  b = dtw_barycenter_averaging(trainings_array[i][:10], init_barycenter=eucl_mean, max_iter=200)\n",
        "  dtw_barycenters.append(b)\n",
        "  plt.plot(b, linewidth=2)\n",
        "  for j in range(10):\n",
        "    plt.plot(trainings_array[i][j], alpha=0.1, color='black', linewidth=2)\n",
        "  plt.title(\"DTW\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWv_rTzsucYb"
      },
      "outputs": [],
      "source": [
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0MuuWtYudnz"
      },
      "outputs": [],
      "source": [
        "with open('dtw_barycenters.pickle', 'wb') as fp:\n",
        "    pickle.dump(dtw_barycenters, fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOU5TEhjq9hy"
      },
      "outputs": [],
      "source": [
        "gammas = [0.01, 0.1, 1., 10., 100.]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T192ubCEqeU9"
      },
      "outputs": [],
      "source": [
        "# SoftDTW barycenters\n",
        "\n",
        "sdtw_barycenters = [[], [], [], [], []]\n",
        "\n",
        "for i in idxs:\n",
        "  eucl_mean = np.mean(trainings_array[i][:10], axis=0)\n",
        "  print(\"Soft-DTW barycenters on the \" + catgs[i] + \" dataset\")\n",
        "  for g, gamma in enumerate(gammas):\n",
        "    color = sns.color_palette(\"flare\")[g]\n",
        "    func = SoftDTWValueAndGrad(gamma)\n",
        "    b = barycenter(trainings_array[i][:10], X_init=eucl_mean, value_and_grad=func)\n",
        "    sdtw_barycenters[g].append(b)\n",
        "    plt.plot(b, color=color, linewidth=2, label=\"$\\gamma = $\" + str(gamma))\n",
        "    for j in range(10):\n",
        "      plt.plot(trainings_array[i][j], alpha=0.07, color='black', linewidth=2)\n",
        "    plt.legend(prop={'size': 30})\n",
        "    plt.title(\"Soft-DTW\")\n",
        "    plt.show()\n",
        "    # save figure\n",
        "    plt.savefig('/content/drive/MyDrive/plots time series project/' + 'sdtw_' + str(gamma) + '_' + catgs[i] + '.png')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('sdtw_barycenters.pickle', 'wb') as fp:\n",
        "    pickle.dump(sdtw_barycenters, fp)"
      ],
      "metadata": {
        "id": "5HBJngoUAkWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txIeuGo5r32d"
      },
      "outputs": [],
      "source": [
        "# Sharp barycenters\n",
        "\n",
        "sharp_barycenters = [[], [], [], [], []]\n",
        "\n",
        "for i in idxs:\n",
        "  eucl_mean = np.mean(trainings_array[i][:10], axis=0)\n",
        "  print(\"Sharp barycenters on the \" + catgs[i] + \" dataset\")\n",
        "  for g, gamma in enumerate(gammas):\n",
        "    color = sns.color_palette(\"flare\")[g]\n",
        "    func = SharpSoftDTWValueAndGrad(gamma)\n",
        "    b = barycenter(trainings_array[i][:10], X_init=eucl_mean, value_and_grad=func)\n",
        "    sharp_barycenters[g].append(b)\n",
        "    plt.plot(b, color=color, linewidth=2, label=\"$\\gamma = $\" + str(gamma))\n",
        "    for j in range(10):\n",
        "      plt.plot(trainings_array[i][j], alpha=0.1, color='black', linewidth=2)\n",
        "    plt.legend(prop={'size': 30})\n",
        "    plt.title(\"Sharp\")\n",
        "    plt.show()\n",
        "    # save figure\n",
        "    plt.savefig('/content/drive/MyDrive/plots time series project/' + 'sharp_' + str(gamma) + '_' + catgs[i] + '.png')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('sharp_barycenters.pickle', 'wb') as fp:\n",
        "    pickle.dump(sharp_barycenters, fp)"
      ],
      "metadata": {
        "id": "y8H7wvFRAok9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7oamzTwsith"
      },
      "outputs": [],
      "source": [
        "# Mean-cost barycenters\n",
        "\n",
        "mean_cost_barycenters = []\n",
        "\n",
        "for i in idxs:\n",
        "  eucl_mean = np.mean(trainings_array[i][:10], axis=0)\n",
        "  print(\"Mean-cost barycenters on the \" + catgs[i] + \" dataset\")\n",
        "  b = barycenter(trainings_array[i][:10], X_init=eucl_mean, value_and_grad=mean_cost_value_and_grad)\n",
        "  mean_cost_barycenters.append(b)\n",
        "  plt.plot(b, linewidth=2)\n",
        "  for j in range(10):\n",
        "    plt.plot(trainings_array[i][j], alpha=0.1, color='black', linewidth=2)\n",
        "  plt.title(\"Mean-cost\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('mean_cost_barycenters.pickle', 'wb') as fp:\n",
        "    pickle.dump(mean_cost_barycenters, fp)"
      ],
      "metadata": {
        "id": "bhafWmPg6t-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fpfkexzntkzu"
      },
      "outputs": [],
      "source": [
        "# Soft-DTW divergence\n",
        "\n",
        "sdtw_div_barycenters = [[], [], [], [], []]\n",
        "\n",
        "for idx, i in enumerate(idxs):\n",
        "  print(\"Soft-DTW divergence barycenters on the \" + catgs[i] + \" dataset\")\n",
        "  for g, gamma in enumerate(gammas):\n",
        "    color = sns.color_palette(\"flare\")[g]\n",
        "    func = SoftDTWDivValueAndGrad(gamma)\n",
        "    b = barycenter(trainings_array[i][:10], X_init=sdtw_barycenters[g][idx], value_and_grad=func)\n",
        "    sdtw_div_barycenters[g].append(b)\n",
        "    plt.plot(b, color=color, linewidth=2, label=\"$\\gamma = $\" + str(gamma))\n",
        "    for j in range(10):\n",
        "      plt.plot(trainings_array[i][j], alpha=0.1, color='black', linewidth=2)\n",
        "    plt.legend(prop={'size': 30})\n",
        "    plt.title(\"Soft-DTW divergence\")\n",
        "    plt.show()\n",
        "    # save figure\n",
        "    plt.savefig('/content/drive/MyDrive/plots time series project/' + 'sdtw_div_' + str(gamma) + '_' + catgs[i] + '.png')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('sdtw_div_barycenters.pickle', 'wb') as fp:\n",
        "    pickle.dump(sdtw_div_barycenters, fp)"
      ],
      "metadata": {
        "id": "wB5O9B6nAsEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KimIfx2Et4vk"
      },
      "outputs": [],
      "source": [
        "# Sharp divergence\n",
        "\n",
        "sharp_div_barycenters = [[], [], [], [], []]\n",
        "\n",
        "for idx, i in enumerate(idxs):\n",
        "  print(\"Sharp divergence barycenters on the \" + catgs[i] + \" dataset\")\n",
        "  for g, gamma in enumerate(gammas):\n",
        "    color = sns.color_palette(\"flare\")[g]\n",
        "    func = SharpSoftDTWDivValueAndGrad(gamma)\n",
        "    b = barycenter(trainings_array[i][:10], X_init=sharp_barycenters[g][idx], value_and_grad=func)\n",
        "    sharp_div_barycenters[g].append(b)\n",
        "    plt.plot(b, color=color, linewidth=2, label=\"$\\gamma = $\" + str(gamma))\n",
        "    for j in range(10):\n",
        "      plt.plot(trainings_array[i][j], alpha=0.1, color='black', linewidth=2)\n",
        "    plt.legend(prop={'size': 30})\n",
        "    plt.title(\"Sharp divergence\")\n",
        "    plt.show()\n",
        "    # save figure\n",
        "    plt.savefig('/content/drive/MyDrive/plots time series project/' + 'sharp_div_' + str(gamma) + '_' + catgs[i] + '.png')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('sharp_div_barycenters.pickle', 'wb') as fp:\n",
        "    pickle.dump(sharp_div_barycenters, fp)"
      ],
      "metadata": {
        "id": "KOvFdnL1AvG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xg0q1nuuDzD"
      },
      "outputs": [],
      "source": [
        "# Mean-cost divergence\n",
        "\n",
        "mean_cost_div_barycenters = []\n",
        "\n",
        "for idx, i in enumerate(idxs):\n",
        "  print(\"Mean-cost divergence barycenters on the \" + catgs[i] + \" dataset\")\n",
        "  b = barycenter(trainings_array[i][:10], X_init=mean_cost_barycenters[idx], value_and_grad=mean_cost_div_value_and_grad)\n",
        "  mean_cost_div_barycenters.append(b)\n",
        "  plt.plot(b, linewidth=2)\n",
        "  for j in range(10):\n",
        "    plt.plot(trainings_array[i][j], alpha=0.1, color='black', linewidth=2)\n",
        "  plt.title(\"Mean-cost divergence\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('mean_cost_div_barycenters.pickle', 'wb') as fp:\n",
        "    pickle.dump(mean_cost_div_barycenters, fp)"
      ],
      "metadata": {
        "id": "sr7DYbGWATIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quantitative results"
      ],
      "metadata": {
        "id": "4fS7HQJ_6kHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tslearn.metrics import dtw"
      ],
      "metadata": {
        "id": "aWB5J3uz70c2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, i in enumerate(idxs):\n",
        "  print(\"distances for dataset \" + catgs[i])\n",
        "  to_avg = trainings_array[i][:10]\n",
        "  dtw_dists = np.zeros((len(to_avg), 1))\n",
        "  sdtw_dists = np.zeros((len(to_avg), len(gammas)))\n",
        "  sharp_dists = np.zeros((len(to_avg), len(gammas)))\n",
        "  mean_cost_dists = np.zeros((len(to_avg), 1))\n",
        "  sdtw_div_dists = np.zeros((len(to_avg), len(gammas)))\n",
        "  sharp_div_dists = np.zeros((len(to_avg), len(gammas)))\n",
        "  mean_cost_div_dists = np.zeros((len(to_avg), 1))\n",
        "  for s_idx, s in enumerate(to_avg):\n",
        "    dtw_dists[s_idx] = dtw(dtw_barycenters[idx], s)\n",
        "    mean_cost_dists[s_idx] = dtw(mean_cost_barycenters[idx], s)\n",
        "    mean_cost_div_dists[s_idx] = dtw(mean_cost_div_barycenters[idx], s)\n",
        "    for g_idx, g in enumerate(gammas):\n",
        "      sdtw_dists[s_idx][g_idx] = dtw(sdtw_barycenters[g_idx][idx], s)\n",
        "      sharp_dists[s_idx][g_idx] = dtw(sharp_barycenters[g_idx][idx], s)\n",
        "      sdtw_div_dists[s_idx][g_idx] = dtw(sdtw_div_barycenters[g_idx][idx], s)\n",
        "      sharp_div_dists[s_idx][g_idx] = dtw(sharp_div_barycenters[g_idx][idx], s)\n",
        "  dtw_dist = np.mean(dtw_dists, axis=0)\n",
        "  sdtw_dist = np.mean(sdtw_dists, axis=0)\n",
        "  sharp_dist = np.mean(sharp_dists, axis=0)\n",
        "  mean_cost_dist = np.mean(mean_cost_dists, axis=0)\n",
        "  mean_cost_div_dist = np.mean(mean_cost_div_dists, axis=0)\n",
        "  sdtw_div_dist = np.mean(sdtw_div_dists, axis=0)\n",
        "  sharp_div_dist = np.mean(sharp_div_dists, axis=0)\n",
        "  print(\"dtw: \", (dtw_dist))\n",
        "  print(\"sdtw: \", (sdtw_dist))\n",
        "  print(\"sharp: \", (sharp_dist))\n",
        "  print(\"mean_cost: \", (mean_cost_dist))\n",
        "  print(\"mean_cost_div: \", (mean_cost_div_dist))\n",
        "  print(\"sdtw_div: \", (sdtw_div_dist))\n",
        "  print(\"sharp_div: \", (sharp_div_dist))\n",
        "  print(\"-------\")"
      ],
      "metadata": {
        "id": "UC2mnHYs6nfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_9FyKTuKJ3N"
      },
      "source": [
        "# Time series clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kv2REXPOKOGw"
      },
      "source": [
        "We apply the various discrepancies presented in the paper to a task unexamined by the authors: clustering time series with the K-means algorithm.\n",
        "\n",
        "Experimental setup:\n",
        "- downsample time series to length 40\n",
        "- number of clusters equal to the number of classes in the data\n",
        "- gammas from $1e-4$ to $100$\n",
        "- max_iter for K-means equal to 30, max_iter for barycenter computation equal to 100\n",
        "- random initialization\n",
        "\n",
        "Qualitative comparisons: smoothness of clusters learned\n",
        "\n",
        "Quantitative comparisons: DTW distance from the cluster center to the time series for each cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utilities"
      ],
      "metadata": {
        "id": "A96ggynOoCbN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UICcsPCkgXf5"
      },
      "outputs": [],
      "source": [
        "from tslearn.clustering import TimeSeriesKMeans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJ4DBvE-KNoU"
      },
      "outputs": [],
      "source": [
        "def make_data(i, samples=100, sz=40):\n",
        "  X = TimeSeriesResampler(sz=sz).fit_transform(trainings_array[i][:samples])\n",
        "  return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0wEYVzrf_5H"
      },
      "outputs": [],
      "source": [
        "def plot(X, y, n_clusters, km_object, sz=40, title=None, savefig=None):\n",
        "  for yi in range(n_clusters):\n",
        "    plt.subplot(n_clusters//2+1, 2, 1+yi)\n",
        "    for xx in X[y == yi]:\n",
        "      plt.plot(xx.squeeze(axis=-1), \"k-\", alpha=.2)\n",
        "    plt.plot(km_object.centroids[yi].ravel(), \"r-\")\n",
        "    plt.xlim(0, sz)\n",
        "    plt.ylim(-4, 4)\n",
        "    plt.title(\"Cluster %d\" % (yi + 1))\n",
        "  plt.tight_layout()\n",
        "  if title is not None:\n",
        "    plt.suptitle(title, y=1.05, size=16)\n",
        "  plt.show()\n",
        "  if savefig is not None:\n",
        "    plt.savefig(savefig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlJGlaFCgEJh"
      },
      "outputs": [],
      "source": [
        "# implement the K-means algorithm\n",
        "\n",
        "class KMeans():\n",
        "\n",
        "  def __init__(self, metric, n_clusters, init=None, max_iter_outer=100, max_iter_inner=30, tol=1e-5):\n",
        "\n",
        "    self.n_clusters = n_clusters\n",
        "    self.max_iter_outer = max_iter_outer\n",
        "    self.max_iter_inner = max_iter_inner\n",
        "    self.metric = metric\n",
        "    self.init = init\n",
        "    self.tol = tol\n",
        "\n",
        "  def fit(self, X_train):\n",
        "\n",
        "    if self.init is None: # Random initialization for the centroids\n",
        "      self.centroids = [X_train[np.random.choice(range(len(X_train)))] for _ in range(self.n_clusters)]\n",
        "    else:\n",
        "      self.centroids = self.init\n",
        "\n",
        "    # Iterate, adjusting centroids until inertia has converged or until passed max_iter\n",
        "    iteration = 0\n",
        "    prev_centroids = None\n",
        "    diff_inertias = np.inf\n",
        "    prev_inertia = np.inf\n",
        "    while diff_inertias > self.tol and iteration < self.max_iter_outer:\n",
        "      # Sort each datapoint, assigning to nearest centroid\n",
        "      sorted_points = [[self.centroids[i]] for i in range(self.n_clusters)] # each cluster contains at least its centroid\n",
        "      for x in X_train:\n",
        "        if self.metric == \"dtw_\":\n",
        "          dists = np.array([dtw(x, centroid) for centroid in self.centroids])\n",
        "        else:\n",
        "          dists = np.array([self.metric(x, centroid)[0] for centroid in self.centroids])\n",
        "        centroid_idx = np.argmin(dists)\n",
        "        sorted_points[centroid_idx].append(x)\n",
        "      # Update centroids\n",
        "      prev_centroids = self.centroids\n",
        "      if self.metric == \"dtw_\":\n",
        "        self.centroids = [dtw_barycenter_averaging(cluster, init_barycenter = np.mean(cluster, axis=0), max_iter=self.max_iter_inner) for cluster in sorted_points]\n",
        "      else:\n",
        "        self.centroids = [barycenter(cluster, X_init=np.mean(cluster, axis=0), value_and_grad=self.metric, max_iter=self.max_iter_inner) for cluster in sorted_points]\n",
        "      for i, centroid in enumerate(self.centroids):\n",
        "        if np.isnan(centroid).any():  # Catch any np.nans, resulting from an empty centroid\n",
        "          self.centroids[i] = prev_centroids[i]\n",
        "      # Compute mean inertia (distance to the nearest centroid)\n",
        "      inertia = 0\n",
        "      for i, centroid in enumerate(self.centroids):\n",
        "        inertia_cluster = 0\n",
        "        for x in sorted_points[i]:\n",
        "          if self.metric == \"dtw_\":\n",
        "            inertia_cluster += dtw(x, centroid)\n",
        "          else:\n",
        "            inertia_cluster += self.metric(x, centroid)[0]\n",
        "        inertia_cluster /= len(sorted_points[i])\n",
        "        inertia += inertia_cluster\n",
        "      inertia /= self.n_clusters\n",
        "      diff_inertias = np.abs(inertia - prev_inertia)\n",
        "      prev_inertia = inertia\n",
        "      iteration += 1\n",
        "\n",
        "  def evaluate(self, X):\n",
        "\n",
        "    centroids = []\n",
        "    centroid_idxs = []\n",
        "    for x in X:\n",
        "      if self.metric == \"dtw_\":\n",
        "        dists = np.array([dtw(x, centroid) for centroid in self.centroids])\n",
        "      else:\n",
        "        dists = np.array([self.metric(x, centroid)[0] for centroid in self.centroids])\n",
        "      centroid_idx = np.argmin(dists)\n",
        "      centroids.append(self.centroids[centroid_idx])\n",
        "      centroid_idxs.append(centroid_idx)\n",
        "    return centroids, centroid_idxs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experiments"
      ],
      "metadata": {
        "id": "qiVnRlFLoGQ_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Or2fScfGgCmP"
      },
      "outputs": [],
      "source": [
        "gammas = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idxs = [116, 119, 99, 80, 22, 16]"
      ],
      "metadata": {
        "id": "R_38zEipOvyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### DTW"
      ],
      "metadata": {
        "id": "nyd0dEYev2oV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "km_dtw = []\n",
        "\n",
        "for i in idxs:\n",
        "  print(\"KMeans for: \", catgs[i])\n",
        "  X = make_data(i)\n",
        "\n",
        "  metric = \"dtw_\"\n",
        "  nb_clusters = len(trainings[i][0].unique()) # number of classes in the dataset\n",
        "  km = KMeans(metric=metric, n_clusters=nb_clusters, max_iter_outer=30, max_iter_inner=100, init=None, tol=1e-5)\n",
        "  km.fit(X)\n",
        "  km_dtw.append(km)\n",
        "  centroids, centroid_idxs = km.evaluate(X)\n",
        "  plot(X, np.array(centroid_idxs), n_clusters=nb_clusters, km_object=km, title=\"DTW\", savefig='/content/drive/MyDrive/plots time series project/' + 'KM_DTW_' + catgs[i] + '.png')\n",
        "\n",
        "print(\"-------\")"
      ],
      "metadata": {
        "id": "ujhTfBeEv4ZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"DTW:\")\n",
        "km_dtw_dists = np.zeros((1, len(idxs)))\n",
        "\n",
        "for idx, i in enumerate(idxs):\n",
        "  print(\"DTW distances for: \" + catgs[i])\n",
        "  X = make_data(i)\n",
        "  centroids, centroid_idxs = km_dtw[0].evaluate(X)\n",
        "  dists = []\n",
        "  for c, centroid in enumerate(centroids):\n",
        "    xx = X[np.array(centroid_idxs) == c]\n",
        "    if len(xx) != 0:\n",
        "      dists.append(np.mean([dtw(x, centroid) for x in xx])) # mean along the samples axis\n",
        "  km_dtw_dists[0, idx] = np.mean(dists) # mean among the clusters"
      ],
      "metadata": {
        "id": "TcUeG7K4wVLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.round(km_dtw_dists, 2))"
      ],
      "metadata": {
        "id": "nIA98YKUwVLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SoftDTW"
      ],
      "metadata": {
        "id": "TU00wQf-oLN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "km_sdtw = [[] for _ in range(len(gammas))]\n",
        "\n",
        "for i in idxs:\n",
        "  print(\"KMeans for: \", catgs[i])\n",
        "  X = make_data(i)\n",
        "\n",
        "  for g, gamma in enumerate(gammas):\n",
        "    print(\"gamma: \", gamma)\n",
        "    metric = SoftDTWValueAndGrad(gamma=gamma)\n",
        "    nb_clusters = len(trainings[i][0].unique()) # number of classes in the dataset\n",
        "    km = KMeans(metric=metric, n_clusters=nb_clusters, max_iter_outer=30, max_iter_inner=100, init=None, tol=1e-5)\n",
        "    km.fit(X)\n",
        "    km_sdtw[g].append(km)\n",
        "    centroids, centroid_idxs = km.evaluate(X)\n",
        "    plot(X, np.array(centroid_idxs), n_clusters=nb_clusters, km_object=km, title=\"SDTW, gamma: \" + str(gamma), savefig='/content/drive/MyDrive/plots time series project/' + 'KM_SDTW_' + str(gamma) + '_' + catgs[i] + '.png')\n",
        "\n",
        "  print(\"-------\")"
      ],
      "metadata": {
        "id": "JRLkm1yyaUxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute mean DTW distance of time series to centroid for each cluster\n",
        "\n",
        "print(\"SoftDTW:\")\n",
        "\n",
        "km_sdtw_dists = np.zeros((len(gammas), len(idxs)))\n",
        "\n",
        "for idx, i in enumerate(idxs):\n",
        "  print(\"DTW distances for: \" + catgs[i])\n",
        "  X = make_data(i)\n",
        "  for g, gamma in enumerate(gammas):\n",
        "    centroids, centroid_idxs = km_sdtw[g][0].evaluate(X)\n",
        "    dists = []\n",
        "    for c, centroid in enumerate(centroids):\n",
        "      xx = X[np.array(centroid_idxs) == c]\n",
        "      if len(xx) != 0:\n",
        "        dists.append(np.mean([dtw(x, centroid) for x in xx])) # mean along the samples axis\n",
        "    km_sdtw_dists[g][idx] = np.mean(dists) # mean among the clusters"
      ],
      "metadata": {
        "id": "QJAeeU1viel4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for g in range(len(gammas)):\n",
        "  print(\"gamma: \", gammas[g])\n",
        "  print(np.round(km_sdtw_dists[g, :], 2))"
      ],
      "metadata": {
        "id": "G1Km9VuOmiun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Sharp"
      ],
      "metadata": {
        "id": "Z-FiOdkYoNMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "km_sharp = [[] for _ in range(len(gammas))]\n",
        "\n",
        "for i in idxs:\n",
        "  print(\"KMeans for: \", catgs[i])\n",
        "  X = make_data(i)\n",
        "\n",
        "  for g, gamma in enumerate(gammas):\n",
        "    print(\"gamma: \", gamma)\n",
        "    metric = SharpSoftDTWValueAndGrad(gamma=gamma)\n",
        "    nb_clusters = len(trainings[i][0].unique()) # number of classes in the dataset\n",
        "    km = KMeans(metric=metric, n_clusters=nb_clusters, max_iter_outer=30, max_iter_inner=100, init=None, tol=1e-5)\n",
        "    km.fit(X)\n",
        "    km_sharp[g].append(km)\n",
        "    centroids, centroid_idxs = km.evaluate(X)\n",
        "    plot(X, np.array(centroid_idxs), n_clusters=nb_clusters, km_object=km, title=\"Sharp, gamma: \" + str(gamma), savefig='/content/drive/MyDrive/plots time series project/' + 'KM_Sharp_' + str(gamma) + '_' + catgs[i] + '.png')\n",
        "\n",
        "  print(\"-------\")"
      ],
      "metadata": {
        "id": "v8Wk0QSiab_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sharp:\")\n",
        "km_sharp_dists = np.zeros((len(gammas), len(idxs)))\n",
        "\n",
        "for idx, i in enumerate(idxs):\n",
        "  print(\"DTW distances for: \" + catgs[i])\n",
        "  X = make_data(i)\n",
        "  for g, gamma in enumerate(gammas):\n",
        "    centroids, centroid_idxs = km_sharp[g][0].evaluate(X)\n",
        "    dists = []\n",
        "    for c, centroid in enumerate(centroids):\n",
        "      xx = X[np.array(centroid_idxs) == c]\n",
        "      if len(xx) != 0:\n",
        "        dists.append(np.mean([dtw(x, centroid) for x in xx])) # mean along the samples axis\n",
        "    km_sharp_dists[g][idx] = np.mean(dists) # mean among the clusters"
      ],
      "metadata": {
        "id": "bGHD-xIElDdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for g in range(len(gammas)):\n",
        "  print(\"gamma: \", gammas[g])\n",
        "  print(np.round(km_sharp_dists[g, :], 2))"
      ],
      "metadata": {
        "id": "GdBR_6p2mqfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Sharp div"
      ],
      "metadata": {
        "id": "cd_i8ZWioPDT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "km_sharp_div = [[] for _ in range(len(gammas))]\n",
        "\n",
        "for i in idxs:\n",
        "  print(\"KMeans for: \", catgs[i])\n",
        "  X = make_data(i)\n",
        "\n",
        "  for g, gamma in enumerate(gammas):\n",
        "    print(\"gamma: \", gamma)\n",
        "    metric = SharpSoftDTWDivValueAndGrad(gamma=gamma)\n",
        "    nb_clusters = len(trainings[i][0].unique()) # number of classes in the dataset\n",
        "    km = KMeans(metric=metric, n_clusters=nb_clusters, max_iter_outer=30, max_iter_inner=100, init=None, tol=1e-5)\n",
        "    km.fit(X)\n",
        "    km_sharp_div[g].append(km)\n",
        "    centroids, centroid_idxs = km.evaluate(X)\n",
        "    plot(X, np.array(centroid_idxs), n_clusters=nb_clusters, km_object=km, title=\"Sharp DIV, gamma: \" + str(gamma), savefig='/content/drive/MyDrive/plots time series project/' + 'KM_Sharp_Div_' + str(gamma) + '_' + catgs[i] + '.png')\n",
        "\n",
        "  print(\"-------\")"
      ],
      "metadata": {
        "id": "Z4IDeQw_bwS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sharp Div:\")\n",
        "km_sharp_div_dists = np.zeros((len(gammas), len(idxs)))\n",
        "\n",
        "for idx, i in enumerate(idxs):\n",
        "  print(\"DTW distances for: \" + catgs[i])\n",
        "  X = make_data(i)\n",
        "  for g, gamma in enumerate(gammas):\n",
        "    centroids, centroid_idxs = km_sharp_div[g][0].evaluate(X)\n",
        "    dists = []\n",
        "    for c, centroid in enumerate(centroids):\n",
        "      xx = X[np.array(centroid_idxs) == c]\n",
        "      if len(xx) != 0:\n",
        "        dists.append(np.mean([dtw(x, centroid) for x in xx])) # mean along the samples axis\n",
        "    km_sharp_div_dists[g][idx] = np.mean(dists) # mean among the clusters"
      ],
      "metadata": {
        "id": "yxMdN1pslG0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for g in range(len(gammas)):\n",
        "  print(\"gamma: \", gammas[g])\n",
        "  print(np.round(km_sharp_div_dists[g, :], 2))"
      ],
      "metadata": {
        "id": "tcticH72nMiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mean Cost"
      ],
      "metadata": {
        "id": "dkLnAOxaoQ4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "km_mean_cost = []\n",
        "\n",
        "for i in idxs:\n",
        "  print(\"KMeans for: \", catgs[i])\n",
        "  X = make_data(i)\n",
        "\n",
        "  metric = mean_cost_value_and_grad\n",
        "  nb_clusters = len(trainings[i][0].unique()) # number of classes in the dataset\n",
        "  km = KMeans(metric=metric, n_clusters=nb_clusters, max_iter_outer=30, max_iter_inner=100, init=None, tol=1e-5)\n",
        "  km.fit(X)\n",
        "  km_mean_cost.append(km)\n",
        "  centroids, centroid_idxs = km.evaluate(X)\n",
        "  plot(X, np.array(centroid_idxs), n_clusters=nb_clusters, km_object=km, title=\"Mean cost\", savefig='/content/drive/MyDrive/plots time series project/' + 'KM_Meancost_' + catgs[i] + '.png')\n",
        "\n",
        "  print(\"-------\")"
      ],
      "metadata": {
        "id": "Q7lB-nKKgPnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Mean Cost:\")\n",
        "km_meancost_dists = np.zeros((1, len(idxs)))\n",
        "\n",
        "for idx, i in enumerate(idxs):\n",
        "  print(\"DTW distances for: \" + catgs[i])\n",
        "  X = make_data(i)\n",
        "  centroids, centroid_idxs = km_mean_cost[0].evaluate(X)\n",
        "  dists = []\n",
        "  for c, centroid in enumerate(centroids):\n",
        "    xx = X[np.array(centroid_idxs) == c]\n",
        "    if len(xx) != 0:\n",
        "      dists.append(np.mean([dtw(x, centroid) for x in xx])) # mean along the samples axis\n",
        "  km_meancost_dists[0, idx] = np.mean(dists) # mean among the clusters"
      ],
      "metadata": {
        "id": "OD9fW_GfmW9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.round(km_meancost_dists, 2))"
      ],
      "metadata": {
        "id": "TRk9fnMOnQIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mean Cost Div"
      ],
      "metadata": {
        "id": "yNsX2FjHoTB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "km_mean_cost_div = []\n",
        "\n",
        "for i in idxs:\n",
        "  print(\"KMeans for: \", catgs[i])\n",
        "  X = make_data(i)\n",
        "\n",
        "  metric = mean_cost_div_value_and_grad\n",
        "  nb_clusters = len(trainings[i][0].unique()) # number of classes in the dataset\n",
        "  km = KMeans(metric=metric, n_clusters=nb_clusters, max_iter_outer=30, max_iter_inner=100, init=None, tol=1e-5)\n",
        "  km.fit(X)\n",
        "  km_mean_cost_div.append(km)\n",
        "  centroids, centroid_idxs = km.evaluate(X)\n",
        "  plot(X, np.array(centroid_idxs), n_clusters=nb_clusters, km_object=km, title=\"Mean cost DIV\", savefig='/content/drive/MyDrive/plots time series project/' + 'KM_Meancost_Div_' + catgs[i] + '.png')\n",
        "\n",
        "  print(\"-------\")"
      ],
      "metadata": {
        "id": "J8m5Q8TqgpdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Mean Cost Div:\")\n",
        "km_meancost_div_dists = np.zeros((1, len(idxs)))\n",
        "\n",
        "for idx, i in enumerate(idxs):\n",
        "  print(\"DTW distances for: \" + catgs[i])\n",
        "  X = make_data(i)\n",
        "  centroids, centroid_idxs = km_mean_cost_div[0].evaluate(X)\n",
        "  dists = []\n",
        "  for c, centroid in enumerate(centroids):\n",
        "    xx = X[np.array(centroid_idxs) == c]\n",
        "    if len(xx) != 0:\n",
        "      dists.append(np.mean([dtw(x, centroid) for x in xx])) # mean along the samples axis\n",
        "  km_meancost_div_dists[0, idx] = np.mean(dists) # mean among the clusters"
      ],
      "metadata": {
        "id": "awa3FfkZld9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.round(km_meancost_div_dists, 2))"
      ],
      "metadata": {
        "id": "BKErbJ6mnwkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SoftDTW Div"
      ],
      "metadata": {
        "id": "PIfY4NvToU8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "km_sdtw_div = [[] for _ in range(len(gammas))]\n",
        "\n",
        "for i in idxs:\n",
        "  print(\"KMeans for: \", catgs[i])\n",
        "  X = make_data(i)\n",
        "\n",
        "  for g, gamma in enumerate(gammas):\n",
        "    print(\"gamma: \", gamma)\n",
        "    metric = SoftDTWDivValueAndGrad(gamma=gamma)\n",
        "    nb_clusters = len(trainings[i][0].unique()) # number of classes in the dataset\n",
        "    km = KMeans(metric=metric, n_clusters=nb_clusters, max_iter_outer=30, max_iter_inner=100, init=None, tol=1e-5)\n",
        "    km.fit(X)\n",
        "    km_sdtw_div[g].append(km)\n",
        "    centroids, centroid_idxs = km.evaluate(X)\n",
        "    plot(X, np.array(centroid_idxs), n_clusters=nb_clusters, km_object=km, title=\"SDTW Div, gamma: \" + str(gamma), savefig='/content/drive/MyDrive/plots time series project/' + 'KM_SDTW_Div_' + str(gamma) + '_' + catgs[i] + '.png')\n",
        "\n",
        "  print(\"-------\")"
      ],
      "metadata": {
        "id": "fThhUt4xPJJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"SoftDTW Div:\")\n",
        "km_sdtw_div_dists = np.zeros((len(gammas), len(idxs)))\n",
        "\n",
        "for idx, i in enumerate(idxs):\n",
        "  print(\"DTW distances for: \" + catgs[i])\n",
        "  X = make_data(i)\n",
        "  for g, gamma in enumerate(gammas):\n",
        "    centroids, centroid_idxs = km_sdtw_div[g][0].evaluate(X)\n",
        "    dists = []\n",
        "    for c, centroid in enumerate(centroids):\n",
        "      xx = X[np.array(centroid_idxs) == c]\n",
        "      if len(xx) != 0:\n",
        "        dists.append(np.mean([dtw(x, centroid) for x in xx])) # mean along the samples axis\n",
        "    km_sdtw_div_dists[g][idx] = np.mean(dists) # mean among the clusters"
      ],
      "metadata": {
        "id": "NO0vzQpolYeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for g in range(len(gammas)):\n",
        "  print(\"gamma: \", gammas[g])\n",
        "  print(np.round(km_sdtw_div_dists[g, :], 2))"
      ],
      "metadata": {
        "id": "zuWf72lsbPYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Time series forecasting"
      ],
      "metadata": {
        "id": "qT8A1pKtKbvg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the Soft DTW divergence as a loss function for a neural network in order to study its performance in comparison to the Soft DTW and the Mean Squared Error.\n",
        "\n",
        "Experimental setup:\n",
        "- For each dataset, we train 3  groups of models:\n",
        "  - Soft DTW for $\\gamma\\in \\{0.01, 0.1, 1,10,100\\}$\n",
        "  - Soft DTW Div for $\\gamma\\in \\{0.01, 0.1, 1,10,100\\}$\n",
        "  - Mean Squared Error (only one model in this group)\n",
        "- We use 60% of the dataset to predict the next 40%\n",
        "\n",
        "\n",
        "Qualitatively, we plot the obtained predictions and compare them to the ground truth.\n",
        "\n",
        "Quantitavively, we compute the DTW distance and the MSE between the predictions and the ground truth."
      ],
      "metadata": {
        "id": "evKAaJClKksY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utilities"
      ],
      "metadata": {
        "id": "EPHqBxuKOnNH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build MLP for time series prediction.\n",
        "from tslearn.metrics import SoftDTWLossPyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "if torch.cuda.is_available():\n",
        " dev = \"cuda:0\"\n",
        "else:\n",
        " dev = \"cpu\"\n",
        "device = torch.device(dev)\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super(MLP, self).__init__()\n",
        "    self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "    self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    if not isinstance(x, torch.Tensor):\n",
        "            x = torch.Tensor(x)\n",
        "\n",
        "    batch_size = x.size(0)\n",
        "    x_reshaped = torch.reshape(x, (batch_size, -1))  # Manipulations to deal with time series format\n",
        "    output = F.sigmoid(self.fc1(x_reshaped))\n",
        "    output = self.fc2(output)\n",
        "    return torch.reshape(output, (batch_size, -1, 1))  # Manipulations to deal with time series format"
      ],
      "metadata": {
        "id": "dmAncmfMK2FP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_models(cat, trainings, hidden_size=300, epochs=150, batch_size=50, lr=1e-2, gammas=[1e-2, 1e-1, 1, 10, 100], max_norm=100.,divergence=True):\n",
        "\n",
        "  i = cat\n",
        "\n",
        "  X_train = np.array(trainings[i])[:, 1:]\n",
        "  input_size = int(X_train.shape[1]*0.6)\n",
        "  output_size = X_train.shape[1] - input_size\n",
        "\n",
        "  x = torch.Tensor(X_train[:, :input_size]).unsqueeze(-1)\n",
        "  x = (x - x.mean(dim=0))/x.std(dim=0)\n",
        "  y = torch.Tensor(X_train[:, input_size:]).unsqueeze(-1)\n",
        "  y = (y - y.mean(dim=0))/y.std(dim=0)\n",
        "\n",
        "  X_val = np.array(tests[i])[:, 1:]\n",
        "  x_val = torch.Tensor(X_val[:, :input_size]).unsqueeze(-1)\n",
        "  x_val = (x_val - x_val.mean(dim=0))/x_val.std(dim=0)\n",
        "  x_val=x_val.to(device)\n",
        "  y_val = torch.Tensor(X_val[:, input_size:]).unsqueeze(-1)\n",
        "  y_val = (y_val - y_val.mean(dim=0))/y_val.std(dim=0)\n",
        "  y_val=y_val.to(device)\n",
        "\n",
        "  models = []\n",
        "  for gamma in gammas:\n",
        "\n",
        "    model = MLP(input_size=input_size, hidden_size=hidden_size, output_size=output_size).to(device)\n",
        "    loss_fn = SoftDTWLossPyTorch(gamma=gamma, normalize=divergence).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # task: predict next values given the first 60% of values\n",
        "\n",
        "    losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      shuffled_idxs = torch.randperm(x.size(0))\n",
        "      for batch_idx in range(0, x.size(0), batch_size):\n",
        "      # select batch\n",
        "        idxs = shuffled_idxs[batch_idx:batch_idx + batch_size]\n",
        "        x_batch = x[idxs].to(device)\n",
        "        y_batch = y[idxs].to(device)\n",
        "        pred = model(x_batch)\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(pred, y_batch).mean()\n",
        "        losses.append(loss.detach().numpy())\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
        "        optimizer.step()\n",
        "      if epoch % 10 == 0:\n",
        "        # validation loss\n",
        "        pred = model(x_val)\n",
        "        val_loss = loss_fn(pred, y_val).mean()\n",
        "        print(\"Epoch: {}, Train loss: {}, Validation loss: {}\".format(epoch, loss, val_loss))\n",
        "\n",
        "    plt.plot(np.array(losses))\n",
        "    plt.title(\"gamma = {}\".format(gamma))\n",
        "    plt.show()\n",
        "\n",
        "    models.append(model)\n",
        "\n",
        "  # add model with MSE loss\n",
        "\n",
        "  model = MLP(input_size=input_size, hidden_size=hidden_size, output_size=output_size).to(device)\n",
        "  loss_fn = nn.MSELoss().to(device)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "  losses = []\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "      shuffled_idxs = torch.randperm(x.size(0))\n",
        "      for batch_idx in range(0, x.size(0), batch_size):\n",
        "      # select batch\n",
        "        idxs = shuffled_idxs[batch_idx:batch_idx + batch_size]\n",
        "        x_batch = x[idxs].to(device)\n",
        "        y_batch = y[idxs].to(device)\n",
        "        pred = model(x_batch)\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(pred, y_batch).mean()\n",
        "        losses.append(loss.detach().numpy())\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
        "        optimizer.step()\n",
        "      if epoch % 10 == 0:\n",
        "        print(\"Epoch: {}, Loss: {}\".format(epoch, loss))\n",
        "\n",
        "  plt.plot(np.array(losses))\n",
        "  plt.title(\"MSE\")\n",
        "  plt.show()\n",
        "\n",
        "  models.append(model)\n",
        "\n",
        "  return models"
      ],
      "metadata": {
        "id": "4we5XTf5O4or"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_models(models, cat, testings):\n",
        "  X_test = np.array(testings[cat])[:, 1:]\n",
        "  input_size = int(X_test.shape[1]*0.6)\n",
        "  res = []\n",
        "  for m in range(len(models)):\n",
        "    result = models[m](torch.Tensor(X_test[:, :input_size]).unsqueeze(-1)).detach().numpy().squeeze()\n",
        "    # result = result * X_test[:, input_size:].std(1).reshape(-1, 1) + X_test[:, input_size:].mean(1).reshape(-1, 1)\n",
        "    res.append(result)\n",
        "  return res"
      ],
      "metadata": {
        "id": "EP67Z1RtO7NT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "def plot_forecasts(res, cat, test,gammas):\n",
        "  input_size = int(np.array(test[cat])[:,1:].shape[1]*0.6)\n",
        "  output_size = test[cat].shape[1]- input_size\n",
        "\n",
        "  X_val = np.array(test[cat])[:, 1:]\n",
        "  X_val = (X_val - X_val.mean(axis=0))/X_val.std(axis=0)\n",
        "  gt = X_val\n",
        "  print(gt.shape)\n",
        "  print(test[cat].shape[0])\n",
        "  for i in range(0, 10):\n",
        "    for m in range(len(res)):\n",
        "      color = sns.color_palette(\"magma\")[m%6]\n",
        "      if m < len(res)-1:\n",
        "        plt.plot(np.arange(input_size + output_size - 1), gt[i],color='grey', label='Ground truth')\n",
        "        plt.plot(np.arange(input_size, input_size + output_size -1), res[m][i], color='red', label='$\\gamma$ = {}'.format(gammas[m]), alpha=0.6)\n",
        "        plt.plot(np.arange(input_size, input_size + output_size -1), res[-1][i],color='green', label='MSE', alpha=0.6)\n",
        "        plt.axvline(x = input_size, linestyle = 'dashed', color = 'k')\n",
        "        plt.title(\"{}\".format(gammas[m]))\n",
        "        plt.legend()\n",
        "        plt.grid()\n",
        "        plt.show()\n",
        "      else:\n",
        "        plt.plot(np.arange(input_size, input_size + output_size -1), res[m][i],color='green', label='MSE', alpha=0.6)\n",
        "        plt.axvline(x = input_size, linestyle = 'dashed', color = 'k')\n",
        "        plt.grid()\n",
        "\n",
        "        plt.title(\"{}\".format(i))\n",
        "        plt.legend()\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "DsZoIST0lP8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tslearn.metrics import soft_dtw, SoftDTWLossPyTorch, dtw\n",
        "\n",
        "\n",
        "def error(res, cat, test):\n",
        "  input_size = int(np.array(test[cat])[:,1:].shape[1]*0.6)\n",
        "  gt = np.array(test[cat])[:, 1:][np.newaxis, :, input_size:]\n",
        "  gt = (gt - gt.mean(axis=1))/gt.std(axis=1)\n",
        "  res = np.array(res)\n",
        "\n",
        "  # MSE\n",
        "  mse = np.mean((gt - res)**2, axis=2)\n",
        "  std_mse = np.std(mse, axis=1)\n",
        "  mse = np.mean(mse, axis=1)\n",
        "\n",
        "  # DTW\n",
        "  dtw_models = np.zeros((len(res), gt.shape[1]))\n",
        "  for m in range(len(res)):\n",
        "    for ts in range(gt.shape[1]):\n",
        "      dist = dtw(gt[0, ts], res[m][ts])\n",
        "      dtw_models[m][ts] = dist\n",
        "  std_dtw = np.std(dtw_models, axis=1)\n",
        "  dtws = np.mean(dtw_models, axis=1)\n",
        "  print(\"For \" + catgs[cat])\n",
        "  print(\"MSE: {} +- {}\".format(np.round(mse,2), np.round(std_mse,2)))\n",
        "  print(\"DTW: {} +- {}\".format(np.round(dtws,2), np.round(std_dtw,2)))"
      ],
      "metadata": {
        "id": "sKGATX4ToMlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiments"
      ],
      "metadata": {
        "id": "8OL7tpGCPiu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Indexes of the categories you wish to study\n",
        "idxs = [116, 119, 22, 16]\n",
        "\n",
        "#Values of gamma you wish to study\n",
        "gammas = [1,1000,10000]"
      ],
      "metadata": {
        "id": "h_rBjd_MPHxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Soft DTW Loss"
      ],
      "metadata": {
        "id": "eTkMdqy0PODf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "forecast_models=[]\n",
        "for i in idxs:\n",
        "  forecast_models.append(train_models(i, trainings, gammas=gammas,divergence=False))\n",
        "\n",
        "# This cell takes a while to run depending on the number of datasets that you train on"
      ],
      "metadata": {
        "id": "d0I15FZSPQhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results=[]\n",
        "for i,idx in enumerate(idxs):\n",
        "  results.append(eval_models(forecast_models[i],idx,tests))\n"
      ],
      "metadata": {
        "id": "Fphn1L_UPRIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(idxs)):\n",
        "  plot_forecasts(results[i], idxs[i], tests,gammas)\n",
        "  error(results[i], idxs[i], tests)"
      ],
      "metadata": {
        "id": "kDdSuGclpJEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Soft DTW Divergence Loss"
      ],
      "metadata": {
        "id": "Fb2u_YakPRkq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "forecast_models=[]\n",
        "for i in idxs:\n",
        "  forecast_models.append(train_models(i, trainings, gammas=gammas, divergence=True))"
      ],
      "metadata": {
        "id": "xbJ85nH3PV5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results=[]\n",
        "for i,idx in enumerate(idxs):\n",
        "  results.append(eval_models(forecast_models[i],idx,tests))"
      ],
      "metadata": {
        "id": "q2BFhh6XjovL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(idxs)):\n",
        "  plot_forecasts(results[i], idxs[i], tests,gammas)\n",
        "  error(results[i], idxs[i], tests)"
      ],
      "metadata": {
        "id": "n25Pt_YrpRXG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "-rnKo-COcSoR",
        "7YvGtxrFcvrs",
        "S-YG5K3wdP_Z",
        "y2Dp6Kkogdkf",
        "4fS7HQJ_6kHa",
        "nyd0dEYev2oV",
        "TU00wQf-oLN4",
        "dkLnAOxaoQ4G",
        "yNsX2FjHoTB9",
        "PIfY4NvToU8t",
        "qT8A1pKtKbvg"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}