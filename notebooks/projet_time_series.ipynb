{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vglxTkZDb-2q"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'matplotlib'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJSniUqOgV3g"
      },
      "outputs": [],
      "source": [
        "from sdtw_div.numba_ops import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MY_BUCKET = \"mon_nom_utilisateur_sspcloud\"\n",
        "CHEMIN_FICHIER = \"ensae-reproductibilite/data/raw/data.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YvGtxrFcvrs"
      },
      "source": [
        "# Callable metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5KoXAnCEsFU4"
      },
      "outputs": [],
      "source": [
        "class SoftDTW():\n",
        "  def __init__(self, gamma):\n",
        "    super(SoftDTW, self).__init__()\n",
        "    self.gamma = gamma\n",
        "\n",
        "  def __call__(self, x, y):\n",
        "    return sdtw(x, y, gamma=self.gamma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MG6KhJ4TsHv4"
      },
      "outputs": [],
      "source": [
        "class SoftDTWValueAndGrad():\n",
        "  def __init__(self, gamma):\n",
        "    super(SoftDTWValueAndGrad, self).__init__()\n",
        "    self.gamma = gamma\n",
        "\n",
        "  def __call__(self, x, y):\n",
        "    return sdtw_value_and_grad(x, y, gamma=self.gamma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "yj8RL1QMsJJk"
      },
      "outputs": [],
      "source": [
        "class SharpSoftDTW():\n",
        "  def __init__(self, gamma):\n",
        "    super(SharpSoftDTW, self).__init__()\n",
        "    self.gamma = gamma\n",
        "\n",
        "  def __call__(self, x, y):\n",
        "    return sharp_sdtw(x, y, gamma=self.gamma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6UHfb85-sLIV"
      },
      "outputs": [],
      "source": [
        "class SharpSoftDTWValueAndGrad():\n",
        "  def __init__(self, gamma):\n",
        "    super(SharpSoftDTWValueAndGrad, self).__init__()\n",
        "    self.gamma = gamma\n",
        "\n",
        "  def __call__(self, x, y):\n",
        "    return sharp_sdtw_value_and_grad(x, y, gamma=self.gamma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "I4E6WymHcHNH"
      },
      "outputs": [],
      "source": [
        "class SoftDTWDiv():\n",
        "  def __init__(self, gamma):\n",
        "    super(SoftDTWDiv, self).__init__()\n",
        "    self.gamma = gamma\n",
        "\n",
        "  def __call__(self, x, y):\n",
        "    return sdtw_div(x, y, gamma=self.gamma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NnXk0ao3cLs9"
      },
      "outputs": [],
      "source": [
        "class SoftDTWDivValueAndGrad():\n",
        "  def __init__(self, gamma):\n",
        "    super(SoftDTWDivValueAndGrad, self).__init__()\n",
        "    self.gamma = gamma\n",
        "\n",
        "  def __call__(self, x, y):\n",
        "    return sdtw_div_value_and_grad(x, y, gamma=self.gamma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8fQTSIcQcNue"
      },
      "outputs": [],
      "source": [
        "class SharpSoftDTWDiv():\n",
        "  def __init__(self, gamma):\n",
        "    super(SharpSoftDTWDiv, self).__init__()\n",
        "    self.gamma = gamma\n",
        "\n",
        "  def __call__(self, x, y):\n",
        "    return sharp_sdtw_div(x, y, gamma=self.gamma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "FYnxjfG9f6X4"
      },
      "outputs": [],
      "source": [
        "class SharpSoftDTWDivValueAndGrad():\n",
        "  def __init__(self, gamma):\n",
        "    super(SharpSoftDTWDivValueAndGrad, self).__init__()\n",
        "    self.gamma = gamma\n",
        "\n",
        "  def __call__(self, x, y):\n",
        "    return sharp_sdtw_div_value_and_grad(x, y, gamma=self.gamma)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-YG5K3wdP_Z"
      },
      "source": [
        "# Setting up the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "RhMk-LJGc3wf"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pandas'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "JguD6IeSc5Fy"
      },
      "outputs": [],
      "source": [
        "# Import the times series through Google drive\n",
        "# The time series are available at this link https://www.cs.ucr.edu/%7Eeamonn/time_series_data_2018/\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive/')\n",
        "\n",
        "# data_location = '/content/drive/MyDrive/'\n",
        "data_dir = '/home/onyxia/work/SoftDTWForEcon/UCRArchive_2018'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df = \u001b[43mpd\u001b[49m.read_csv(\u001b[33m\"\u001b[39m\u001b[33m../DS_ICA_CSV_FR/DS_ICA_data.csv\u001b[39m\u001b[33m\"\u001b[39m,sep=\u001b[33m\"\u001b[39m\u001b[33m;\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(df.columns)\n\u001b[32m      3\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33mTIME_PERIOD\u001b[39m\u001b[33m\"\u001b[39m] = pd.to_datetime(df[\u001b[33m\"\u001b[39m\u001b[33mTIME_PERIOD\u001b[39m\u001b[33m\"\u001b[39m], \u001b[38;5;28mformat\u001b[39m=\u001b[33m\"\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY-\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mNameError\u001b[39m: name 'pd' is not defined"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(\"../DS_ICA_CSV_FR/DS_ICA_data.csv\",sep=\";\", encoding=\"utf-8\")\n",
        "print(df.columns)\n",
        "df[\"TIME_PERIOD\"] = pd.to_datetime(df[\"TIME_PERIOD\"], format=\"%Y-%m\")\n",
        "colonne = df.columns[0]\n",
        "df_activity = df[(df[colonne] == \"L\") & (df[\"SEASONAL_ADJUST\"] == \"Y\")&(df[\"IDX_TYPE\"]==\"ICA_SERV\")].sort_values(by=\"TIME_PERIOD\", ascending=True)\n",
        "df_activity_2016 = df_activity[df_activity[\"TIME_PERIOD\"].dt.year == 2012]\n",
        "print(df_activity_2016)\n",
        "print(df_activity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'plt' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mplt\u001b[49m.figure(figsize=(\u001b[32m12\u001b[39m, \u001b[32m6\u001b[39m))  \u001b[38;5;66;03m# Définir la taille du graphique\u001b[39;00m\n\u001b[32m      2\u001b[39m plt.plot(df_activity[\u001b[33m\"\u001b[39m\u001b[33mTIME_PERIOD\u001b[39m\u001b[33m\"\u001b[39m], df_activity[\u001b[33m\"\u001b[39m\u001b[33mOBS_VALUE\u001b[39m\u001b[33m\"\u001b[39m],marker=\u001b[33m'\u001b[39m\u001b[33mo\u001b[39m\u001b[33m'\u001b[39m, label=\u001b[33m\"\u001b[39m\u001b[33mOBS_VALUE\u001b[39m\u001b[33m\"\u001b[39m)  \n\u001b[32m      3\u001b[39m plt.xlabel(\u001b[33m\"\u001b[39m\u001b[33mDate\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mNameError\u001b[39m: name 'plt' is not defined"
          ]
        }
      ],
      "source": [
        "plt.figure(figsize=(12, 6))  # Définir la taille du graphique\n",
        "plt.plot(df_activity[\"TIME_PERIOD\"], df_activity[\"OBS_VALUE\"],marker='o', label=\"OBS_VALUE\")  \n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"OBS_VALUE\")\n",
        "plt.title(\"Évolution de OBS_VALUE dans le temps\")\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_size = 20\n",
        "output_size = 5\n",
        "def create_time_series_window(values,input_size=20,output_size=5):\n",
        "        X=[]\n",
        "        y=[]\n",
        "        for i in range(len(values) - input_size - output_size):\n",
        "\n",
        "                X.append(values[i:i + input_size])  \n",
        "                y.append(values[i + input_size:i + input_size + output_size])  \n",
        "        return np.array(X), np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RzC7i4HDdB1O"
      },
      "outputs": [],
      "source": [
        "# Create training and test sets\n",
        "\n",
        "catgs = os.listdir(data_dir)\n",
        "# remove Missing_value_and_variable_length_datasets_adjusted from catgs\n",
        "catgs.remove('Missing_value_and_variable_length_datasets_adjusted')\n",
        "catgs_missing = os.listdir(data_dir + '/Missing_value_and_variable_length_datasets_adjusted')\n",
        "catgs_missing.remove('REAME.md')\n",
        "catgs_missing.remove('missing_value_and_variable_length_datasets_info.csv')\n",
        "trainings = []\n",
        "tests = []\n",
        "\n",
        "for folder in catgs:\n",
        "  file = pd.read_csv(data_dir + '/' + folder + '/' + folder + '_TRAIN.tsv', sep='\\t', header=None)\n",
        "  trainings.append(file)\n",
        "  file = pd.read_csv(data_dir + '/' + folder + '/' + folder + '_TEST.tsv', sep='\\t', header=None)\n",
        "  tests.append(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>141</th>\n",
              "      <th>142</th>\n",
              "      <th>143</th>\n",
              "      <th>144</th>\n",
              "      <th>145</th>\n",
              "      <th>146</th>\n",
              "      <th>147</th>\n",
              "      <th>148</th>\n",
              "      <th>149</th>\n",
              "      <th>150</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>-1.125013</td>\n",
              "      <td>-1.131338</td>\n",
              "      <td>-1.138288</td>\n",
              "      <td>-1.146687</td>\n",
              "      <td>-1.138639</td>\n",
              "      <td>-1.141431</td>\n",
              "      <td>-1.143691</td>\n",
              "      <td>-1.144379</td>\n",
              "      <td>-1.154912</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.205572</td>\n",
              "      <td>-1.281235</td>\n",
              "      <td>-1.323420</td>\n",
              "      <td>-1.345800</td>\n",
              "      <td>-1.344547</td>\n",
              "      <td>-1.301098</td>\n",
              "      <td>-1.265903</td>\n",
              "      <td>-1.212717</td>\n",
              "      <td>-1.206178</td>\n",
              "      <td>-1.218422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>-0.626956</td>\n",
              "      <td>-0.625919</td>\n",
              "      <td>-0.627538</td>\n",
              "      <td>-0.626326</td>\n",
              "      <td>-0.624085</td>\n",
              "      <td>-0.624708</td>\n",
              "      <td>-0.625006</td>\n",
              "      <td>-0.624175</td>\n",
              "      <td>-0.624018</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.620019</td>\n",
              "      <td>-0.621505</td>\n",
              "      <td>-0.621526</td>\n",
              "      <td>-0.624091</td>\n",
              "      <td>-0.623989</td>\n",
              "      <td>-0.624350</td>\n",
              "      <td>-0.624376</td>\n",
              "      <td>-0.619471</td>\n",
              "      <td>-0.612058</td>\n",
              "      <td>-0.606422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>-2.001163</td>\n",
              "      <td>-1.999575</td>\n",
              "      <td>-1.999537</td>\n",
              "      <td>-1.999196</td>\n",
              "      <td>-1.999004</td>\n",
              "      <td>-2.000315</td>\n",
              "      <td>-1.998425</td>\n",
              "      <td>-2.000679</td>\n",
              "      <td>-1.999995</td>\n",
              "      <td>...</td>\n",
              "      <td>0.312798</td>\n",
              "      <td>0.242217</td>\n",
              "      <td>0.145716</td>\n",
              "      <td>0.014012</td>\n",
              "      <td>-0.151780</td>\n",
              "      <td>-0.333427</td>\n",
              "      <td>-0.577435</td>\n",
              "      <td>-0.812720</td>\n",
              "      <td>-1.071147</td>\n",
              "      <td>-1.323383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>-1.004587</td>\n",
              "      <td>-0.999843</td>\n",
              "      <td>-0.995250</td>\n",
              "      <td>-0.992019</td>\n",
              "      <td>-0.991200</td>\n",
              "      <td>-0.987556</td>\n",
              "      <td>-0.996473</td>\n",
              "      <td>-0.996878</td>\n",
              "      <td>-0.998673</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.080245</td>\n",
              "      <td>-1.060166</td>\n",
              "      <td>-1.044471</td>\n",
              "      <td>-1.039528</td>\n",
              "      <td>-1.040693</td>\n",
              "      <td>-1.044162</td>\n",
              "      <td>-1.044058</td>\n",
              "      <td>-1.044916</td>\n",
              "      <td>-1.044226</td>\n",
              "      <td>-1.043262</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.742625</td>\n",
              "      <td>-0.743770</td>\n",
              "      <td>-0.743900</td>\n",
              "      <td>-0.744873</td>\n",
              "      <td>-0.744745</td>\n",
              "      <td>-0.745364</td>\n",
              "      <td>-0.747078</td>\n",
              "      <td>-0.746615</td>\n",
              "      <td>-0.746705</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.699312</td>\n",
              "      <td>-0.696897</td>\n",
              "      <td>-0.694059</td>\n",
              "      <td>-0.689150</td>\n",
              "      <td>-0.687534</td>\n",
              "      <td>-0.682798</td>\n",
              "      <td>-0.682153</td>\n",
              "      <td>-0.681030</td>\n",
              "      <td>-0.670519</td>\n",
              "      <td>-0.657403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>2</td>\n",
              "      <td>-0.580006</td>\n",
              "      <td>-0.583332</td>\n",
              "      <td>-0.586108</td>\n",
              "      <td>-0.589118</td>\n",
              "      <td>-0.591951</td>\n",
              "      <td>-0.599196</td>\n",
              "      <td>-0.609292</td>\n",
              "      <td>-0.618504</td>\n",
              "      <td>-0.627164</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.536779</td>\n",
              "      <td>-0.537708</td>\n",
              "      <td>-0.537554</td>\n",
              "      <td>-0.538319</td>\n",
              "      <td>-0.538915</td>\n",
              "      <td>-0.541542</td>\n",
              "      <td>-0.545838</td>\n",
              "      <td>-0.546621</td>\n",
              "      <td>-0.548831</td>\n",
              "      <td>-0.553552</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.728153</td>\n",
              "      <td>-0.730242</td>\n",
              "      <td>-0.733560</td>\n",
              "      <td>-0.734188</td>\n",
              "      <td>-0.734331</td>\n",
              "      <td>-0.734660</td>\n",
              "      <td>-0.733856</td>\n",
              "      <td>-0.733397</td>\n",
              "      <td>-0.732929</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.788042</td>\n",
              "      <td>-0.768710</td>\n",
              "      <td>-0.748156</td>\n",
              "      <td>-0.728561</td>\n",
              "      <td>-0.711126</td>\n",
              "      <td>-0.699604</td>\n",
              "      <td>-0.689584</td>\n",
              "      <td>-0.687476</td>\n",
              "      <td>-0.686448</td>\n",
              "      <td>-0.690183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>2</td>\n",
              "      <td>-0.738012</td>\n",
              "      <td>-0.736301</td>\n",
              "      <td>-0.731226</td>\n",
              "      <td>-0.728455</td>\n",
              "      <td>-0.728883</td>\n",
              "      <td>-0.727372</td>\n",
              "      <td>-0.724525</td>\n",
              "      <td>-0.720916</td>\n",
              "      <td>-0.719828</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.617504</td>\n",
              "      <td>-0.613856</td>\n",
              "      <td>-0.611590</td>\n",
              "      <td>-0.609785</td>\n",
              "      <td>-0.609814</td>\n",
              "      <td>-0.608848</td>\n",
              "      <td>-0.610023</td>\n",
              "      <td>-0.609646</td>\n",
              "      <td>-0.608616</td>\n",
              "      <td>-0.612177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>2</td>\n",
              "      <td>-1.265111</td>\n",
              "      <td>-1.256093</td>\n",
              "      <td>-1.259421</td>\n",
              "      <td>-1.256351</td>\n",
              "      <td>-1.253265</td>\n",
              "      <td>-1.260103</td>\n",
              "      <td>-1.265063</td>\n",
              "      <td>-1.256396</td>\n",
              "      <td>-1.246350</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.192413</td>\n",
              "      <td>-1.193333</td>\n",
              "      <td>-1.195697</td>\n",
              "      <td>-1.189598</td>\n",
              "      <td>-1.177099</td>\n",
              "      <td>-1.188014</td>\n",
              "      <td>-1.189629</td>\n",
              "      <td>-1.199970</td>\n",
              "      <td>-1.193374</td>\n",
              "      <td>-1.192835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>1</td>\n",
              "      <td>-1.427205</td>\n",
              "      <td>-1.408303</td>\n",
              "      <td>-1.347118</td>\n",
              "      <td>-1.291666</td>\n",
              "      <td>-1.266331</td>\n",
              "      <td>-1.264420</td>\n",
              "      <td>-1.271496</td>\n",
              "      <td>-1.275212</td>\n",
              "      <td>-1.278654</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.028170</td>\n",
              "      <td>-1.032759</td>\n",
              "      <td>-1.039843</td>\n",
              "      <td>-1.046789</td>\n",
              "      <td>-1.054712</td>\n",
              "      <td>-1.065305</td>\n",
              "      <td>-1.082951</td>\n",
              "      <td>-1.103461</td>\n",
              "      <td>-1.153119</td>\n",
              "      <td>-1.222043</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>150 rows × 151 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     0         1         2         3         4         5         6    \\\n",
              "0      1 -1.125013 -1.131338 -1.138288 -1.146687 -1.138639 -1.141431   \n",
              "1      2 -0.626956 -0.625919 -0.627538 -0.626326 -0.624085 -0.624708   \n",
              "2      2 -2.001163 -1.999575 -1.999537 -1.999196 -1.999004 -2.000315   \n",
              "3      1 -1.004587 -0.999843 -0.995250 -0.992019 -0.991200 -0.987556   \n",
              "4      1 -0.742625 -0.743770 -0.743900 -0.744873 -0.744745 -0.745364   \n",
              "..   ...       ...       ...       ...       ...       ...       ...   \n",
              "145    2 -0.580006 -0.583332 -0.586108 -0.589118 -0.591951 -0.599196   \n",
              "146    1 -0.728153 -0.730242 -0.733560 -0.734188 -0.734331 -0.734660   \n",
              "147    2 -0.738012 -0.736301 -0.731226 -0.728455 -0.728883 -0.727372   \n",
              "148    2 -1.265111 -1.256093 -1.259421 -1.256351 -1.253265 -1.260103   \n",
              "149    1 -1.427205 -1.408303 -1.347118 -1.291666 -1.266331 -1.264420   \n",
              "\n",
              "          7         8         9    ...       141       142       143  \\\n",
              "0   -1.143691 -1.144379 -1.154912  ... -1.205572 -1.281235 -1.323420   \n",
              "1   -0.625006 -0.624175 -0.624018  ... -0.620019 -0.621505 -0.621526   \n",
              "2   -1.998425 -2.000679 -1.999995  ...  0.312798  0.242217  0.145716   \n",
              "3   -0.996473 -0.996878 -0.998673  ... -1.080245 -1.060166 -1.044471   \n",
              "4   -0.747078 -0.746615 -0.746705  ... -0.699312 -0.696897 -0.694059   \n",
              "..        ...       ...       ...  ...       ...       ...       ...   \n",
              "145 -0.609292 -0.618504 -0.627164  ... -0.536779 -0.537708 -0.537554   \n",
              "146 -0.733856 -0.733397 -0.732929  ... -0.788042 -0.768710 -0.748156   \n",
              "147 -0.724525 -0.720916 -0.719828  ... -0.617504 -0.613856 -0.611590   \n",
              "148 -1.265063 -1.256396 -1.246350  ... -1.192413 -1.193333 -1.195697   \n",
              "149 -1.271496 -1.275212 -1.278654  ... -1.028170 -1.032759 -1.039843   \n",
              "\n",
              "          144       145       146       147       148       149       150  \n",
              "0   -1.345800 -1.344547 -1.301098 -1.265903 -1.212717 -1.206178 -1.218422  \n",
              "1   -0.624091 -0.623989 -0.624350 -0.624376 -0.619471 -0.612058 -0.606422  \n",
              "2    0.014012 -0.151780 -0.333427 -0.577435 -0.812720 -1.071147 -1.323383  \n",
              "3   -1.039528 -1.040693 -1.044162 -1.044058 -1.044916 -1.044226 -1.043262  \n",
              "4   -0.689150 -0.687534 -0.682798 -0.682153 -0.681030 -0.670519 -0.657403  \n",
              "..        ...       ...       ...       ...       ...       ...       ...  \n",
              "145 -0.538319 -0.538915 -0.541542 -0.545838 -0.546621 -0.548831 -0.553552  \n",
              "146 -0.728561 -0.711126 -0.699604 -0.689584 -0.687476 -0.686448 -0.690183  \n",
              "147 -0.609785 -0.609814 -0.608848 -0.610023 -0.609646 -0.608616 -0.612177  \n",
              "148 -1.189598 -1.177099 -1.188014 -1.189629 -1.199970 -1.193374 -1.192835  \n",
              "149 -1.046789 -1.054712 -1.065305 -1.082951 -1.103461 -1.153119 -1.222043  \n",
              "\n",
              "[150 rows x 151 columns]"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tests[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>141</th>\n",
              "      <th>142</th>\n",
              "      <th>143</th>\n",
              "      <th>144</th>\n",
              "      <th>145</th>\n",
              "      <th>146</th>\n",
              "      <th>147</th>\n",
              "      <th>148</th>\n",
              "      <th>149</th>\n",
              "      <th>150</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>-0.647885</td>\n",
              "      <td>-0.641992</td>\n",
              "      <td>-0.638186</td>\n",
              "      <td>-0.638259</td>\n",
              "      <td>-0.638345</td>\n",
              "      <td>-0.638697</td>\n",
              "      <td>-0.643049</td>\n",
              "      <td>-0.643768</td>\n",
              "      <td>-0.645050</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.639264</td>\n",
              "      <td>-0.639716</td>\n",
              "      <td>-0.639735</td>\n",
              "      <td>-0.640184</td>\n",
              "      <td>-0.639235</td>\n",
              "      <td>-0.639395</td>\n",
              "      <td>-0.640231</td>\n",
              "      <td>-0.640429</td>\n",
              "      <td>-0.638666</td>\n",
              "      <td>-0.638657</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>-0.644427</td>\n",
              "      <td>-0.645401</td>\n",
              "      <td>-0.647055</td>\n",
              "      <td>-0.647492</td>\n",
              "      <td>-0.646910</td>\n",
              "      <td>-0.643884</td>\n",
              "      <td>-0.639731</td>\n",
              "      <td>-0.638094</td>\n",
              "      <td>-0.635297</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.641140</td>\n",
              "      <td>-0.641426</td>\n",
              "      <td>-0.639267</td>\n",
              "      <td>-0.637797</td>\n",
              "      <td>-0.637680</td>\n",
              "      <td>-0.635260</td>\n",
              "      <td>-0.635490</td>\n",
              "      <td>-0.634934</td>\n",
              "      <td>-0.634497</td>\n",
              "      <td>-0.631596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.778353</td>\n",
              "      <td>-0.778279</td>\n",
              "      <td>-0.777151</td>\n",
              "      <td>-0.777684</td>\n",
              "      <td>-0.775900</td>\n",
              "      <td>-0.772421</td>\n",
              "      <td>-0.765464</td>\n",
              "      <td>-0.762275</td>\n",
              "      <td>-0.763752</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.722055</td>\n",
              "      <td>-0.718712</td>\n",
              "      <td>-0.713534</td>\n",
              "      <td>-0.710021</td>\n",
              "      <td>-0.704126</td>\n",
              "      <td>-0.703263</td>\n",
              "      <td>-0.703393</td>\n",
              "      <td>-0.704196</td>\n",
              "      <td>-0.707605</td>\n",
              "      <td>-0.707120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.750060</td>\n",
              "      <td>-0.748103</td>\n",
              "      <td>-0.746164</td>\n",
              "      <td>-0.745926</td>\n",
              "      <td>-0.743767</td>\n",
              "      <td>-0.743805</td>\n",
              "      <td>-0.745213</td>\n",
              "      <td>-0.745082</td>\n",
              "      <td>-0.745727</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.721667</td>\n",
              "      <td>-0.724661</td>\n",
              "      <td>-0.729229</td>\n",
              "      <td>-0.728940</td>\n",
              "      <td>-0.727834</td>\n",
              "      <td>-0.728244</td>\n",
              "      <td>-0.726453</td>\n",
              "      <td>-0.725517</td>\n",
              "      <td>-0.725191</td>\n",
              "      <td>-0.724679</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>-0.599539</td>\n",
              "      <td>-0.597422</td>\n",
              "      <td>-0.599269</td>\n",
              "      <td>-0.598259</td>\n",
              "      <td>-0.597582</td>\n",
              "      <td>-0.591303</td>\n",
              "      <td>-0.589020</td>\n",
              "      <td>-0.587533</td>\n",
              "      <td>-0.585462</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.644036</td>\n",
              "      <td>-0.643885</td>\n",
              "      <td>-0.645742</td>\n",
              "      <td>-0.646458</td>\n",
              "      <td>-0.646464</td>\n",
              "      <td>-0.645585</td>\n",
              "      <td>-0.642412</td>\n",
              "      <td>-0.643337</td>\n",
              "      <td>-0.636803</td>\n",
              "      <td>-0.631716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2</td>\n",
              "      <td>-0.547736</td>\n",
              "      <td>-0.553660</td>\n",
              "      <td>-0.557194</td>\n",
              "      <td>-0.559599</td>\n",
              "      <td>-0.564312</td>\n",
              "      <td>-0.568220</td>\n",
              "      <td>-0.571968</td>\n",
              "      <td>-0.575826</td>\n",
              "      <td>-0.577354</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.706939</td>\n",
              "      <td>-0.704759</td>\n",
              "      <td>-0.703900</td>\n",
              "      <td>-0.704163</td>\n",
              "      <td>-0.705410</td>\n",
              "      <td>-0.705741</td>\n",
              "      <td>-0.703861</td>\n",
              "      <td>-0.706541</td>\n",
              "      <td>-0.710381</td>\n",
              "      <td>-0.710854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2</td>\n",
              "      <td>-1.261183</td>\n",
              "      <td>-1.294884</td>\n",
              "      <td>-1.310105</td>\n",
              "      <td>-1.319604</td>\n",
              "      <td>-1.320262</td>\n",
              "      <td>-1.317001</td>\n",
              "      <td>-1.309626</td>\n",
              "      <td>-1.296729</td>\n",
              "      <td>-1.284765</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.260114</td>\n",
              "      <td>-1.269225</td>\n",
              "      <td>-1.274458</td>\n",
              "      <td>-1.278982</td>\n",
              "      <td>-1.280090</td>\n",
              "      <td>-1.281349</td>\n",
              "      <td>-1.281277</td>\n",
              "      <td>-1.280616</td>\n",
              "      <td>-1.280212</td>\n",
              "      <td>-1.279940</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2</td>\n",
              "      <td>-2.012581</td>\n",
              "      <td>-2.012069</td>\n",
              "      <td>-2.011625</td>\n",
              "      <td>-2.013534</td>\n",
              "      <td>-2.013438</td>\n",
              "      <td>-2.013219</td>\n",
              "      <td>-2.014245</td>\n",
              "      <td>-2.012278</td>\n",
              "      <td>-2.014058</td>\n",
              "      <td>...</td>\n",
              "      <td>0.426970</td>\n",
              "      <td>0.360089</td>\n",
              "      <td>0.274340</td>\n",
              "      <td>0.163402</td>\n",
              "      <td>0.019592</td>\n",
              "      <td>-0.150113</td>\n",
              "      <td>-0.333859</td>\n",
              "      <td>-0.551477</td>\n",
              "      <td>-0.782467</td>\n",
              "      <td>-1.007992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2</td>\n",
              "      <td>-1.065573</td>\n",
              "      <td>-1.066501</td>\n",
              "      <td>-1.067219</td>\n",
              "      <td>-1.068197</td>\n",
              "      <td>-1.067617</td>\n",
              "      <td>-1.071252</td>\n",
              "      <td>-1.067488</td>\n",
              "      <td>-1.067816</td>\n",
              "      <td>-1.067875</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.051603</td>\n",
              "      <td>-1.046511</td>\n",
              "      <td>-1.048871</td>\n",
              "      <td>-1.046295</td>\n",
              "      <td>-1.046580</td>\n",
              "      <td>-1.038032</td>\n",
              "      <td>-1.037612</td>\n",
              "      <td>-1.033426</td>\n",
              "      <td>-1.031462</td>\n",
              "      <td>-1.030165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>-1.177206</td>\n",
              "      <td>-1.175839</td>\n",
              "      <td>-1.173185</td>\n",
              "      <td>-1.170890</td>\n",
              "      <td>-1.169488</td>\n",
              "      <td>-1.166309</td>\n",
              "      <td>-1.165919</td>\n",
              "      <td>-1.167642</td>\n",
              "      <td>-1.166901</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.225565</td>\n",
              "      <td>-1.295701</td>\n",
              "      <td>-1.327421</td>\n",
              "      <td>-1.327071</td>\n",
              "      <td>-1.300439</td>\n",
              "      <td>-1.271138</td>\n",
              "      <td>-1.267283</td>\n",
              "      <td>-1.265006</td>\n",
              "      <td>-1.270722</td>\n",
              "      <td>-1.262134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.816323</td>\n",
              "      <td>-0.814104</td>\n",
              "      <td>-0.815892</td>\n",
              "      <td>-0.814220</td>\n",
              "      <td>-0.816649</td>\n",
              "      <td>-0.816978</td>\n",
              "      <td>-0.811393</td>\n",
              "      <td>-0.811339</td>\n",
              "      <td>-0.812096</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.027205</td>\n",
              "      <td>-1.028066</td>\n",
              "      <td>-1.021606</td>\n",
              "      <td>-1.021195</td>\n",
              "      <td>-1.024795</td>\n",
              "      <td>-1.021716</td>\n",
              "      <td>-1.022889</td>\n",
              "      <td>-1.021380</td>\n",
              "      <td>-1.022272</td>\n",
              "      <td>-1.022983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1</td>\n",
              "      <td>-1.136846</td>\n",
              "      <td>-1.140746</td>\n",
              "      <td>-1.139471</td>\n",
              "      <td>-1.130013</td>\n",
              "      <td>-1.121607</td>\n",
              "      <td>-1.118990</td>\n",
              "      <td>-1.112249</td>\n",
              "      <td>-1.113174</td>\n",
              "      <td>-1.115731</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.186083</td>\n",
              "      <td>-1.154958</td>\n",
              "      <td>-1.130243</td>\n",
              "      <td>-1.118832</td>\n",
              "      <td>-1.122127</td>\n",
              "      <td>-1.124086</td>\n",
              "      <td>-1.120237</td>\n",
              "      <td>-1.111846</td>\n",
              "      <td>-1.103578</td>\n",
              "      <td>-1.103998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1</td>\n",
              "      <td>-1.115016</td>\n",
              "      <td>-1.186591</td>\n",
              "      <td>-1.263632</td>\n",
              "      <td>-1.341706</td>\n",
              "      <td>-1.385512</td>\n",
              "      <td>-1.406174</td>\n",
              "      <td>-1.396433</td>\n",
              "      <td>-1.338130</td>\n",
              "      <td>-1.306990</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.991173</td>\n",
              "      <td>-0.995515</td>\n",
              "      <td>-0.996563</td>\n",
              "      <td>-0.998444</td>\n",
              "      <td>-1.001865</td>\n",
              "      <td>-1.007205</td>\n",
              "      <td>-1.012854</td>\n",
              "      <td>-1.021305</td>\n",
              "      <td>-1.029152</td>\n",
              "      <td>-1.044706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1</td>\n",
              "      <td>-1.201155</td>\n",
              "      <td>-1.208410</td>\n",
              "      <td>-1.206482</td>\n",
              "      <td>-1.203056</td>\n",
              "      <td>-1.180973</td>\n",
              "      <td>-1.165620</td>\n",
              "      <td>-1.164265</td>\n",
              "      <td>-1.168617</td>\n",
              "      <td>-1.163736</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.091031</td>\n",
              "      <td>-1.165393</td>\n",
              "      <td>-1.272919</td>\n",
              "      <td>-1.343542</td>\n",
              "      <td>-1.337689</td>\n",
              "      <td>-1.336491</td>\n",
              "      <td>-1.284089</td>\n",
              "      <td>-1.206961</td>\n",
              "      <td>-1.122139</td>\n",
              "      <td>-1.082205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2</td>\n",
              "      <td>-0.603291</td>\n",
              "      <td>-0.607376</td>\n",
              "      <td>-0.606829</td>\n",
              "      <td>-0.607565</td>\n",
              "      <td>-0.604953</td>\n",
              "      <td>-0.596126</td>\n",
              "      <td>-0.590059</td>\n",
              "      <td>-0.586220</td>\n",
              "      <td>-0.583763</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.596761</td>\n",
              "      <td>-0.594511</td>\n",
              "      <td>-0.595603</td>\n",
              "      <td>-0.603554</td>\n",
              "      <td>-0.612209</td>\n",
              "      <td>-0.623519</td>\n",
              "      <td>-0.625307</td>\n",
              "      <td>-0.625411</td>\n",
              "      <td>-0.614697</td>\n",
              "      <td>-0.604779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1</td>\n",
              "      <td>-1.162008</td>\n",
              "      <td>-1.161359</td>\n",
              "      <td>-1.162379</td>\n",
              "      <td>-1.164074</td>\n",
              "      <td>-1.161457</td>\n",
              "      <td>-1.157800</td>\n",
              "      <td>-1.159455</td>\n",
              "      <td>-1.161424</td>\n",
              "      <td>-1.158713</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.127586</td>\n",
              "      <td>-1.136727</td>\n",
              "      <td>-1.137836</td>\n",
              "      <td>-1.132538</td>\n",
              "      <td>-1.129391</td>\n",
              "      <td>-1.128199</td>\n",
              "      <td>-1.128224</td>\n",
              "      <td>-1.125779</td>\n",
              "      <td>-1.121353</td>\n",
              "      <td>-1.116188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>2</td>\n",
              "      <td>-0.808018</td>\n",
              "      <td>-0.809292</td>\n",
              "      <td>-0.808723</td>\n",
              "      <td>-0.805904</td>\n",
              "      <td>-0.803404</td>\n",
              "      <td>-0.801347</td>\n",
              "      <td>-0.798875</td>\n",
              "      <td>-0.794809</td>\n",
              "      <td>-0.791848</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.695193</td>\n",
              "      <td>-0.695277</td>\n",
              "      <td>-0.690663</td>\n",
              "      <td>-0.690106</td>\n",
              "      <td>-0.685703</td>\n",
              "      <td>-0.680779</td>\n",
              "      <td>-0.676761</td>\n",
              "      <td>-0.673248</td>\n",
              "      <td>-0.668439</td>\n",
              "      <td>-0.663304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>2</td>\n",
              "      <td>-0.622128</td>\n",
              "      <td>-0.619164</td>\n",
              "      <td>-0.613308</td>\n",
              "      <td>-0.611285</td>\n",
              "      <td>-0.607336</td>\n",
              "      <td>-0.607347</td>\n",
              "      <td>-0.607487</td>\n",
              "      <td>-0.606411</td>\n",
              "      <td>-0.606287</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.586214</td>\n",
              "      <td>-0.595359</td>\n",
              "      <td>-0.598823</td>\n",
              "      <td>-0.603955</td>\n",
              "      <td>-0.609916</td>\n",
              "      <td>-0.615390</td>\n",
              "      <td>-0.624009</td>\n",
              "      <td>-0.628865</td>\n",
              "      <td>-0.640098</td>\n",
              "      <td>-0.650745</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1</td>\n",
              "      <td>-1.054014</td>\n",
              "      <td>-1.050717</td>\n",
              "      <td>-1.047666</td>\n",
              "      <td>-1.047448</td>\n",
              "      <td>-1.048769</td>\n",
              "      <td>-1.047781</td>\n",
              "      <td>-1.047891</td>\n",
              "      <td>-1.047536</td>\n",
              "      <td>-1.047369</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.082618</td>\n",
              "      <td>-1.071515</td>\n",
              "      <td>-1.063564</td>\n",
              "      <td>-1.066977</td>\n",
              "      <td>-1.071325</td>\n",
              "      <td>-1.068112</td>\n",
              "      <td>-1.068966</td>\n",
              "      <td>-1.069154</td>\n",
              "      <td>-1.066642</td>\n",
              "      <td>-1.064798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>2</td>\n",
              "      <td>-1.082541</td>\n",
              "      <td>-1.082470</td>\n",
              "      <td>-1.084301</td>\n",
              "      <td>-1.083262</td>\n",
              "      <td>-1.084104</td>\n",
              "      <td>-1.085222</td>\n",
              "      <td>-1.083233</td>\n",
              "      <td>-1.083495</td>\n",
              "      <td>-1.083226</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.147159</td>\n",
              "      <td>-1.146026</td>\n",
              "      <td>-1.145903</td>\n",
              "      <td>-1.147187</td>\n",
              "      <td>-1.145955</td>\n",
              "      <td>-1.148524</td>\n",
              "      <td>-1.149558</td>\n",
              "      <td>-1.148339</td>\n",
              "      <td>-1.149799</td>\n",
              "      <td>-1.148310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.851924</td>\n",
              "      <td>-0.849687</td>\n",
              "      <td>-0.850311</td>\n",
              "      <td>-0.848363</td>\n",
              "      <td>-0.845198</td>\n",
              "      <td>-0.839203</td>\n",
              "      <td>-0.834818</td>\n",
              "      <td>-0.834412</td>\n",
              "      <td>-0.834664</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.883772</td>\n",
              "      <td>-0.881958</td>\n",
              "      <td>-0.871798</td>\n",
              "      <td>-0.861634</td>\n",
              "      <td>-0.845883</td>\n",
              "      <td>-0.837268</td>\n",
              "      <td>-0.819175</td>\n",
              "      <td>-0.812857</td>\n",
              "      <td>-0.805104</td>\n",
              "      <td>-0.802605</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.769293</td>\n",
              "      <td>-0.772803</td>\n",
              "      <td>-0.767545</td>\n",
              "      <td>-0.753188</td>\n",
              "      <td>-0.738470</td>\n",
              "      <td>-0.721032</td>\n",
              "      <td>-0.700963</td>\n",
              "      <td>-0.685223</td>\n",
              "      <td>-0.671344</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.047628</td>\n",
              "      <td>-1.025212</td>\n",
              "      <td>-1.014792</td>\n",
              "      <td>-1.006514</td>\n",
              "      <td>-1.003012</td>\n",
              "      <td>-1.000901</td>\n",
              "      <td>-1.001019</td>\n",
              "      <td>-0.997393</td>\n",
              "      <td>-0.996212</td>\n",
              "      <td>-0.994838</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>1</td>\n",
              "      <td>-1.238278</td>\n",
              "      <td>-1.186778</td>\n",
              "      <td>-1.147941</td>\n",
              "      <td>-1.125283</td>\n",
              "      <td>-1.125837</td>\n",
              "      <td>-1.129168</td>\n",
              "      <td>-1.135536</td>\n",
              "      <td>-1.139822</td>\n",
              "      <td>-1.132811</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.032562</td>\n",
              "      <td>-1.112665</td>\n",
              "      <td>-1.182298</td>\n",
              "      <td>-1.252019</td>\n",
              "      <td>-1.319470</td>\n",
              "      <td>-1.329326</td>\n",
              "      <td>-1.334852</td>\n",
              "      <td>-1.299284</td>\n",
              "      <td>-1.246920</td>\n",
              "      <td>-1.213891</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>2</td>\n",
              "      <td>-1.755800</td>\n",
              "      <td>-1.746496</td>\n",
              "      <td>-1.732612</td>\n",
              "      <td>-1.727370</td>\n",
              "      <td>-1.725305</td>\n",
              "      <td>-1.724826</td>\n",
              "      <td>-1.721212</td>\n",
              "      <td>-1.722971</td>\n",
              "      <td>-1.718352</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.815714</td>\n",
              "      <td>-1.051389</td>\n",
              "      <td>-1.275793</td>\n",
              "      <td>-1.470041</td>\n",
              "      <td>-1.639795</td>\n",
              "      <td>-1.757432</td>\n",
              "      <td>-1.826076</td>\n",
              "      <td>-1.870268</td>\n",
              "      <td>-1.894537</td>\n",
              "      <td>-1.898637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.716838</td>\n",
              "      <td>-0.713062</td>\n",
              "      <td>-0.710098</td>\n",
              "      <td>-0.711421</td>\n",
              "      <td>-0.709967</td>\n",
              "      <td>-0.711136</td>\n",
              "      <td>-0.708688</td>\n",
              "      <td>-0.708387</td>\n",
              "      <td>-0.708409</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.552814</td>\n",
              "      <td>-0.558172</td>\n",
              "      <td>-0.562053</td>\n",
              "      <td>-0.567406</td>\n",
              "      <td>-0.569266</td>\n",
              "      <td>-0.572434</td>\n",
              "      <td>-0.571733</td>\n",
              "      <td>-0.570443</td>\n",
              "      <td>-0.569006</td>\n",
              "      <td>-0.566393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>2</td>\n",
              "      <td>-0.969391</td>\n",
              "      <td>-0.972108</td>\n",
              "      <td>-0.972616</td>\n",
              "      <td>-0.973649</td>\n",
              "      <td>-0.973924</td>\n",
              "      <td>-0.976987</td>\n",
              "      <td>-0.976895</td>\n",
              "      <td>-0.977544</td>\n",
              "      <td>-0.979135</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.922834</td>\n",
              "      <td>-0.924728</td>\n",
              "      <td>-0.922734</td>\n",
              "      <td>-0.924245</td>\n",
              "      <td>-0.923938</td>\n",
              "      <td>-0.923793</td>\n",
              "      <td>-0.924096</td>\n",
              "      <td>-0.925417</td>\n",
              "      <td>-0.926572</td>\n",
              "      <td>-0.929978</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.971732</td>\n",
              "      <td>-1.012182</td>\n",
              "      <td>-1.049772</td>\n",
              "      <td>-1.077144</td>\n",
              "      <td>-1.092190</td>\n",
              "      <td>-1.092920</td>\n",
              "      <td>-1.090993</td>\n",
              "      <td>-1.083252</td>\n",
              "      <td>-1.074275</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.269844</td>\n",
              "      <td>-1.239248</td>\n",
              "      <td>-1.200457</td>\n",
              "      <td>-1.170905</td>\n",
              "      <td>-1.158130</td>\n",
              "      <td>-1.153122</td>\n",
              "      <td>-1.154176</td>\n",
              "      <td>-1.154564</td>\n",
              "      <td>-1.150073</td>\n",
              "      <td>-1.145040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>1</td>\n",
              "      <td>-1.081020</td>\n",
              "      <td>-1.078859</td>\n",
              "      <td>-1.079472</td>\n",
              "      <td>-1.080085</td>\n",
              "      <td>-1.078681</td>\n",
              "      <td>-1.078272</td>\n",
              "      <td>-1.077298</td>\n",
              "      <td>-1.078944</td>\n",
              "      <td>-1.077041</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.201060</td>\n",
              "      <td>-1.202613</td>\n",
              "      <td>-1.203260</td>\n",
              "      <td>-1.200616</td>\n",
              "      <td>-1.196168</td>\n",
              "      <td>-1.199188</td>\n",
              "      <td>-1.195577</td>\n",
              "      <td>-1.186975</td>\n",
              "      <td>-1.173823</td>\n",
              "      <td>-1.166574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>2</td>\n",
              "      <td>-1.006849</td>\n",
              "      <td>-1.008827</td>\n",
              "      <td>-1.008523</td>\n",
              "      <td>-1.011347</td>\n",
              "      <td>-1.011705</td>\n",
              "      <td>-1.011933</td>\n",
              "      <td>-1.014436</td>\n",
              "      <td>-1.012562</td>\n",
              "      <td>-1.017186</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.114653</td>\n",
              "      <td>-1.114498</td>\n",
              "      <td>-1.117535</td>\n",
              "      <td>-1.119042</td>\n",
              "      <td>-1.120056</td>\n",
              "      <td>-1.120553</td>\n",
              "      <td>-1.120983</td>\n",
              "      <td>-1.122364</td>\n",
              "      <td>-1.121879</td>\n",
              "      <td>-1.122235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>1</td>\n",
              "      <td>-1.441403</td>\n",
              "      <td>-1.504756</td>\n",
              "      <td>-1.553785</td>\n",
              "      <td>-1.566228</td>\n",
              "      <td>-1.550400</td>\n",
              "      <td>-1.505409</td>\n",
              "      <td>-1.454040</td>\n",
              "      <td>-1.408540</td>\n",
              "      <td>-1.375772</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.134365</td>\n",
              "      <td>-1.141934</td>\n",
              "      <td>-1.150923</td>\n",
              "      <td>-1.155292</td>\n",
              "      <td>-1.157989</td>\n",
              "      <td>-1.162612</td>\n",
              "      <td>-1.165037</td>\n",
              "      <td>-1.170046</td>\n",
              "      <td>-1.174333</td>\n",
              "      <td>-1.174406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.964333</td>\n",
              "      <td>-0.964264</td>\n",
              "      <td>-0.964772</td>\n",
              "      <td>-0.963092</td>\n",
              "      <td>-0.963757</td>\n",
              "      <td>-0.962386</td>\n",
              "      <td>-0.961999</td>\n",
              "      <td>-0.961540</td>\n",
              "      <td>-0.960973</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.896700</td>\n",
              "      <td>-0.893666</td>\n",
              "      <td>-0.891841</td>\n",
              "      <td>-0.893301</td>\n",
              "      <td>-0.891128</td>\n",
              "      <td>-0.890467</td>\n",
              "      <td>-0.889976</td>\n",
              "      <td>-0.890854</td>\n",
              "      <td>-0.890339</td>\n",
              "      <td>-0.893748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>2</td>\n",
              "      <td>-1.481718</td>\n",
              "      <td>-1.482924</td>\n",
              "      <td>-1.483160</td>\n",
              "      <td>-1.482303</td>\n",
              "      <td>-1.479985</td>\n",
              "      <td>-1.480745</td>\n",
              "      <td>-1.481513</td>\n",
              "      <td>-1.480230</td>\n",
              "      <td>-1.481170</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.367792</td>\n",
              "      <td>-1.362261</td>\n",
              "      <td>-1.363540</td>\n",
              "      <td>-1.364604</td>\n",
              "      <td>-1.368324</td>\n",
              "      <td>-1.370659</td>\n",
              "      <td>-1.374070</td>\n",
              "      <td>-1.373879</td>\n",
              "      <td>-1.373785</td>\n",
              "      <td>-1.374736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>2</td>\n",
              "      <td>-0.981131</td>\n",
              "      <td>-0.979102</td>\n",
              "      <td>-0.977645</td>\n",
              "      <td>-0.975884</td>\n",
              "      <td>-0.975598</td>\n",
              "      <td>-0.977514</td>\n",
              "      <td>-0.977998</td>\n",
              "      <td>-0.979422</td>\n",
              "      <td>-0.980816</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.971106</td>\n",
              "      <td>-0.971827</td>\n",
              "      <td>-0.970183</td>\n",
              "      <td>-0.965389</td>\n",
              "      <td>-0.965832</td>\n",
              "      <td>-0.966595</td>\n",
              "      <td>-0.967407</td>\n",
              "      <td>-0.969254</td>\n",
              "      <td>-0.970416</td>\n",
              "      <td>-0.970535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.723961</td>\n",
              "      <td>-0.725263</td>\n",
              "      <td>-0.729254</td>\n",
              "      <td>-0.732690</td>\n",
              "      <td>-0.733603</td>\n",
              "      <td>-0.734398</td>\n",
              "      <td>-0.736329</td>\n",
              "      <td>-0.738404</td>\n",
              "      <td>-0.740447</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.757875</td>\n",
              "      <td>-0.756425</td>\n",
              "      <td>-0.755322</td>\n",
              "      <td>-0.757650</td>\n",
              "      <td>-0.760134</td>\n",
              "      <td>-0.757973</td>\n",
              "      <td>-0.753264</td>\n",
              "      <td>-0.751725</td>\n",
              "      <td>-0.749770</td>\n",
              "      <td>-0.744602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>2</td>\n",
              "      <td>-0.644121</td>\n",
              "      <td>-0.641005</td>\n",
              "      <td>-0.639447</td>\n",
              "      <td>-0.636397</td>\n",
              "      <td>-0.635603</td>\n",
              "      <td>-0.632853</td>\n",
              "      <td>-0.631492</td>\n",
              "      <td>-0.628741</td>\n",
              "      <td>-0.625365</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.623083</td>\n",
              "      <td>-0.619075</td>\n",
              "      <td>-0.618718</td>\n",
              "      <td>-0.614866</td>\n",
              "      <td>-0.614519</td>\n",
              "      <td>-0.609214</td>\n",
              "      <td>-0.607610</td>\n",
              "      <td>-0.604666</td>\n",
              "      <td>-0.605630</td>\n",
              "      <td>-0.600113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.604468</td>\n",
              "      <td>-0.608084</td>\n",
              "      <td>-0.613065</td>\n",
              "      <td>-0.614926</td>\n",
              "      <td>-0.615522</td>\n",
              "      <td>-0.616300</td>\n",
              "      <td>-0.617992</td>\n",
              "      <td>-0.618821</td>\n",
              "      <td>-0.619901</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.646419</td>\n",
              "      <td>-0.647263</td>\n",
              "      <td>-0.646744</td>\n",
              "      <td>-0.645072</td>\n",
              "      <td>-0.643929</td>\n",
              "      <td>-0.642893</td>\n",
              "      <td>-0.642514</td>\n",
              "      <td>-0.640783</td>\n",
              "      <td>-0.640529</td>\n",
              "      <td>-0.640211</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>2</td>\n",
              "      <td>-1.287009</td>\n",
              "      <td>-1.282432</td>\n",
              "      <td>-1.303656</td>\n",
              "      <td>-1.293082</td>\n",
              "      <td>-1.293506</td>\n",
              "      <td>-1.292730</td>\n",
              "      <td>-1.299701</td>\n",
              "      <td>-1.302802</td>\n",
              "      <td>-1.299218</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.198844</td>\n",
              "      <td>-1.197890</td>\n",
              "      <td>-1.197334</td>\n",
              "      <td>-1.197650</td>\n",
              "      <td>-1.197966</td>\n",
              "      <td>-1.198309</td>\n",
              "      <td>-1.199803</td>\n",
              "      <td>-1.202034</td>\n",
              "      <td>-1.203569</td>\n",
              "      <td>-1.205487</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>2</td>\n",
              "      <td>-0.546345</td>\n",
              "      <td>-0.548584</td>\n",
              "      <td>-0.549649</td>\n",
              "      <td>-0.549325</td>\n",
              "      <td>-0.550956</td>\n",
              "      <td>-0.556537</td>\n",
              "      <td>-0.556732</td>\n",
              "      <td>-0.562642</td>\n",
              "      <td>-0.564610</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.671652</td>\n",
              "      <td>-0.665614</td>\n",
              "      <td>-0.659891</td>\n",
              "      <td>-0.656913</td>\n",
              "      <td>-0.654146</td>\n",
              "      <td>-0.652727</td>\n",
              "      <td>-0.651819</td>\n",
              "      <td>-0.651962</td>\n",
              "      <td>-0.651519</td>\n",
              "      <td>-0.653748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>2</td>\n",
              "      <td>-1.623368</td>\n",
              "      <td>-1.623696</td>\n",
              "      <td>-1.622161</td>\n",
              "      <td>-1.622906</td>\n",
              "      <td>-1.621739</td>\n",
              "      <td>-1.621860</td>\n",
              "      <td>-1.622854</td>\n",
              "      <td>-1.621253</td>\n",
              "      <td>-1.622584</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.403411</td>\n",
              "      <td>-1.549479</td>\n",
              "      <td>-1.660080</td>\n",
              "      <td>-1.745874</td>\n",
              "      <td>-1.797701</td>\n",
              "      <td>-1.821457</td>\n",
              "      <td>-1.820755</td>\n",
              "      <td>-1.814826</td>\n",
              "      <td>-1.802348</td>\n",
              "      <td>-1.779259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>2</td>\n",
              "      <td>-1.113247</td>\n",
              "      <td>-1.113157</td>\n",
              "      <td>-1.114343</td>\n",
              "      <td>-1.113907</td>\n",
              "      <td>-1.115143</td>\n",
              "      <td>-1.117166</td>\n",
              "      <td>-1.115657</td>\n",
              "      <td>-1.117199</td>\n",
              "      <td>-1.118094</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.174622</td>\n",
              "      <td>-1.173644</td>\n",
              "      <td>-1.174277</td>\n",
              "      <td>-1.175176</td>\n",
              "      <td>-1.176694</td>\n",
              "      <td>-1.175051</td>\n",
              "      <td>-1.176095</td>\n",
              "      <td>-1.175167</td>\n",
              "      <td>-1.174964</td>\n",
              "      <td>-1.174871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>2</td>\n",
              "      <td>-0.744119</td>\n",
              "      <td>-0.734262</td>\n",
              "      <td>-0.730512</td>\n",
              "      <td>-0.721463</td>\n",
              "      <td>-0.708476</td>\n",
              "      <td>-0.699061</td>\n",
              "      <td>-0.695946</td>\n",
              "      <td>-0.693064</td>\n",
              "      <td>-0.693325</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.712959</td>\n",
              "      <td>-0.711650</td>\n",
              "      <td>-0.711025</td>\n",
              "      <td>-0.711942</td>\n",
              "      <td>-0.708887</td>\n",
              "      <td>-0.706704</td>\n",
              "      <td>-0.706233</td>\n",
              "      <td>-0.704465</td>\n",
              "      <td>-0.704567</td>\n",
              "      <td>-0.703899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>1</td>\n",
              "      <td>-1.153652</td>\n",
              "      <td>-1.150899</td>\n",
              "      <td>-1.150287</td>\n",
              "      <td>-1.149413</td>\n",
              "      <td>-1.149148</td>\n",
              "      <td>-1.147418</td>\n",
              "      <td>-1.147949</td>\n",
              "      <td>-1.143553</td>\n",
              "      <td>-1.145072</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.943241</td>\n",
              "      <td>-0.999805</td>\n",
              "      <td>-1.074357</td>\n",
              "      <td>-1.160415</td>\n",
              "      <td>-1.230090</td>\n",
              "      <td>-1.262929</td>\n",
              "      <td>-1.260485</td>\n",
              "      <td>-1.237534</td>\n",
              "      <td>-1.213591</td>\n",
              "      <td>-1.196872</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.969865</td>\n",
              "      <td>-0.972680</td>\n",
              "      <td>-0.970600</td>\n",
              "      <td>-0.971651</td>\n",
              "      <td>-0.968887</td>\n",
              "      <td>-0.967290</td>\n",
              "      <td>-0.965224</td>\n",
              "      <td>-0.964983</td>\n",
              "      <td>-0.964252</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.013655</td>\n",
              "      <td>-1.015520</td>\n",
              "      <td>-1.014679</td>\n",
              "      <td>-1.011770</td>\n",
              "      <td>-1.006992</td>\n",
              "      <td>-1.003574</td>\n",
              "      <td>-1.006509</td>\n",
              "      <td>-1.006429</td>\n",
              "      <td>-1.006552</td>\n",
              "      <td>-1.003153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>1</td>\n",
              "      <td>-1.160162</td>\n",
              "      <td>-1.243003</td>\n",
              "      <td>-1.254295</td>\n",
              "      <td>-1.234512</td>\n",
              "      <td>-1.166222</td>\n",
              "      <td>-1.096319</td>\n",
              "      <td>-1.039987</td>\n",
              "      <td>-1.026723</td>\n",
              "      <td>-1.033920</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.233655</td>\n",
              "      <td>-1.221131</td>\n",
              "      <td>-1.180033</td>\n",
              "      <td>-1.121479</td>\n",
              "      <td>-1.083692</td>\n",
              "      <td>-1.053213</td>\n",
              "      <td>-1.038400</td>\n",
              "      <td>-1.034783</td>\n",
              "      <td>-1.038866</td>\n",
              "      <td>-1.033090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>2</td>\n",
              "      <td>-1.036995</td>\n",
              "      <td>-1.037497</td>\n",
              "      <td>-1.037294</td>\n",
              "      <td>-1.037132</td>\n",
              "      <td>-1.040754</td>\n",
              "      <td>-1.040785</td>\n",
              "      <td>-1.040019</td>\n",
              "      <td>-1.038676</td>\n",
              "      <td>-1.039535</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.149979</td>\n",
              "      <td>-1.147545</td>\n",
              "      <td>-1.149843</td>\n",
              "      <td>-1.151544</td>\n",
              "      <td>-1.152749</td>\n",
              "      <td>-1.150443</td>\n",
              "      <td>-1.150210</td>\n",
              "      <td>-1.152899</td>\n",
              "      <td>-1.150172</td>\n",
              "      <td>-1.151864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>2</td>\n",
              "      <td>-0.564906</td>\n",
              "      <td>-0.565050</td>\n",
              "      <td>-0.566477</td>\n",
              "      <td>-0.564775</td>\n",
              "      <td>-0.565744</td>\n",
              "      <td>-0.564183</td>\n",
              "      <td>-0.564637</td>\n",
              "      <td>-0.563114</td>\n",
              "      <td>-0.564548</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.723784</td>\n",
              "      <td>-0.725717</td>\n",
              "      <td>-0.726697</td>\n",
              "      <td>-0.725656</td>\n",
              "      <td>-0.726813</td>\n",
              "      <td>-0.726768</td>\n",
              "      <td>-0.725525</td>\n",
              "      <td>-0.724171</td>\n",
              "      <td>-0.720889</td>\n",
              "      <td>-0.718832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.614644</td>\n",
              "      <td>-0.614987</td>\n",
              "      <td>-0.614786</td>\n",
              "      <td>-0.614042</td>\n",
              "      <td>-0.613145</td>\n",
              "      <td>-0.612391</td>\n",
              "      <td>-0.610861</td>\n",
              "      <td>-0.608124</td>\n",
              "      <td>-0.606101</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.776073</td>\n",
              "      <td>-0.784714</td>\n",
              "      <td>-0.796625</td>\n",
              "      <td>-0.804653</td>\n",
              "      <td>-0.800372</td>\n",
              "      <td>-0.797827</td>\n",
              "      <td>-0.795079</td>\n",
              "      <td>-0.797045</td>\n",
              "      <td>-0.801276</td>\n",
              "      <td>-0.808361</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>2</td>\n",
              "      <td>-0.779126</td>\n",
              "      <td>-0.778379</td>\n",
              "      <td>-0.775745</td>\n",
              "      <td>-0.776247</td>\n",
              "      <td>-0.773167</td>\n",
              "      <td>-0.773882</td>\n",
              "      <td>-0.768961</td>\n",
              "      <td>-0.762046</td>\n",
              "      <td>-0.758626</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.512137</td>\n",
              "      <td>-0.512710</td>\n",
              "      <td>-0.510124</td>\n",
              "      <td>-0.511867</td>\n",
              "      <td>-0.511880</td>\n",
              "      <td>-0.507179</td>\n",
              "      <td>-0.506964</td>\n",
              "      <td>-0.505006</td>\n",
              "      <td>-0.503731</td>\n",
              "      <td>-0.504385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.703033</td>\n",
              "      <td>-0.702618</td>\n",
              "      <td>-0.702504</td>\n",
              "      <td>-0.701361</td>\n",
              "      <td>-0.700449</td>\n",
              "      <td>-0.700562</td>\n",
              "      <td>-0.700974</td>\n",
              "      <td>-0.701128</td>\n",
              "      <td>-0.702176</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.640131</td>\n",
              "      <td>-0.640705</td>\n",
              "      <td>-0.639800</td>\n",
              "      <td>-0.639546</td>\n",
              "      <td>-0.637827</td>\n",
              "      <td>-0.639201</td>\n",
              "      <td>-0.640260</td>\n",
              "      <td>-0.641134</td>\n",
              "      <td>-0.641406</td>\n",
              "      <td>-0.642109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>2</td>\n",
              "      <td>-1.435720</td>\n",
              "      <td>-1.432272</td>\n",
              "      <td>-1.432929</td>\n",
              "      <td>-1.431641</td>\n",
              "      <td>-1.432595</td>\n",
              "      <td>-1.432303</td>\n",
              "      <td>-1.433452</td>\n",
              "      <td>-1.432427</td>\n",
              "      <td>-1.433273</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.477989</td>\n",
              "      <td>-1.461869</td>\n",
              "      <td>-1.447364</td>\n",
              "      <td>-1.438888</td>\n",
              "      <td>-1.437793</td>\n",
              "      <td>-1.436885</td>\n",
              "      <td>-1.434493</td>\n",
              "      <td>-1.435462</td>\n",
              "      <td>-1.435282</td>\n",
              "      <td>-1.430884</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>50 rows × 151 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    0         1         2         3         4         5         6         7    \\\n",
              "0     2 -0.647885 -0.641992 -0.638186 -0.638259 -0.638345 -0.638697 -0.643049   \n",
              "1     2 -0.644427 -0.645401 -0.647055 -0.647492 -0.646910 -0.643884 -0.639731   \n",
              "2     1 -0.778353 -0.778279 -0.777151 -0.777684 -0.775900 -0.772421 -0.765464   \n",
              "3     1 -0.750060 -0.748103 -0.746164 -0.745926 -0.743767 -0.743805 -0.745213   \n",
              "4     2 -0.599539 -0.597422 -0.599269 -0.598259 -0.597582 -0.591303 -0.589020   \n",
              "5     2 -0.547736 -0.553660 -0.557194 -0.559599 -0.564312 -0.568220 -0.571968   \n",
              "6     2 -1.261183 -1.294884 -1.310105 -1.319604 -1.320262 -1.317001 -1.309626   \n",
              "7     2 -2.012581 -2.012069 -2.011625 -2.013534 -2.013438 -2.013219 -2.014245   \n",
              "8     2 -1.065573 -1.066501 -1.067219 -1.068197 -1.067617 -1.071252 -1.067488   \n",
              "9     1 -1.177206 -1.175839 -1.173185 -1.170890 -1.169488 -1.166309 -1.165919   \n",
              "10    1 -0.816323 -0.814104 -0.815892 -0.814220 -0.816649 -0.816978 -0.811393   \n",
              "11    1 -1.136846 -1.140746 -1.139471 -1.130013 -1.121607 -1.118990 -1.112249   \n",
              "12    1 -1.115016 -1.186591 -1.263632 -1.341706 -1.385512 -1.406174 -1.396433   \n",
              "13    1 -1.201155 -1.208410 -1.206482 -1.203056 -1.180973 -1.165620 -1.164265   \n",
              "14    2 -0.603291 -0.607376 -0.606829 -0.607565 -0.604953 -0.596126 -0.590059   \n",
              "15    1 -1.162008 -1.161359 -1.162379 -1.164074 -1.161457 -1.157800 -1.159455   \n",
              "16    2 -0.808018 -0.809292 -0.808723 -0.805904 -0.803404 -0.801347 -0.798875   \n",
              "17    2 -0.622128 -0.619164 -0.613308 -0.611285 -0.607336 -0.607347 -0.607487   \n",
              "18    1 -1.054014 -1.050717 -1.047666 -1.047448 -1.048769 -1.047781 -1.047891   \n",
              "19    2 -1.082541 -1.082470 -1.084301 -1.083262 -1.084104 -1.085222 -1.083233   \n",
              "20    1 -0.851924 -0.849687 -0.850311 -0.848363 -0.845198 -0.839203 -0.834818   \n",
              "21    1 -0.769293 -0.772803 -0.767545 -0.753188 -0.738470 -0.721032 -0.700963   \n",
              "22    1 -1.238278 -1.186778 -1.147941 -1.125283 -1.125837 -1.129168 -1.135536   \n",
              "23    2 -1.755800 -1.746496 -1.732612 -1.727370 -1.725305 -1.724826 -1.721212   \n",
              "24    1 -0.716838 -0.713062 -0.710098 -0.711421 -0.709967 -0.711136 -0.708688   \n",
              "25    2 -0.969391 -0.972108 -0.972616 -0.973649 -0.973924 -0.976987 -0.976895   \n",
              "26    1 -0.971732 -1.012182 -1.049772 -1.077144 -1.092190 -1.092920 -1.090993   \n",
              "27    1 -1.081020 -1.078859 -1.079472 -1.080085 -1.078681 -1.078272 -1.077298   \n",
              "28    2 -1.006849 -1.008827 -1.008523 -1.011347 -1.011705 -1.011933 -1.014436   \n",
              "29    1 -1.441403 -1.504756 -1.553785 -1.566228 -1.550400 -1.505409 -1.454040   \n",
              "30    1 -0.964333 -0.964264 -0.964772 -0.963092 -0.963757 -0.962386 -0.961999   \n",
              "31    2 -1.481718 -1.482924 -1.483160 -1.482303 -1.479985 -1.480745 -1.481513   \n",
              "32    2 -0.981131 -0.979102 -0.977645 -0.975884 -0.975598 -0.977514 -0.977998   \n",
              "33    1 -0.723961 -0.725263 -0.729254 -0.732690 -0.733603 -0.734398 -0.736329   \n",
              "34    2 -0.644121 -0.641005 -0.639447 -0.636397 -0.635603 -0.632853 -0.631492   \n",
              "35    1 -0.604468 -0.608084 -0.613065 -0.614926 -0.615522 -0.616300 -0.617992   \n",
              "36    2 -1.287009 -1.282432 -1.303656 -1.293082 -1.293506 -1.292730 -1.299701   \n",
              "37    2 -0.546345 -0.548584 -0.549649 -0.549325 -0.550956 -0.556537 -0.556732   \n",
              "38    2 -1.623368 -1.623696 -1.622161 -1.622906 -1.621739 -1.621860 -1.622854   \n",
              "39    2 -1.113247 -1.113157 -1.114343 -1.113907 -1.115143 -1.117166 -1.115657   \n",
              "40    2 -0.744119 -0.734262 -0.730512 -0.721463 -0.708476 -0.699061 -0.695946   \n",
              "41    1 -1.153652 -1.150899 -1.150287 -1.149413 -1.149148 -1.147418 -1.147949   \n",
              "42    1 -0.969865 -0.972680 -0.970600 -0.971651 -0.968887 -0.967290 -0.965224   \n",
              "43    1 -1.160162 -1.243003 -1.254295 -1.234512 -1.166222 -1.096319 -1.039987   \n",
              "44    2 -1.036995 -1.037497 -1.037294 -1.037132 -1.040754 -1.040785 -1.040019   \n",
              "45    2 -0.564906 -0.565050 -0.566477 -0.564775 -0.565744 -0.564183 -0.564637   \n",
              "46    1 -0.614644 -0.614987 -0.614786 -0.614042 -0.613145 -0.612391 -0.610861   \n",
              "47    2 -0.779126 -0.778379 -0.775745 -0.776247 -0.773167 -0.773882 -0.768961   \n",
              "48    1 -0.703033 -0.702618 -0.702504 -0.701361 -0.700449 -0.700562 -0.700974   \n",
              "49    2 -1.435720 -1.432272 -1.432929 -1.431641 -1.432595 -1.432303 -1.433452   \n",
              "\n",
              "         8         9    ...       141       142       143       144       145  \\\n",
              "0  -0.643768 -0.645050  ... -0.639264 -0.639716 -0.639735 -0.640184 -0.639235   \n",
              "1  -0.638094 -0.635297  ... -0.641140 -0.641426 -0.639267 -0.637797 -0.637680   \n",
              "2  -0.762275 -0.763752  ... -0.722055 -0.718712 -0.713534 -0.710021 -0.704126   \n",
              "3  -0.745082 -0.745727  ... -0.721667 -0.724661 -0.729229 -0.728940 -0.727834   \n",
              "4  -0.587533 -0.585462  ... -0.644036 -0.643885 -0.645742 -0.646458 -0.646464   \n",
              "5  -0.575826 -0.577354  ... -0.706939 -0.704759 -0.703900 -0.704163 -0.705410   \n",
              "6  -1.296729 -1.284765  ... -1.260114 -1.269225 -1.274458 -1.278982 -1.280090   \n",
              "7  -2.012278 -2.014058  ...  0.426970  0.360089  0.274340  0.163402  0.019592   \n",
              "8  -1.067816 -1.067875  ... -1.051603 -1.046511 -1.048871 -1.046295 -1.046580   \n",
              "9  -1.167642 -1.166901  ... -1.225565 -1.295701 -1.327421 -1.327071 -1.300439   \n",
              "10 -0.811339 -0.812096  ... -1.027205 -1.028066 -1.021606 -1.021195 -1.024795   \n",
              "11 -1.113174 -1.115731  ... -1.186083 -1.154958 -1.130243 -1.118832 -1.122127   \n",
              "12 -1.338130 -1.306990  ... -0.991173 -0.995515 -0.996563 -0.998444 -1.001865   \n",
              "13 -1.168617 -1.163736  ... -1.091031 -1.165393 -1.272919 -1.343542 -1.337689   \n",
              "14 -0.586220 -0.583763  ... -0.596761 -0.594511 -0.595603 -0.603554 -0.612209   \n",
              "15 -1.161424 -1.158713  ... -1.127586 -1.136727 -1.137836 -1.132538 -1.129391   \n",
              "16 -0.794809 -0.791848  ... -0.695193 -0.695277 -0.690663 -0.690106 -0.685703   \n",
              "17 -0.606411 -0.606287  ... -0.586214 -0.595359 -0.598823 -0.603955 -0.609916   \n",
              "18 -1.047536 -1.047369  ... -1.082618 -1.071515 -1.063564 -1.066977 -1.071325   \n",
              "19 -1.083495 -1.083226  ... -1.147159 -1.146026 -1.145903 -1.147187 -1.145955   \n",
              "20 -0.834412 -0.834664  ... -0.883772 -0.881958 -0.871798 -0.861634 -0.845883   \n",
              "21 -0.685223 -0.671344  ... -1.047628 -1.025212 -1.014792 -1.006514 -1.003012   \n",
              "22 -1.139822 -1.132811  ... -1.032562 -1.112665 -1.182298 -1.252019 -1.319470   \n",
              "23 -1.722971 -1.718352  ... -0.815714 -1.051389 -1.275793 -1.470041 -1.639795   \n",
              "24 -0.708387 -0.708409  ... -0.552814 -0.558172 -0.562053 -0.567406 -0.569266   \n",
              "25 -0.977544 -0.979135  ... -0.922834 -0.924728 -0.922734 -0.924245 -0.923938   \n",
              "26 -1.083252 -1.074275  ... -1.269844 -1.239248 -1.200457 -1.170905 -1.158130   \n",
              "27 -1.078944 -1.077041  ... -1.201060 -1.202613 -1.203260 -1.200616 -1.196168   \n",
              "28 -1.012562 -1.017186  ... -1.114653 -1.114498 -1.117535 -1.119042 -1.120056   \n",
              "29 -1.408540 -1.375772  ... -1.134365 -1.141934 -1.150923 -1.155292 -1.157989   \n",
              "30 -0.961540 -0.960973  ... -0.896700 -0.893666 -0.891841 -0.893301 -0.891128   \n",
              "31 -1.480230 -1.481170  ... -1.367792 -1.362261 -1.363540 -1.364604 -1.368324   \n",
              "32 -0.979422 -0.980816  ... -0.971106 -0.971827 -0.970183 -0.965389 -0.965832   \n",
              "33 -0.738404 -0.740447  ... -0.757875 -0.756425 -0.755322 -0.757650 -0.760134   \n",
              "34 -0.628741 -0.625365  ... -0.623083 -0.619075 -0.618718 -0.614866 -0.614519   \n",
              "35 -0.618821 -0.619901  ... -0.646419 -0.647263 -0.646744 -0.645072 -0.643929   \n",
              "36 -1.302802 -1.299218  ... -1.198844 -1.197890 -1.197334 -1.197650 -1.197966   \n",
              "37 -0.562642 -0.564610  ... -0.671652 -0.665614 -0.659891 -0.656913 -0.654146   \n",
              "38 -1.621253 -1.622584  ... -1.403411 -1.549479 -1.660080 -1.745874 -1.797701   \n",
              "39 -1.117199 -1.118094  ... -1.174622 -1.173644 -1.174277 -1.175176 -1.176694   \n",
              "40 -0.693064 -0.693325  ... -0.712959 -0.711650 -0.711025 -0.711942 -0.708887   \n",
              "41 -1.143553 -1.145072  ... -0.943241 -0.999805 -1.074357 -1.160415 -1.230090   \n",
              "42 -0.964983 -0.964252  ... -1.013655 -1.015520 -1.014679 -1.011770 -1.006992   \n",
              "43 -1.026723 -1.033920  ... -1.233655 -1.221131 -1.180033 -1.121479 -1.083692   \n",
              "44 -1.038676 -1.039535  ... -1.149979 -1.147545 -1.149843 -1.151544 -1.152749   \n",
              "45 -0.563114 -0.564548  ... -0.723784 -0.725717 -0.726697 -0.725656 -0.726813   \n",
              "46 -0.608124 -0.606101  ... -0.776073 -0.784714 -0.796625 -0.804653 -0.800372   \n",
              "47 -0.762046 -0.758626  ... -0.512137 -0.512710 -0.510124 -0.511867 -0.511880   \n",
              "48 -0.701128 -0.702176  ... -0.640131 -0.640705 -0.639800 -0.639546 -0.637827   \n",
              "49 -1.432427 -1.433273  ... -1.477989 -1.461869 -1.447364 -1.438888 -1.437793   \n",
              "\n",
              "         146       147       148       149       150  \n",
              "0  -0.639395 -0.640231 -0.640429 -0.638666 -0.638657  \n",
              "1  -0.635260 -0.635490 -0.634934 -0.634497 -0.631596  \n",
              "2  -0.703263 -0.703393 -0.704196 -0.707605 -0.707120  \n",
              "3  -0.728244 -0.726453 -0.725517 -0.725191 -0.724679  \n",
              "4  -0.645585 -0.642412 -0.643337 -0.636803 -0.631716  \n",
              "5  -0.705741 -0.703861 -0.706541 -0.710381 -0.710854  \n",
              "6  -1.281349 -1.281277 -1.280616 -1.280212 -1.279940  \n",
              "7  -0.150113 -0.333859 -0.551477 -0.782467 -1.007992  \n",
              "8  -1.038032 -1.037612 -1.033426 -1.031462 -1.030165  \n",
              "9  -1.271138 -1.267283 -1.265006 -1.270722 -1.262134  \n",
              "10 -1.021716 -1.022889 -1.021380 -1.022272 -1.022983  \n",
              "11 -1.124086 -1.120237 -1.111846 -1.103578 -1.103998  \n",
              "12 -1.007205 -1.012854 -1.021305 -1.029152 -1.044706  \n",
              "13 -1.336491 -1.284089 -1.206961 -1.122139 -1.082205  \n",
              "14 -0.623519 -0.625307 -0.625411 -0.614697 -0.604779  \n",
              "15 -1.128199 -1.128224 -1.125779 -1.121353 -1.116188  \n",
              "16 -0.680779 -0.676761 -0.673248 -0.668439 -0.663304  \n",
              "17 -0.615390 -0.624009 -0.628865 -0.640098 -0.650745  \n",
              "18 -1.068112 -1.068966 -1.069154 -1.066642 -1.064798  \n",
              "19 -1.148524 -1.149558 -1.148339 -1.149799 -1.148310  \n",
              "20 -0.837268 -0.819175 -0.812857 -0.805104 -0.802605  \n",
              "21 -1.000901 -1.001019 -0.997393 -0.996212 -0.994838  \n",
              "22 -1.329326 -1.334852 -1.299284 -1.246920 -1.213891  \n",
              "23 -1.757432 -1.826076 -1.870268 -1.894537 -1.898637  \n",
              "24 -0.572434 -0.571733 -0.570443 -0.569006 -0.566393  \n",
              "25 -0.923793 -0.924096 -0.925417 -0.926572 -0.929978  \n",
              "26 -1.153122 -1.154176 -1.154564 -1.150073 -1.145040  \n",
              "27 -1.199188 -1.195577 -1.186975 -1.173823 -1.166574  \n",
              "28 -1.120553 -1.120983 -1.122364 -1.121879 -1.122235  \n",
              "29 -1.162612 -1.165037 -1.170046 -1.174333 -1.174406  \n",
              "30 -0.890467 -0.889976 -0.890854 -0.890339 -0.893748  \n",
              "31 -1.370659 -1.374070 -1.373879 -1.373785 -1.374736  \n",
              "32 -0.966595 -0.967407 -0.969254 -0.970416 -0.970535  \n",
              "33 -0.757973 -0.753264 -0.751725 -0.749770 -0.744602  \n",
              "34 -0.609214 -0.607610 -0.604666 -0.605630 -0.600113  \n",
              "35 -0.642893 -0.642514 -0.640783 -0.640529 -0.640211  \n",
              "36 -1.198309 -1.199803 -1.202034 -1.203569 -1.205487  \n",
              "37 -0.652727 -0.651819 -0.651962 -0.651519 -0.653748  \n",
              "38 -1.821457 -1.820755 -1.814826 -1.802348 -1.779259  \n",
              "39 -1.175051 -1.176095 -1.175167 -1.174964 -1.174871  \n",
              "40 -0.706704 -0.706233 -0.704465 -0.704567 -0.703899  \n",
              "41 -1.262929 -1.260485 -1.237534 -1.213591 -1.196872  \n",
              "42 -1.003574 -1.006509 -1.006429 -1.006552 -1.003153  \n",
              "43 -1.053213 -1.038400 -1.034783 -1.038866 -1.033090  \n",
              "44 -1.150443 -1.150210 -1.152899 -1.150172 -1.151864  \n",
              "45 -0.726768 -0.725525 -0.724171 -0.720889 -0.718832  \n",
              "46 -0.797827 -0.795079 -0.797045 -0.801276 -0.808361  \n",
              "47 -0.507179 -0.506964 -0.505006 -0.503731 -0.504385  \n",
              "48 -0.639201 -0.640260 -0.641134 -0.641406 -0.642109  \n",
              "49 -1.436885 -1.434493 -1.435462 -1.435282 -1.430884  \n",
              "\n",
              "[50 rows x 151 columns]"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainings[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9huTJ2gRdPAJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 GunPoint\n",
            "1 AllGestureWiimoteZ\n",
            "2 ShakeGestureWiimoteZ\n",
            "3 ProximalPhalanxTW\n",
            "4 ACSF1\n",
            "5 DiatomSizeReduction\n",
            "6 Mallat\n",
            "7 Chinatown\n",
            "8 SmoothSubspace\n",
            "9 LargeKitchenAppliances\n",
            "10 UWaveGestureLibraryY\n",
            "11 Trace\n",
            "12 FordB\n",
            "13 ECG5000\n",
            "14 SonyAIBORobotSurface2\n",
            "15 BME\n",
            "16 Earthquakes\n",
            "17 UWaveGestureLibraryZ\n",
            "18 MiddlePhalanxTW\n",
            "19 TwoPatterns\n",
            "20 PLAID\n",
            "21 Yoga\n",
            "22 PowerCons\n",
            "23 Fungi\n",
            "24 Worms\n",
            "25 GestureMidAirD2\n",
            "26 CBF\n",
            "27 Wafer\n",
            "28 HouseTwenty\n",
            "29 Lightning7\n",
            "30 StarLightCurves\n",
            "31 ElectricDevices\n",
            "32 PigArtPressure\n",
            "33 CricketX\n",
            "34 PigCVP\n",
            "35 FreezerSmallTrain\n",
            "36 GunPointMaleVersusFemale\n",
            "37 InsectEPGRegularTrain\n",
            "38 SyntheticControl\n",
            "39 UWaveGestureLibraryX\n",
            "40 ChlorineConcentration\n",
            "41 FordA\n",
            "42 ECG200\n",
            "43 SwedishLeaf\n",
            "44 HandOutlines\n",
            "45 FaceFour\n",
            "46 OliveOil\n",
            "47 Strawberry\n",
            "48 Car\n",
            "49 Coffee\n",
            "50 Computers\n",
            "51 Beef\n",
            "52 DodgerLoopDay\n",
            "53 ECGFiveDays\n",
            "54 ToeSegmentation2\n",
            "55 DistalPhalanxTW\n",
            "56 FiftyWords\n",
            "57 GunPointOldVersusYoung\n",
            "58 MixedShapesRegularTrain\n",
            "59 ScreenType\n",
            "60 EOGVerticalSignal\n",
            "61 MiddlePhalanxOutlineAgeGroup\n",
            "62 GunPointAgeSpan\n",
            "63 SemgHandSubjectCh2\n",
            "64 CricketZ\n",
            "65 EthanolLevel\n",
            "66 FacesUCR\n",
            "67 ProximalPhalanxOutlineAgeGroup\n",
            "68 WordSynonyms\n",
            "69 MiddlePhalanxOutlineCorrect\n",
            "70 CricketY\n",
            "71 ProximalPhalanxOutlineCorrect\n",
            "72 GestureMidAirD3\n",
            "73 SemgHandGenderCh2\n",
            "74 MelbournePedestrian\n",
            "75 UWaveGestureLibraryAll\n",
            "76 SemgHandMovementCh2\n",
            "77 FaceAll\n",
            "78 Rock\n",
            "79 Phoneme\n",
            "80 MoteStrain\n",
            "81 Herring\n",
            "82 PigAirwayPressure\n",
            "83 WormsTwoClass\n",
            "84 Wine\n",
            "85 AllGestureWiimoteY\n",
            "86 DistalPhalanxOutlineCorrect\n",
            "87 InlineSkate\n",
            "88 GestureMidAirD1\n",
            "89 RefrigerationDevices\n",
            "90 BeetleFly\n",
            "91 DodgerLoopGame\n",
            "92 ItalyPowerDemand\n",
            "93 BirdChicken\n",
            "94 ArrowHead\n",
            "95 InsectWingbeatSound\n",
            "96 SmallKitchenAppliances\n",
            "97 ShapesAll\n",
            "98 NonInvasiveFetalECGThorax2\n",
            "99 PickupGestureWiimoteZ\n",
            "100 NonInvasiveFetalECGThorax1\n",
            "101 Lightning2\n",
            "102 DistalPhalanxOutlineAgeGroup\n",
            "103 DodgerLoopWeekend\n",
            "104 InsectEPGSmallTrain\n",
            "105 MedicalImages\n",
            "106 EOGHorizontalSignal\n",
            "107 UMD\n",
            "108 OSULeaf\n",
            "109 CinCECGTorso\n",
            "110 Adiac\n",
            "111 Haptics\n",
            "112 MixedShapesSmallTrain\n",
            "113 Ham\n",
            "114 Symbols\n",
            "115 Fish\n",
            "116 AllGestureWiimoteX\n",
            "117 GesturePebbleZ2\n",
            "118 ShapeletSim\n",
            "119 Plane\n",
            "120 GesturePebbleZ1\n",
            "121 PhalangesOutlinesCorrect\n",
            "122 Crop\n",
            "123 Meat\n",
            "124 FreezerRegularTrain\n",
            "125 SonyAIBORobotSurface1\n",
            "126 TwoLeadECG\n",
            "127 ToeSegmentation1\n"
          ]
        }
      ],
      "source": [
        "idxs = []\n",
        "for i, cat in enumerate(catgs):\n",
        "  # if trainings[i].shape[1] > 100 and trainings[i].shape[0] > 100 and tests[i].shape[0] > 400: #sufficient data size and samples\n",
        "  print(i, cat)\n",
        "  idxs.append(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2Dp6Kkogdkf"
      },
      "source": [
        "# Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "opWnO-4sYvSw"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/onyxia/work/SoftDTWForEcon/.venv/lib/python3.12/site-packages/tslearn/bases/bases.py:15: UserWarning: h5py not installed, hdf5 features will not be supported.\n",
            "Install h5py to use hdf5 features: http://docs.h5py.org/\n",
            "  warn(h5py_msg)\n"
          ]
        }
      ],
      "source": [
        "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
        "import scipy as sp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_kbdw5WKf3jh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AllGestureWiimoteX has 112527 missing values in trainings\n",
            "AllGestureWiimoteX has 262736 missing values in tests\n",
            "--------------------------------\n",
            "Plane has no missing values in trainings\n",
            "Plane has no missing values in tests\n",
            "--------------------------------\n",
            "PickupGestureWiimoteZ has 10756 missing values in trainings\n",
            "PickupGestureWiimoteZ has 10773 missing values in tests\n",
            "--------------------------------\n",
            "MoteStrain has no missing values in trainings\n",
            "MoteStrain has no missing values in tests\n",
            "--------------------------------\n",
            "PowerCons has no missing values in trainings\n",
            "PowerCons has no missing values in tests\n",
            "--------------------------------\n",
            "Earthquakes has no missing values in trainings\n",
            "Earthquakes has no missing values in tests\n",
            "--------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Missing values detection\n",
        "\n",
        "idxs = [116, 119, 99, 80, 22, 16]\n",
        "\n",
        "# Here we look at which time series have missing values\n",
        "\n",
        "for i in idxs:\n",
        "  if trainings[i].isna().values.any():\n",
        "      print(catgs[i] + \" has \" + str(trainings[i].isna().values.sum()) + \" missing values in trainings\")\n",
        "      trainings[i].interpolate(method='linear', inplace=True)\n",
        "  else:\n",
        "      print(catgs[i] + \" has no missing values in trainings\")\n",
        "\n",
        "  if tests[i].isna().values.any():\n",
        "      print(catgs[i] + \" has \" + str(tests[i].isna().values.sum()) + \" missing values in tests\")\n",
        "      tests[i].interpolate(method='linear', inplace=True)\n",
        "  else:\n",
        "      print(catgs[i] + \" has no missing values in tests\")\n",
        "  print(\"--------------------------------\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OKVlAgZSMmIt"
      },
      "outputs": [],
      "source": [
        "# Checking for outliers\n",
        "\n",
        "def fig_ax(figsize=(15, 5)):\n",
        "    return plt.subplots(figsize=figsize)\n",
        "\n",
        "def outlier_detection(data):\n",
        "  quantile_threshold_low, quantile_threshold_high = 0.01, 0.997\n",
        "\n",
        "  fig, ax = fig_ax()\n",
        "  _ = ax.hist(data, 20)\n",
        "\n",
        "  threshold_low, threshold_high = np.quantile(\n",
        "      data, [quantile_threshold_low, quantile_threshold_high]\n",
        "  )\n",
        "\n",
        "  _ = ax.axvline(threshold_low, ls=\"--\", color=\"k\")\n",
        "  _ = ax.axvline(threshold_high, ls=\"--\", color=\"k\")\n",
        "\n",
        "  fig, ax = fig_ax()\n",
        "  ax.plot(data, \"*-\")\n",
        "\n",
        "  outlier_mask = (data < threshold_low) | (\n",
        "      data > threshold_high\n",
        "  )\n",
        "\n",
        "  ax.plot(\n",
        "      data[outlier_mask],\n",
        "      \"*\",\n",
        "      label=\"Outliers\",\n",
        "  )\n",
        "\n",
        "  plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "UKOXhhMqZSNh"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1500x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1500x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "outlier_detection(trainings[80][1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUBXuTNhmQj3"
      },
      "source": [
        "# Time series averaging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSCiu0rynKf_"
      },
      "source": [
        "Experimental setup:\n",
        "- in the original paper: DTW, SDTW, SHARP, MEANCOST with Euclidean mean initialization and $\\gamma = 1$. Divergences with their \"biased counterpart\" as initialization and $\\gamma = 10$.\n",
        "- here: all discrepancies are computed with $\\gamma$ in $\\{0.01, 0.1, 1, 10, 100\\}$. We keep the same initializations as in the original paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMKtImCQo16m"
      },
      "outputs": [],
      "source": [
        "from scipy.optimize import minimize\n",
        "from tslearn.barycenters import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_tWOAjPsXdJ"
      },
      "outputs": [],
      "source": [
        "idxs = [116, 119, 99, 80, 22, 16]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxNNslF6k5-1"
      },
      "outputs": [],
      "source": [
        "# DTW barycenters\n",
        "# We use the DBA algorithm to compute the barycenters\n",
        "\n",
        "dtw_barycenters = []\n",
        "\n",
        "for i in idxs:\n",
        "  eucl_mean = np.mean(trainings_array[i][:10], axis=0) # we average on 10 samples from the dataset\n",
        "  print(\"DTW barycenters on the \" + catgs[i] + \" dataset\")\n",
        "  b = dtw_barycenter_averaging(trainings_array[i][:10], init_barycenter=eucl_mean, max_iter=200)\n",
        "  dtw_barycenters.append(b)\n",
        "  plt.plot(b, linewidth=2)\n",
        "  for j in range(10):\n",
        "    plt.plot(trainings_array[i][j], alpha=0.1, color='black', linewidth=2)\n",
        "  plt.title(\"DTW\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWv_rTzsucYb"
      },
      "outputs": [],
      "source": [
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0MuuWtYudnz"
      },
      "outputs": [],
      "source": [
        "with open('dtw_barycenters.pickle', 'wb') as fp:\n",
        "    pickle.dump(dtw_barycenters, fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOU5TEhjq9hy"
      },
      "outputs": [],
      "source": [
        "gammas = [0.01, 0.1, 1., 10., 100.]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T192ubCEqeU9"
      },
      "outputs": [],
      "source": [
        "# SoftDTW barycenters\n",
        "\n",
        "sdtw_barycenters = [[], [], [], [], []]\n",
        "\n",
        "for i in idxs:\n",
        "  eucl_mean = np.mean(trainings_array[i][:10], axis=0)\n",
        "  print(\"Soft-DTW barycenters on the \" + catgs[i] + \" dataset\")\n",
        "  for g, gamma in enumerate(gammas):\n",
        "    color = sns.color_palette(\"flare\")[g]\n",
        "    func = SoftDTWValueAndGrad(gamma)\n",
        "    b = barycenter(trainings_array[i][:10], X_init=eucl_mean, value_and_grad=func)\n",
        "    sdtw_barycenters[g].append(b)\n",
        "    plt.plot(b, color=color, linewidth=2, label=\"$\\gamma = $\" + str(gamma))\n",
        "    for j in range(10):\n",
        "      plt.plot(trainings_array[i][j], alpha=0.07, color='black', linewidth=2)\n",
        "    plt.legend(prop={'size': 30})\n",
        "    plt.title(\"Soft-DTW\")\n",
        "    plt.show()\n",
        "    # save figure\n",
        "    plt.savefig('/content/drive/MyDrive/plots time series project/' + 'sdtw_' + str(gamma) + '_' + catgs[i] + '.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HBJngoUAkWc"
      },
      "outputs": [],
      "source": [
        "with open('sdtw_barycenters.pickle', 'wb') as fp:\n",
        "    pickle.dump(sdtw_barycenters, fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txIeuGo5r32d"
      },
      "outputs": [],
      "source": [
        "# Sharp barycenters\n",
        "\n",
        "sharp_barycenters = [[], [], [], [], []]\n",
        "\n",
        "for i in idxs:\n",
        "  eucl_mean = np.mean(trainings_array[i][:10], axis=0)\n",
        "  print(\"Sharp barycenters on the \" + catgs[i] + \" dataset\")\n",
        "  for g, gamma in enumerate(gammas):\n",
        "    color = sns.color_palette(\"flare\")[g]\n",
        "    func = SharpSoftDTWValueAndGrad(gamma)\n",
        "    b = barycenter(trainings_array[i][:10], X_init=eucl_mean, value_and_grad=func)\n",
        "    sharp_barycenters[g].append(b)\n",
        "    plt.plot(b, color=color, linewidth=2, label=\"$\\gamma = $\" + str(gamma))\n",
        "    for j in range(10):\n",
        "      plt.plot(trainings_array[i][j], alpha=0.1, color='black', linewidth=2)\n",
        "    plt.legend(prop={'size': 30})\n",
        "    plt.title(\"Sharp\")\n",
        "    plt.show()\n",
        "    # save figure\n",
        "    plt.savefig('/content/drive/MyDrive/plots time series project/' + 'sharp_' + str(gamma) + '_' + catgs[i] + '.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8H7wvFRAok9"
      },
      "outputs": [],
      "source": [
        "with open('sharp_barycenters.pickle', 'wb') as fp:\n",
        "    pickle.dump(sharp_barycenters, fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7oamzTwsith"
      },
      "outputs": [],
      "source": [
        "# Mean-cost barycenters\n",
        "\n",
        "mean_cost_barycenters = []\n",
        "\n",
        "for i in idxs:\n",
        "  eucl_mean = np.mean(trainings_array[i][:10], axis=0)\n",
        "  print(\"Mean-cost barycenters on the \" + catgs[i] + \" dataset\")\n",
        "  b = barycenter(trainings_array[i][:10], X_init=eucl_mean, value_and_grad=mean_cost_value_and_grad)\n",
        "  mean_cost_barycenters.append(b)\n",
        "  plt.plot(b, linewidth=2)\n",
        "  for j in range(10):\n",
        "    plt.plot(trainings_array[i][j], alpha=0.1, color='black', linewidth=2)\n",
        "  plt.title(\"Mean-cost\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhafWmPg6t-i"
      },
      "outputs": [],
      "source": [
        "with open('mean_cost_barycenters.pickle', 'wb') as fp:\n",
        "    pickle.dump(mean_cost_barycenters, fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fpfkexzntkzu"
      },
      "outputs": [],
      "source": [
        "# Soft-DTW divergence\n",
        "\n",
        "sdtw_div_barycenters = [[], [], [], [], []]\n",
        "\n",
        "for idx, i in enumerate(idxs):\n",
        "  print(\"Soft-DTW divergence barycenters on the \" + catgs[i] + \" dataset\")\n",
        "  for g, gamma in enumerate(gammas):\n",
        "    color = sns.color_palette(\"flare\")[g]\n",
        "    func = SoftDTWDivValueAndGrad(gamma)\n",
        "    b = barycenter(trainings_array[i][:10], X_init=sdtw_barycenters[g][idx], value_and_grad=func)\n",
        "    sdtw_div_barycenters[g].append(b)\n",
        "    plt.plot(b, color=color, linewidth=2, label=\"$\\gamma = $\" + str(gamma))\n",
        "    for j in range(10):\n",
        "      plt.plot(trainings_array[i][j], alpha=0.1, color='black', linewidth=2)\n",
        "    plt.legend(prop={'size': 30})\n",
        "    plt.title(\"Soft-DTW divergence\")\n",
        "    plt.show()\n",
        "    # save figure\n",
        "    plt.savefig('/content/drive/MyDrive/plots time series project/' + 'sdtw_div_' + str(gamma) + '_' + catgs[i] + '.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wB5O9B6nAsEB"
      },
      "outputs": [],
      "source": [
        "with open('sdtw_div_barycenters.pickle', 'wb') as fp:\n",
        "    pickle.dump(sdtw_div_barycenters, fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KimIfx2Et4vk"
      },
      "outputs": [],
      "source": [
        "# Sharp divergence\n",
        "\n",
        "sharp_div_barycenters = [[], [], [], [], []]\n",
        "\n",
        "for idx, i in enumerate(idxs):\n",
        "  print(\"Sharp divergence barycenters on the \" + catgs[i] + \" dataset\")\n",
        "  for g, gamma in enumerate(gammas):\n",
        "    color = sns.color_palette(\"flare\")[g]\n",
        "    func = SharpSoftDTWDivValueAndGrad(gamma)\n",
        "    b = barycenter(trainings_array[i][:10], X_init=sharp_barycenters[g][idx], value_and_grad=func)\n",
        "    sharp_div_barycenters[g].append(b)\n",
        "    plt.plot(b, color=color, linewidth=2, label=\"$\\gamma = $\" + str(gamma))\n",
        "    for j in range(10):\n",
        "      plt.plot(trainings_array[i][j], alpha=0.1, color='black', linewidth=2)\n",
        "    plt.legend(prop={'size': 30})\n",
        "    plt.title(\"Sharp divergence\")\n",
        "    plt.show()\n",
        "    # save figure\n",
        "    plt.savefig('/content/drive/MyDrive/plots time series project/' + 'sharp_div_' + str(gamma) + '_' + catgs[i] + '.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOvFdnL1AvG2"
      },
      "outputs": [],
      "source": [
        "with open('sharp_div_barycenters.pickle', 'wb') as fp:\n",
        "    pickle.dump(sharp_div_barycenters, fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xg0q1nuuDzD"
      },
      "outputs": [],
      "source": [
        "# Mean-cost divergence\n",
        "\n",
        "mean_cost_div_barycenters = []\n",
        "\n",
        "for idx, i in enumerate(idxs):\n",
        "  print(\"Mean-cost divergence barycenters on the \" + catgs[i] + \" dataset\")\n",
        "  b = barycenter(trainings_array[i][:10], X_init=mean_cost_barycenters[idx], value_and_grad=mean_cost_div_value_and_grad)\n",
        "  mean_cost_div_barycenters.append(b)\n",
        "  plt.plot(b, linewidth=2)\n",
        "  for j in range(10):\n",
        "    plt.plot(trainings_array[i][j], alpha=0.1, color='black', linewidth=2)\n",
        "  plt.title(\"Mean-cost divergence\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sr7DYbGWATIL"
      },
      "outputs": [],
      "source": [
        "with open('mean_cost_div_barycenters.pickle', 'wb') as fp:\n",
        "    pickle.dump(mean_cost_div_barycenters, fp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fS7HQJ_6kHa"
      },
      "source": [
        "### Quantitative results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWB5J3uz70c2"
      },
      "outputs": [],
      "source": [
        "from tslearn.metrics import dtw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UC2mnHYs6nfG"
      },
      "outputs": [],
      "source": [
        "for idx, i in enumerate(idxs):\n",
        "  print(\"distances for dataset \" + catgs[i])\n",
        "  to_avg = trainings_array[i][:10]\n",
        "  dtw_dists = np.zeros((len(to_avg), 1))\n",
        "  sdtw_dists = np.zeros((len(to_avg), len(gammas)))\n",
        "  sharp_dists = np.zeros((len(to_avg), len(gammas)))\n",
        "  mean_cost_dists = np.zeros((len(to_avg), 1))\n",
        "  sdtw_div_dists = np.zeros((len(to_avg), len(gammas)))\n",
        "  sharp_div_dists = np.zeros((len(to_avg), len(gammas)))\n",
        "  mean_cost_div_dists = np.zeros((len(to_avg), 1))\n",
        "  for s_idx, s in enumerate(to_avg):\n",
        "    dtw_dists[s_idx] = dtw(dtw_barycenters[idx], s)\n",
        "    mean_cost_dists[s_idx] = dtw(mean_cost_barycenters[idx], s)\n",
        "    mean_cost_div_dists[s_idx] = dtw(mean_cost_div_barycenters[idx], s)\n",
        "    for g_idx, g in enumerate(gammas):\n",
        "      sdtw_dists[s_idx][g_idx] = dtw(sdtw_barycenters[g_idx][idx], s)\n",
        "      sharp_dists[s_idx][g_idx] = dtw(sharp_barycenters[g_idx][idx], s)\n",
        "      sdtw_div_dists[s_idx][g_idx] = dtw(sdtw_div_barycenters[g_idx][idx], s)\n",
        "      sharp_div_dists[s_idx][g_idx] = dtw(sharp_div_barycenters[g_idx][idx], s)\n",
        "  dtw_dist = np.mean(dtw_dists, axis=0)\n",
        "  sdtw_dist = np.mean(sdtw_dists, axis=0)\n",
        "  sharp_dist = np.mean(sharp_dists, axis=0)\n",
        "  mean_cost_dist = np.mean(mean_cost_dists, axis=0)\n",
        "  mean_cost_div_dist = np.mean(mean_cost_div_dists, axis=0)\n",
        "  sdtw_div_dist = np.mean(sdtw_div_dists, axis=0)\n",
        "  sharp_div_dist = np.mean(sharp_div_dists, axis=0)\n",
        "  print(\"dtw: \", (dtw_dist))\n",
        "  print(\"sdtw: \", (sdtw_dist))\n",
        "  print(\"sharp: \", (sharp_dist))\n",
        "  print(\"mean_cost: \", (mean_cost_dist))\n",
        "  print(\"mean_cost_div: \", (mean_cost_div_dist))\n",
        "  print(\"sdtw_div: \", (sdtw_div_dist))\n",
        "  print(\"sharp_div: \", (sharp_div_dist))\n",
        "  print(\"-------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_9FyKTuKJ3N"
      },
      "source": [
        "# Time series clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kv2REXPOKOGw"
      },
      "source": [
        "We apply the various discrepancies presented in the paper to a task unexamined by the authors: clustering time series with the K-means algorithm.\n",
        "\n",
        "Experimental setup:\n",
        "- downsample time series to length 40\n",
        "- number of clusters equal to the number of classes in the data\n",
        "- gammas from $1e-4$ to $100$\n",
        "- max_iter for K-means equal to 30, max_iter for barycenter computation equal to 100\n",
        "- random initialization\n",
        "\n",
        "Qualitative comparisons: smoothness of clusters learned\n",
        "\n",
        "Quantitative comparisons: DTW distance from the cluster center to the time series for each cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A96ggynOoCbN"
      },
      "source": [
        "### Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UICcsPCkgXf5"
      },
      "outputs": [],
      "source": [
        "from tslearn.clustering import TimeSeriesKMeans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJ4DBvE-KNoU"
      },
      "outputs": [],
      "source": [
        "def make_data(i, samples=100, sz=40):\n",
        "  X = TimeSeriesResampler(sz=sz).fit_transform(trainings_array[i][:samples])\n",
        "  return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0wEYVzrf_5H"
      },
      "outputs": [],
      "source": [
        "def plot(X, y, n_clusters, km_object, sz=40, title=None, savefig=None):\n",
        "  for yi in range(n_clusters):\n",
        "    plt.subplot(n_clusters//2+1, 2, 1+yi)\n",
        "    for xx in X[y == yi]:\n",
        "      plt.plot(xx.squeeze(axis=-1), \"k-\", alpha=.2)\n",
        "    plt.plot(km_object.centroids[yi].ravel(), \"r-\")\n",
        "    plt.xlim(0, sz)\n",
        "    plt.ylim(-4, 4)\n",
        "    plt.title(\"Cluster %d\" % (yi + 1))\n",
        "  plt.tight_layout()\n",
        "  if title is not None:\n",
        "    plt.suptitle(title, y=1.05, size=16)\n",
        "  plt.show()\n",
        "  if savefig is not None:\n",
        "    plt.savefig(savefig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlJGlaFCgEJh"
      },
      "outputs": [],
      "source": [
        "# implement the K-means algorithm\n",
        "\n",
        "class KMeans():\n",
        "\n",
        "  def __init__(self, metric, n_clusters, init=None, max_iter_outer=100, max_iter_inner=30, tol=1e-5):\n",
        "\n",
        "    self.n_clusters = n_clusters\n",
        "    self.max_iter_outer = max_iter_outer\n",
        "    self.max_iter_inner = max_iter_inner\n",
        "    self.metric = metric\n",
        "    self.init = init\n",
        "    self.tol = tol\n",
        "\n",
        "  def fit(self, X_train):\n",
        "\n",
        "    if self.init is None: # Random initialization for the centroids\n",
        "      self.centroids = [X_train[np.random.choice(range(len(X_train)))] for _ in range(self.n_clusters)]\n",
        "    else:\n",
        "      self.centroids = self.init\n",
        "\n",
        "    # Iterate, adjusting centroids until inertia has converged or until passed max_iter\n",
        "    iteration = 0\n",
        "    prev_centroids = None\n",
        "    diff_inertias = np.inf\n",
        "    prev_inertia = np.inf\n",
        "    while diff_inertias > self.tol and iteration < self.max_iter_outer:\n",
        "      # Sort each datapoint, assigning to nearest centroid\n",
        "      sorted_points = [[self.centroids[i]] for i in range(self.n_clusters)] # each cluster contains at least its centroid\n",
        "      for x in X_train:\n",
        "        if self.metric == \"dtw_\":\n",
        "          dists = np.array([dtw(x, centroid) for centroid in self.centroids])\n",
        "        else:\n",
        "          dists = np.array([self.metric(x, centroid)[0] for centroid in self.centroids])\n",
        "        centroid_idx = np.argmin(dists)\n",
        "        sorted_points[centroid_idx].append(x)\n",
        "      # Update centroids\n",
        "      prev_centroids = self.centroids\n",
        "      if self.metric == \"dtw_\":\n",
        "        self.centroids = [dtw_barycenter_averaging(cluster, init_barycenter = np.mean(cluster, axis=0), max_iter=self.max_iter_inner) for cluster in sorted_points]\n",
        "      else:\n",
        "        self.centroids = [barycenter(cluster, X_init=np.mean(cluster, axis=0), value_and_grad=self.metric, max_iter=self.max_iter_inner) for cluster in sorted_points]\n",
        "      for i, centroid in enumerate(self.centroids):\n",
        "        if np.isnan(centroid).any():  # Catch any np.nans, resulting from an empty centroid\n",
        "          self.centroids[i] = prev_centroids[i]\n",
        "      # Compute mean inertia (distance to the nearest centroid)\n",
        "      inertia = 0\n",
        "      for i, centroid in enumerate(self.centroids):\n",
        "        inertia_cluster = 0\n",
        "        for x in sorted_points[i]:\n",
        "          if self.metric == \"dtw_\":\n",
        "            inertia_cluster += dtw(x, centroid)\n",
        "          else:\n",
        "            inertia_cluster += self.metric(x, centroid)[0]\n",
        "        inertia_cluster /= len(sorted_points[i])\n",
        "        inertia += inertia_cluster\n",
        "      inertia /= self.n_clusters\n",
        "      diff_inertias = np.abs(inertia - prev_inertia)\n",
        "      prev_inertia = inertia\n",
        "      iteration += 1\n",
        "\n",
        "  def evaluate(self, X):\n",
        "\n",
        "    centroids = []\n",
        "    centroid_idxs = []\n",
        "    for x in X:\n",
        "      if self.metric == \"dtw_\":\n",
        "        dists = np.array([dtw(x, centroid) for centroid in self.centroids])\n",
        "      else:\n",
        "        dists = np.array([self.metric(x, centroid)[0] for centroid in self.centroids])\n",
        "      centroid_idx = np.argmin(dists)\n",
        "      centroids.append(self.centroids[centroid_idx])\n",
        "      centroid_idxs.append(centroid_idx)\n",
        "    return centroids, centroid_idxs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiVnRlFLoGQ_"
      },
      "source": [
        "### Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Or2fScfGgCmP"
      },
      "outputs": [],
      "source": [
        "gammas = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_38zEipOvyR"
      },
      "outputs": [],
      "source": [
        "idxs = [116, 119, 99, 80, 22, 16]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyd0dEYev2oV"
      },
      "source": [
        "#### DTW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujhTfBeEv4ZA"
      },
      "outputs": [],
      "source": [
        "km_dtw = []\n",
        "\n",
        "for i in idxs:\n",
        "  print(\"KMeans for: \", catgs[i])\n",
        "  X = make_data(i)\n",
        "\n",
        "  metric = \"dtw_\"\n",
        "  nb_clusters = len(trainings[i][0].unique()) # number of classes in the dataset\n",
        "  km = KMeans(metric=metric, n_clusters=nb_clusters, max_iter_outer=30, max_iter_inner=100, init=None, tol=1e-5)\n",
        "  km.fit(X)\n",
        "  km_dtw.append(km)\n",
        "  centroids, centroid_idxs = km.evaluate(X)\n",
        "  plot(X, np.array(centroid_idxs), n_clusters=nb_clusters, km_object=km, title=\"DTW\", savefig='/content/drive/MyDrive/plots time series project/' + 'KM_DTW_' + catgs[i] + '.png')\n",
        "\n",
        "print(\"-------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TcUeG7K4wVLX"
      },
      "outputs": [],
      "source": [
        "print(\"DTW:\")\n",
        "km_dtw_dists = np.zeros((1, len(idxs)))\n",
        "\n",
        "for idx, i in enumerate(idxs):\n",
        "  print(\"DTW distances for: \" + catgs[i])\n",
        "  X = make_data(i)\n",
        "  centroids, centroid_idxs = km_dtw[0].evaluate(X)\n",
        "  dists = []\n",
        "  for c, centroid in enumerate(centroids):\n",
        "    xx = X[np.array(centroid_idxs) == c]\n",
        "    if len(xx) != 0:\n",
        "      dists.append(np.mean([dtw(x, centroid) for x in xx])) # mean along the samples axis\n",
        "  km_dtw_dists[0, idx] = np.mean(dists) # mean among the clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIA98YKUwVLY"
      },
      "outputs": [],
      "source": [
        "print(np.round(km_dtw_dists, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TU00wQf-oLN4"
      },
      "source": [
        "#### SoftDTW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRLkm1yyaUxJ"
      },
      "outputs": [],
      "source": [
        "km_sdtw = [[] for _ in range(len(gammas))]\n",
        "\n",
        "for i in idxs:\n",
        "  print(\"KMeans for: \", catgs[i])\n",
        "  X = make_data(i)\n",
        "\n",
        "  for g, gamma in enumerate(gammas):\n",
        "    print(\"gamma: \", gamma)\n",
        "    metric = SoftDTWValueAndGrad(gamma=gamma)\n",
        "    nb_clusters = len(trainings[i][0].unique()) # number of classes in the dataset\n",
        "    km = KMeans(metric=metric, n_clusters=nb_clusters, max_iter_outer=30, max_iter_inner=100, init=None, tol=1e-5)\n",
        "    km.fit(X)\n",
        "    km_sdtw[g].append(km)\n",
        "    centroids, centroid_idxs = km.evaluate(X)\n",
        "    plot(X, np.array(centroid_idxs), n_clusters=nb_clusters, km_object=km, title=\"SDTW, gamma: \" + str(gamma), savefig='/content/drive/MyDrive/plots time series project/' + 'KM_SDTW_' + str(gamma) + '_' + catgs[i] + '.png')\n",
        "\n",
        "  print(\"-------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJAeeU1viel4"
      },
      "outputs": [],
      "source": [
        "# compute mean DTW distance of time series to centroid for each cluster\n",
        "\n",
        "print(\"SoftDTW:\")\n",
        "\n",
        "km_sdtw_dists = np.zeros((len(gammas), len(idxs)))\n",
        "\n",
        "for idx, i in enumerate(idxs):\n",
        "  print(\"DTW distances for: \" + catgs[i])\n",
        "  X = make_data(i)\n",
        "  for g, gamma in enumerate(gammas):\n",
        "    centroids, centroid_idxs = km_sdtw[g][0].evaluate(X)\n",
        "    dists = []\n",
        "    for c, centroid in enumerate(centroids):\n",
        "      xx = X[np.array(centroid_idxs) == c]\n",
        "      if len(xx) != 0:\n",
        "        dists.append(np.mean([dtw(x, centroid) for x in xx])) # mean along the samples axis\n",
        "    km_sdtw_dists[g][idx] = np.mean(dists) # mean among the clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1Km9VuOmiun"
      },
      "outputs": [],
      "source": [
        "for g in range(len(gammas)):\n",
        "  print(\"gamma: \", gammas[g])\n",
        "  print(np.round(km_sdtw_dists[g, :], 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-FiOdkYoNMW"
      },
      "source": [
        "#### Sharp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8Wk0QSiab_G"
      },
      "outputs": [],
      "source": [
        "km_sharp = [[] for _ in range(len(gammas))]\n",
        "\n",
        "for i in idxs:\n",
        "  print(\"KMeans for: \", catgs[i])\n",
        "  X = make_data(i)\n",
        "\n",
        "  for g, gamma in enumerate(gammas):\n",
        "    print(\"gamma: \", gamma)\n",
        "    metric = SharpSoftDTWValueAndGrad(gamma=gamma)\n",
        "    nb_clusters = len(trainings[i][0].unique()) # number of classes in the dataset\n",
        "    km = KMeans(metric=metric, n_clusters=nb_clusters, max_iter_outer=30, max_iter_inner=100, init=None, tol=1e-5)\n",
        "    km.fit(X)\n",
        "    km_sharp[g].append(km)\n",
        "    centroids, centroid_idxs = km.evaluate(X)\n",
        "    plot(X, np.array(centroid_idxs), n_clusters=nb_clusters, km_object=km, title=\"Sharp, gamma: \" + str(gamma), savefig='/content/drive/MyDrive/plots time series project/' + 'KM_Sharp_' + str(gamma) + '_' + catgs[i] + '.png')\n",
        "\n",
        "  print(\"-------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGHD-xIElDdi"
      },
      "outputs": [],
      "source": [
        "print(\"Sharp:\")\n",
        "km_sharp_dists = np.zeros((len(gammas), len(idxs)))\n",
        "\n",
        "for idx, i in enumerate(idxs):\n",
        "  print(\"DTW distances for: \" + catgs[i])\n",
        "  X = make_data(i)\n",
        "  for g, gamma in enumerate(gammas):\n",
        "    centroids, centroid_idxs = km_sharp[g][0].evaluate(X)\n",
        "    dists = []\n",
        "    for c, centroid in enumerate(centroids):\n",
        "      xx = X[np.array(centroid_idxs) == c]\n",
        "      if len(xx) != 0:\n",
        "        dists.append(np.mean([dtw(x, centroid) for x in xx])) # mean along the samples axis\n",
        "    km_sharp_dists[g][idx] = np.mean(dists) # mean among the clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdBR_6p2mqfl"
      },
      "outputs": [],
      "source": [
        "for g in range(len(gammas)):\n",
        "  print(\"gamma: \", gammas[g])\n",
        "  print(np.round(km_sharp_dists[g, :], 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd_i8ZWioPDT"
      },
      "source": [
        "#### Sharp div"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4IDeQw_bwS6"
      },
      "outputs": [],
      "source": [
        "km_sharp_div = [[] for _ in range(len(gammas))]\n",
        "\n",
        "for i in idxs:\n",
        "  print(\"KMeans for: \", catgs[i])\n",
        "  X = make_data(i)\n",
        "\n",
        "  for g, gamma in enumerate(gammas):\n",
        "    print(\"gamma: \", gamma)\n",
        "    metric = SharpSoftDTWDivValueAndGrad(gamma=gamma)\n",
        "    nb_clusters = len(trainings[i][0].unique()) # number of classes in the dataset\n",
        "    km = KMeans(metric=metric, n_clusters=nb_clusters, max_iter_outer=30, max_iter_inner=100, init=None, tol=1e-5)\n",
        "    km.fit(X)\n",
        "    km_sharp_div[g].append(km)\n",
        "    centroids, centroid_idxs = km.evaluate(X)\n",
        "    plot(X, np.array(centroid_idxs), n_clusters=nb_clusters, km_object=km, title=\"Sharp DIV, gamma: \" + str(gamma), savefig='/content/drive/MyDrive/plots time series project/' + 'KM_Sharp_Div_' + str(gamma) + '_' + catgs[i] + '.png')\n",
        "\n",
        "  print(\"-------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxMdN1pslG0T"
      },
      "outputs": [],
      "source": [
        "print(\"Sharp Div:\")\n",
        "km_sharp_div_dists = np.zeros((len(gammas), len(idxs)))\n",
        "\n",
        "for idx, i in enumerate(idxs):\n",
        "  print(\"DTW distances for: \" + catgs[i])\n",
        "  X = make_data(i)\n",
        "  for g, gamma in enumerate(gammas):\n",
        "    centroids, centroid_idxs = km_sharp_div[g][0].evaluate(X)\n",
        "    dists = []\n",
        "    for c, centroid in enumerate(centroids):\n",
        "      xx = X[np.array(centroid_idxs) == c]\n",
        "      if len(xx) != 0:\n",
        "        dists.append(np.mean([dtw(x, centroid) for x in xx])) # mean along the samples axis\n",
        "    km_sharp_div_dists[g][idx] = np.mean(dists) # mean among the clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcticH72nMiP"
      },
      "outputs": [],
      "source": [
        "for g in range(len(gammas)):\n",
        "  print(\"gamma: \", gammas[g])\n",
        "  print(np.round(km_sharp_div_dists[g, :], 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkLnAOxaoQ4G"
      },
      "source": [
        "#### Mean Cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7lB-nKKgPnD"
      },
      "outputs": [],
      "source": [
        "km_mean_cost = []\n",
        "\n",
        "for i in idxs:\n",
        "  print(\"KMeans for: \", catgs[i])\n",
        "  X = make_data(i)\n",
        "\n",
        "  metric = mean_cost_value_and_grad\n",
        "  nb_clusters = len(trainings[i][0].unique()) # number of classes in the dataset\n",
        "  km = KMeans(metric=metric, n_clusters=nb_clusters, max_iter_outer=30, max_iter_inner=100, init=None, tol=1e-5)\n",
        "  km.fit(X)\n",
        "  km_mean_cost.append(km)\n",
        "  centroids, centroid_idxs = km.evaluate(X)\n",
        "  plot(X, np.array(centroid_idxs), n_clusters=nb_clusters, km_object=km, title=\"Mean cost\", savefig='/content/drive/MyDrive/plots time series project/' + 'KM_Meancost_' + catgs[i] + '.png')\n",
        "\n",
        "  print(\"-------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OD9fW_GfmW9-"
      },
      "outputs": [],
      "source": [
        "print(\"Mean Cost:\")\n",
        "km_meancost_dists = np.zeros((1, len(idxs)))\n",
        "\n",
        "for idx, i in enumerate(idxs):\n",
        "  print(\"DTW distances for: \" + catgs[i])\n",
        "  X = make_data(i)\n",
        "  centroids, centroid_idxs = km_mean_cost[0].evaluate(X)\n",
        "  dists = []\n",
        "  for c, centroid in enumerate(centroids):\n",
        "    xx = X[np.array(centroid_idxs) == c]\n",
        "    if len(xx) != 0:\n",
        "      dists.append(np.mean([dtw(x, centroid) for x in xx])) # mean along the samples axis\n",
        "  km_meancost_dists[0, idx] = np.mean(dists) # mean among the clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRk9fnMOnQIO"
      },
      "outputs": [],
      "source": [
        "print(np.round(km_meancost_dists, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNsX2FjHoTB9"
      },
      "source": [
        "#### Mean Cost Div"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8m5Q8TqgpdZ"
      },
      "outputs": [],
      "source": [
        "km_mean_cost_div = []\n",
        "\n",
        "for i in idxs:\n",
        "  print(\"KMeans for: \", catgs[i])\n",
        "  X = make_data(i)\n",
        "\n",
        "  metric = mean_cost_div_value_and_grad\n",
        "  nb_clusters = len(trainings[i][0].unique()) # number of classes in the dataset\n",
        "  km = KMeans(metric=metric, n_clusters=nb_clusters, max_iter_outer=30, max_iter_inner=100, init=None, tol=1e-5)\n",
        "  km.fit(X)\n",
        "  km_mean_cost_div.append(km)\n",
        "  centroids, centroid_idxs = km.evaluate(X)\n",
        "  plot(X, np.array(centroid_idxs), n_clusters=nb_clusters, km_object=km, title=\"Mean cost DIV\", savefig='/content/drive/MyDrive/plots time series project/' + 'KM_Meancost_Div_' + catgs[i] + '.png')\n",
        "\n",
        "  print(\"-------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awa3FfkZld9D"
      },
      "outputs": [],
      "source": [
        "print(\"Mean Cost Div:\")\n",
        "km_meancost_div_dists = np.zeros((1, len(idxs)))\n",
        "\n",
        "for idx, i in enumerate(idxs):\n",
        "  print(\"DTW distances for: \" + catgs[i])\n",
        "  X = make_data(i)\n",
        "  centroids, centroid_idxs = km_mean_cost_div[0].evaluate(X)\n",
        "  dists = []\n",
        "  for c, centroid in enumerate(centroids):\n",
        "    xx = X[np.array(centroid_idxs) == c]\n",
        "    if len(xx) != 0:\n",
        "      dists.append(np.mean([dtw(x, centroid) for x in xx])) # mean along the samples axis\n",
        "  km_meancost_div_dists[0, idx] = np.mean(dists) # mean among the clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKErbJ6mnwkd"
      },
      "outputs": [],
      "source": [
        "print(np.round(km_meancost_div_dists, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIfY4NvToU8t"
      },
      "source": [
        "#### SoftDTW Div"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fThhUt4xPJJ-"
      },
      "outputs": [],
      "source": [
        "km_sdtw_div = [[] for _ in range(len(gammas))]\n",
        "\n",
        "for i in idxs:\n",
        "  print(\"KMeans for: \", catgs[i])\n",
        "  X = make_data(i)\n",
        "\n",
        "  for g, gamma in enumerate(gammas):\n",
        "    print(\"gamma: \", gamma)\n",
        "    metric = SoftDTWDivValueAndGrad(gamma=gamma)\n",
        "    nb_clusters = len(trainings[i][0].unique()) # number of classes in the dataset\n",
        "    km = KMeans(metric=metric, n_clusters=nb_clusters, max_iter_outer=30, max_iter_inner=100, init=None, tol=1e-5)\n",
        "    km.fit(X)\n",
        "    km_sdtw_div[g].append(km)\n",
        "    centroids, centroid_idxs = km.evaluate(X)\n",
        "    plot(X, np.array(centroid_idxs), n_clusters=nb_clusters, km_object=km, title=\"SDTW Div, gamma: \" + str(gamma), savefig='/content/drive/MyDrive/plots time series project/' + 'KM_SDTW_Div_' + str(gamma) + '_' + catgs[i] + '.png')\n",
        "\n",
        "  print(\"-------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NO0vzQpolYeD"
      },
      "outputs": [],
      "source": [
        "print(\"SoftDTW Div:\")\n",
        "km_sdtw_div_dists = np.zeros((len(gammas), len(idxs)))\n",
        "\n",
        "for idx, i in enumerate(idxs):\n",
        "  print(\"DTW distances for: \" + catgs[i])\n",
        "  X = make_data(i)\n",
        "  for g, gamma in enumerate(gammas):\n",
        "    centroids, centroid_idxs = km_sdtw_div[g][0].evaluate(X)\n",
        "    dists = []\n",
        "    for c, centroid in enumerate(centroids):\n",
        "      xx = X[np.array(centroid_idxs) == c]\n",
        "      if len(xx) != 0:\n",
        "        dists.append(np.mean([dtw(x, centroid) for x in xx])) # mean along the samples axis\n",
        "    km_sdtw_div_dists[g][idx] = np.mean(dists) # mean among the clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zuWf72lsbPYa"
      },
      "outputs": [],
      "source": [
        "for g in range(len(gammas)):\n",
        "  print(\"gamma: \", gammas[g])\n",
        "  print(np.round(km_sdtw_div_dists[g, :], 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT8A1pKtKbvg"
      },
      "source": [
        "# Time series forecasting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evKAaJClKksY"
      },
      "source": [
        "We use the Soft DTW divergence as a loss function for a neural network in order to study its performance in comparison to the Soft DTW and the Mean Squared Error.\n",
        "\n",
        "Experimental setup:\n",
        "- For each dataset, we train 3  groups of models:\n",
        "  - Soft DTW for $\\gamma\\in \\{0.01, 0.1, 1,10,100\\}$\n",
        "  - Soft DTW Div for $\\gamma\\in \\{0.01, 0.1, 1,10,100\\}$\n",
        "  - Mean Squared Error (only one model in this group)\n",
        "- We use 60% of the dataset to predict the next 40%\n",
        "\n",
        "\n",
        "Qualitatively, we plot the obtained predictions and compare them to the ground truth.\n",
        "\n",
        "Quantitavively, we compute the DTW distance and the MSE between the predictions and the ground truth."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPHqBxuKOnNH"
      },
      "source": [
        "## Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "dmAncmfMK2FP"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tslearn'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Build MLP for time series prediction.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtslearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SoftDTWLossPyTorch\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tslearn'"
          ]
        }
      ],
      "source": [
        "# Build MLP for time series prediction.\n",
        "from tslearn.metrics import SoftDTWLossPyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "if torch.cuda.is_available():\n",
        " dev = \"cuda:0\"\n",
        "else:\n",
        " dev = \"cpu\"\n",
        "device = torch.device(dev)\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super(MLP, self).__init__()\n",
        "    self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "    self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    if not isinstance(x, torch.Tensor):\n",
        "            x = torch.Tensor(x)\n",
        "\n",
        "    batch_size = x.size(0)\n",
        "    x_reshaped = torch.reshape(x, (batch_size, -1))  # Manipulations to deal with time series format\n",
        "    output = F.sigmoid(self.fc1(x_reshaped))\n",
        "    output = self.fc2(output)\n",
        "    return torch.reshape(output, (batch_size, -1, 1))  # Manipulations to deal with time series format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "4we5XTf5O4or"
      },
      "outputs": [],
      "source": [
        "def train_models(cat, trainings, hidden_size=300, epochs=150, batch_size=50, lr=1e-2, gammas=[1e-2, 1e-1, 1, 10, 100], max_norm=100.,divergence=True):\n",
        "\n",
        "  i = cat\n",
        "\n",
        "  X_train = np.array(trainings[i])[:, 1:]\n",
        "  input_size = int(X_train.shape[1]*0.6)\n",
        "  output_size = X_train.shape[1] - input_size\n",
        "\n",
        "  x = torch.Tensor(X_train[:, :input_size]).unsqueeze(-1)\n",
        "  x = (x - x.mean(dim=0))/x.std(dim=0)\n",
        "  y = torch.Tensor(X_train[:, input_size:]).unsqueeze(-1)\n",
        "  y = (y - y.mean(dim=0))/y.std(dim=0)\n",
        "\n",
        "  X_val = np.array(tests[i])[:, 1:]\n",
        "  x_val = torch.Tensor(X_val[:, :input_size]).unsqueeze(-1)\n",
        "  x_val = (x_val - x_val.mean(dim=0))/x_val.std(dim=0)\n",
        "  x_val=x_val.to(device)\n",
        "  y_val = torch.Tensor(X_val[:, input_size:]).unsqueeze(-1)\n",
        "  y_val = (y_val - y_val.mean(dim=0))/y_val.std(dim=0)\n",
        "  y_val=y_val.to(device)\n",
        "\n",
        "  models = []\n",
        "  for gamma in gammas:\n",
        "\n",
        "    model = MLP(input_size=input_size, hidden_size=hidden_size, output_size=output_size).to(device)\n",
        "    loss_fn = SoftDTWLossPyTorch(gamma=gamma, normalize=divergence).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # task: predict next values given the first 60% of values\n",
        "\n",
        "    losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      shuffled_idxs = torch.randperm(x.size(0))\n",
        "      for batch_idx in range(0, x.size(0), batch_size):\n",
        "      # select batch\n",
        "        idxs = shuffled_idxs[batch_idx:batch_idx + batch_size]\n",
        "        x_batch = x[idxs].to(device)\n",
        "        y_batch = y[idxs].to(device)\n",
        "        pred = model(x_batch)\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(pred, y_batch).mean()\n",
        "        losses.append(loss.cpu().detach().numpy())\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
        "        optimizer.step()\n",
        "      if epoch % 10 == 0:\n",
        "        # validation loss\n",
        "        pred = model(x_val)\n",
        "        val_loss = loss_fn(pred, y_val).mean()\n",
        "        print(\"Epoch: {}, Train loss: {}, Validation loss: {}\".format(epoch, loss, val_loss))\n",
        "\n",
        "    plt.plot(np.array(losses))\n",
        "    plt.title(\"gamma = {}\".format(gamma))\n",
        "    plt.show()\n",
        "\n",
        "    models.append(model)\n",
        "\n",
        "  # add model with MSE loss\n",
        "\n",
        "  model = MLP(input_size=input_size, hidden_size=hidden_size, output_size=output_size).to(device)\n",
        "  loss_fn = nn.MSELoss().to(device)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "  losses = []\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "      shuffled_idxs = torch.randperm(x.size(0))\n",
        "      for batch_idx in range(0, x.size(0), batch_size):\n",
        "      # select batch\n",
        "        idxs = shuffled_idxs[batch_idx:batch_idx + batch_size]\n",
        "        x_batch = x[idxs].to(device)\n",
        "        y_batch = y[idxs].to(device)\n",
        "        pred = model(x_batch)\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(pred, y_batch).mean()\n",
        "        losses.append(loss.cpu().detach().numpy())\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
        "        optimizer.step()\n",
        "      if epoch % 10 == 0:\n",
        "        print(\"Epoch: {}, Loss: {}\".format(epoch, loss))\n",
        "\n",
        "  plt.plot(np.array(losses))\n",
        "  plt.title(\"MSE\")\n",
        "  plt.show()\n",
        "\n",
        "  models.append(model)\n",
        "\n",
        "  return models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_test_split(df,value,split_train,input_size,output_size):\n",
        "    values = df[value].values\n",
        "    split_idx = int(len(values)*split_train)\n",
        "    train_data,test_data=values[:split_idx],values[split_idx:]\n",
        "    X_train,y_train = create_time_series_window(train_data,input_size,output_size)\n",
        "    X_test,y_test=create_time_series_window(test_data,input_size,output_size)\n",
        "    return X_train,y_train,X_test,y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'df_activity' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m X_train,y_train,X_test,y_test=train_test_split(\u001b[43mdf_activity\u001b[49m,\u001b[33m\"\u001b[39m\u001b[33mOBS_VALUE\u001b[39m\u001b[33m\"\u001b[39m,\u001b[32m0.6\u001b[39m,input_size,output_size)    \n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(X_train)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(y_train)\n",
            "\u001b[31mNameError\u001b[39m: name 'df_activity' is not defined"
          ]
        }
      ],
      "source": [
        "X_train,y_train,X_test,y_test=train_test_split(df_activity,\"OBS_VALUE\",0.6,input_size,output_size)    \n",
        "print(X_train)\n",
        "print(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_models_insee(value,df, hidden_size=300, epochs=150, batch_size=50, lr=1e-2, gammas=[1e-2, 1e-1, 1, 10, 100], max_norm=100.,divergence=True):\n",
        "\n",
        "\n",
        "  X_train,y_train,X_test,y_test=train_test_split(df,value,0.6,input_size,output_size)    \n",
        "\n",
        "  x = torch.Tensor(np.array(X_train)).unsqueeze(-1)\n",
        "  x = (x - x.mean(dim=0))/x.std(dim=0)\n",
        "  y = torch.Tensor(np.array(y_train)).unsqueeze(-1)\n",
        "  y = (y - y.mean(dim=0))/y.std(dim=0)\n",
        "\n",
        "  X_val =x[-120:].to(device)\n",
        "  y_val =y[-120:].to(device)\n",
        "  X_tensor=x[:-120]\n",
        "  Y_tensor=y[:-120]\n",
        "  \n",
        " \n",
        "\n",
        "\n",
        "  models = []\n",
        "  for gamma in gammas:\n",
        "\n",
        "    model = MLP(input_size=input_size, hidden_size=hidden_size, output_size=output_size).to(device)\n",
        "    loss_fn = SoftDTWLossPyTorch(gamma=gamma, normalize=divergence).to(device)\n",
        "    #loss_fn = SoftDTWLoss(gamma=gamma)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # task: predict next values given the first 60% of values\n",
        "\n",
        "    losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      shuffled_idxs = torch.randperm(X_tensor.size(0))\n",
        "      for batch_idx in range(0, X_tensor.size(0), batch_size):\n",
        "      # select batch\n",
        "        idxs = shuffled_idxs[batch_idx:batch_idx + batch_size]\n",
        "        x_batch = X_tensor[idxs].to(device)\n",
        "        y_batch = Y_tensor[idxs].to(device)\n",
        "        pred = model(x_batch).to(device)\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(pred, y_batch).mean()\n",
        "        losses.append(loss.detach().cpu().numpy())\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
        "        optimizer.step()\n",
        "      if epoch % 10 == 0:\n",
        "        # validation loss\n",
        "        pred = model(X_val).to(device)\n",
        "        val_loss = loss_fn(pred, y_val).mean()\n",
        "        print(\"Epoch: {}, Train loss: {}, Validation loss: {}\".format(epoch, loss, val_loss))\n",
        "        \n",
        "\n",
        "    plt.plot(np.array(losses))\n",
        "    plt.title(\"gamma = {}\".format(gamma))\n",
        "    plt.show()\n",
        "\n",
        "    models.append(model)\n",
        "\n",
        "  # add model with MSE loss\n",
        "\n",
        "  model = MLP(input_size=input_size, hidden_size=hidden_size, output_size=output_size).to(device)\n",
        "  loss_fn = nn.MSELoss().to(device)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "  losses = []\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "      shuffled_idxs = torch.randperm(X_tensor.size(0))\n",
        "      for batch_idx in range(0, X_tensor.size(0), batch_size):\n",
        "      # select batch\n",
        "        idxs = shuffled_idxs[batch_idx:batch_idx + batch_size]\n",
        "        x_batch = X_tensor[idxs].to(device)\n",
        "        y_batch = Y_tensor[idxs].to(device)\n",
        "        pred = model(x_batch)\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(pred, y_batch).mean()\n",
        "        losses.append(loss.cpu().detach().numpy())\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
        "        optimizer.step()\n",
        "      if epoch % 10 == 0:\n",
        "        print(\"Epoch: {}, Loss: {}\".format(epoch, loss))\n",
        "\n",
        "  plt.plot(np.array(losses))\n",
        "  plt.title(\"MSE\")\n",
        "  plt.show()\n",
        "\n",
        "  models.append(model)\n",
        "\n",
        "  return models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "EP67Z1RtO7NT"
      },
      "outputs": [],
      "source": [
        "def eval_models_insee(models,value,df,input_size=20,output_size=5):\n",
        "  X_train,y_train,X_test,y_test = train_test_split(df,value,0.6,input_size,output_size)\n",
        "  x_test = torch.Tensor(np.array(X_test)).unsqueeze(-1).to(device)\n",
        "  x_test = (x_test - x_test.mean(dim=0))/x_test.std(dim=0)\n",
        "  print(x_test.shape)\n",
        "  input_size = int(x_test.shape[1])\n",
        "  print(input_size)\n",
        "  res = []\n",
        "  for m in range(len(models)):\n",
        "    result = models[m](x_test)\n",
        "    print(result.shape)\n",
        "    res.append(result)\n",
        "  return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def eval_models(models, cat, testings):  \n",
        "  X_test = np.array(testings[cat])[:, 1:]\n",
        "  input_size = int(X_test.shape[1]*0.6)\n",
        "  res = []\n",
        "  for m in range(len(models)):\n",
        "    result = models[m](torch.Tensor(X_test[:, :input_size]).unsqueeze(-1)).detach().numpy().squeeze()\n",
        "    # result = result * X_test[:, input_size:].std(1).reshape(-1, 1) + X_test[:, input_size:].mean(1).reshape(-1, 1)\n",
        "    res.append(result)\n",
        "  return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:21: SyntaxWarning: invalid escape sequence '\\g'\n",
            "<>:21: SyntaxWarning: invalid escape sequence '\\g'\n",
            "/tmp/ipykernel_8779/938507474.py:21: SyntaxWarning: invalid escape sequence '\\g'\n",
            "  plt.plot(np.arange(input_size, input_size + output_size), res[m][i].cpu().detach().squeeze(-1), color='red', label='$\\gamma$ = {}'.format(gammas[m]), alpha=0.6)\n",
            "/tmp/ipykernel_8779/938507474.py:21: SyntaxWarning: invalid escape sequence '\\g'\n",
            "  plt.plot(np.arange(input_size, input_size + output_size), res[m][i].cpu().detach().squeeze(-1), color='red', label='$\\gamma$ = {}'.format(gammas[m]), alpha=0.6)\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'seaborn'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mplot_forecasts_insee\u001b[39m(res, value,df,gammas,input_size=\u001b[32m20\u001b[39m,output_size=\u001b[32m5\u001b[39m):\n\u001b[32m      4\u001b[39m   X_train,y_train,X_test,y_test = train_test_split(df,value,\u001b[32m0.6\u001b[39m,input_size,output_size)\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'seaborn'"
          ]
        }
      ],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "def plot_forecasts_insee(res, value,df,gammas,input_size=20,output_size=5):\n",
        "  X_train,y_train,X_test,y_test = train_test_split(df,value,0.6,input_size,output_size)\n",
        "  X_test = np.array(X_test)\n",
        "  input_size = int(X_test.shape[1])\n",
        "  output_size = int(y_test.shape[1])\n",
        "\n",
        "\n",
        "  X_val = (X_test - X_test.mean(axis=0))/X_test.std(axis=0)\n",
        "  y_test= (y_test-y_test.mean(axis=0))/y_test.std(axis=0)\n",
        "  gt = X_test\n",
        "  print(gt.shape)\n",
        "  print(X_test.shape[0])\n",
        "  for i in range(0, 10):\n",
        "    for m in range(len(res)):\n",
        "      color = sns.color_palette(\"magma\")[m%6]\n",
        "      if m < len(res)-1:\n",
        "        #plt.plot(np.arange(input_size), gt[i],color='grey', label='Ground truth')\n",
        "        plt.plot(np.arange(input_size,input_size+output_size), y_test[i],color='grey', label='Ground truth')\n",
        "        plt.plot(np.arange(input_size, input_size + output_size), res[m][i].cpu().detach().squeeze(-1), color='red', label='$\\gamma$ = {}'.format(gammas[m]), alpha=0.6)\n",
        "        #plt.plot(np.arange(input_size, input_size + output_size), res[-1][i].cpu().detach().squeeze(-1),color='green', label='MSE', alpha=0.6)\n",
        "        plt.axvline(x = input_size, linestyle = 'dashed', color = 'k')\n",
        "        plt.title(\"{}\".format(gammas[m]))\n",
        "        plt.legend()\n",
        "        plt.grid()\n",
        "        plt.show()\n",
        "      else:\n",
        "        #plt.plot(np.arange(input_size, input_size + output_size), res[m][i].cpu().detach().squeeze(-1),color='green', label='MSE', alpha=0.6)\n",
        "       # plt.axvline(x = input_size, linestyle = 'dashed', color = 'k')\n",
        "       # plt.grid()\n",
        "\n",
        "        plt.title(\"{}\".format(i))\n",
        "        plt.legend()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "DsZoIST0lP8K"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:17: SyntaxWarning: invalid escape sequence '\\g'\n",
            "<>:17: SyntaxWarning: invalid escape sequence '\\g'\n",
            "/tmp/ipykernel_8779/2729970876.py:17: SyntaxWarning: invalid escape sequence '\\g'\n",
            "  plt.plot(np.arange(input_size, input_size + output_size -1), res[m][i], color='red', label='$\\gamma$ = {}'.format(gammas[m]), alpha=0.6)\n",
            "/tmp/ipykernel_8779/2729970876.py:17: SyntaxWarning: invalid escape sequence '\\g'\n",
            "  plt.plot(np.arange(input_size, input_size + output_size -1), res[m][i], color='red', label='$\\gamma$ = {}'.format(gammas[m]), alpha=0.6)\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'seaborn'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mplot_forecasts\u001b[39m(res, cat, test,gammas):\n\u001b[32m      4\u001b[39m   input_size = \u001b[38;5;28mint\u001b[39m(np.array(test[cat])[:,\u001b[32m1\u001b[39m:].shape[\u001b[32m1\u001b[39m]*\u001b[32m0.6\u001b[39m)\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'seaborn'"
          ]
        }
      ],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "def plot_forecasts(res, cat, test,gammas):\n",
        "  input_size = int(np.array(test[cat])[:,1:].shape[1]*0.6)\n",
        "  output_size = test[cat].shape[1]- input_size\n",
        "\n",
        "  X_val = np.array(test[cat])[:, 1:]\n",
        "  X_val = (X_val - X_val.mean(axis=0))/X_val.std(axis=0)\n",
        "  gt = X_val\n",
        "  print(gt.shape)\n",
        "  print(test[cat].shape[0])\n",
        "  for i in range(0, 10):\n",
        "    for m in range(len(res)):\n",
        "      color = sns.color_palette(\"magma\")[m%6]\n",
        "      if m < len(res)-1:\n",
        "        plo11##t.plot(np.arange(input_size + output_size - 1), gt[i],color='grey', label='Ground truth')\n",
        "        plt.plot(np.arange(input_size, input_size + output_size -1), res[m][i], color='red', label='$\\gamma$ = {}'.format(gammas[m]), alpha=0.6)\n",
        "        plt.plot(np.arange(input_size, input_size + output_size -1), res[-1][i],color='green', label='MSE', alpha=0.6)\n",
        "        plt.axvline(x = input_size, linestyle = 'dashed', color = 'k')\n",
        "        plt.title(\"{}\".format(gammas[m]))\n",
        "        plt.legend()\n",
        "        plt.grid()\n",
        "        plt.show()\n",
        "      else:\n",
        "        plt.plot(np.arange(input_size, input_size + output_size -1), res[m][i],color='green', label='MSE', alpha=0.6)\n",
        "        plt.axvline(x = input_size, linestyle = 'dashed', color = 'k')\n",
        "        plt.grid()\n",
        "\n",
        "        plt.title(\"{}\".format(i))\n",
        "        plt.legend()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "sKGATX4ToMlX"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tslearn'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtslearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m soft_dtw, SoftDTWLossPyTorch, dtw\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34merror\u001b[39m(res, cat, test):\n\u001b[32m      5\u001b[39m   input_size = \u001b[38;5;28mint\u001b[39m(np.array(test[cat])[:,\u001b[32m1\u001b[39m:].shape[\u001b[32m1\u001b[39m]*\u001b[32m0.6\u001b[39m)\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tslearn'"
          ]
        }
      ],
      "source": [
        "from tslearn.metrics import soft_dtw, SoftDTWLossPyTorch, dtw\n",
        "\n",
        "\n",
        "def error(res, cat, test):\n",
        "  input_size = int(np.array(test[cat])[:,1:].shape[1]*0.6)\n",
        "  gt = np.array(test[cat])[:, 1:][np.newaxis, :, input_size:]\n",
        "  gt = (gt - gt.mean(axis=1, keepdims=True))/gt.std(axis=1, keepdims=True)\n",
        "  res = np.array(res)\n",
        "\n",
        "  # MSE\n",
        "  mse = np.mean((gt - res)**2, axis=2)\n",
        "  std_mse = np.std(mse, axis=1)\n",
        "  mse = np.mean(mse, axis=1)\n",
        "\n",
        "  # DTW\n",
        "  dtw_models = np.zeros((len(res), gt.shape[1]))\n",
        "  for m in range(len(res)):\n",
        "    for ts in range(gt.shape[1]):\n",
        "      dist = dtw(gt[0, ts], res[m][ts])\n",
        "      dtw_models[m][ts] = dist\n",
        "  std_dtw = np.std(dtw_models, axis=1)\n",
        "  dtws = np.mean(dtw_models, axis=1)\n",
        "  print(\"For \" + catgs[cat])\n",
        "  print(\"MSE: {} +- {}\".format(np.round(mse,2), np.round(std_mse,2)))\n",
        "  print(\"DTW: {} +- {}\".format(np.round(dtws,2), np.round(std_dtw,2)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tslearn'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtslearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m soft_dtw, SoftDTWLossPyTorch, dtw\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34merror_insee\u001b[39m(res, value,df,input_size=\u001b[32m20\u001b[39m,output_size=\u001b[32m5\u001b[39m):\n\u001b[32m      5\u001b[39m   X_train,y_train,X_test,y_test = train_test_split(df,value,\u001b[32m0.6\u001b[39m,input_size,output_size)\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tslearn'"
          ]
        }
      ],
      "source": [
        "from tslearn.metrics import soft_dtw, SoftDTWLossPyTorch, dtw\n",
        "\n",
        "\n",
        "def error_insee(res, value,df,input_size=20,output_size=5):\n",
        "  X_train,y_train,X_test,y_test = train_test_split(df,value,0.6,input_size,output_size)\n",
        "  y_test = np.array(y_test)\n",
        "  input_size = int(y_test.shape[1])\n",
        "  output_size = int(y_test.shape[1])\n",
        "  gt = y_test\n",
        "  gt = (gt - gt.mean(axis=1, keepdims=True))/gt.std(axis=1, keepdims=True)\n",
        "  res = np.array([r.cpu().detach().numpy() if isinstance(r, torch.Tensor) else r for r in res])\n",
        "\n",
        "\n",
        "  # MSE\n",
        "  mse = np.mean((gt - res[0].squeeze(-1))**2, axis=1)\n",
        "  std_mse = np.std((gt - res[0].squeeze(-1))**2)\n",
        "  mse = np.mean(mse)\n",
        "\n",
        "  # DTW\n",
        "  dtw_models = np.zeros((len(res), gt.shape[1]))\n",
        "  for m in range(len(res)):\n",
        "    for ts in range(gt.shape[1]):\n",
        "      dist = dtw(gt[0, ts], res[m][ts])\n",
        "      dtw_models[m][ts] = dist\n",
        "  std_dtw = np.std(dtw_models, axis=1)\n",
        "  dtws = np.mean(dtw_models, axis=1)\n",
        "  print(\"MSE: {} +- {}\".format(np.round(mse,2), np.round(std_mse,2)))\n",
        "  print(\"DTW: {} +- {}\".format(np.round(dtws,2), np.round(std_dtw,2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OL7tpGCPiu7"
      },
      "source": [
        "## Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "h_rBjd_MPHxv"
      },
      "outputs": [],
      "source": [
        "#Indexes of the categories you wish to study\n",
        "# idxs = [116, 119, 22, 16]\n",
        "idxs = [116]\n",
        "\n",
        "#Values of gamma you wish to study\n",
        "gammas = [1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTkMdqy0PODf"
      },
      "source": [
        "### Soft DTW Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "d0I15FZSPQhj"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'trainings' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m forecast_models=[]\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idxs:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m   forecast_models.append(train_models(i, \u001b[43mtrainings\u001b[49m, gammas=gammas,divergence=\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# This cell takes a while to run depending on the number of datasets that you train on\u001b[39;00m\n",
            "\u001b[31mNameError\u001b[39m: name 'trainings' is not defined"
          ]
        }
      ],
      "source": [
        "forecast_models=[]\n",
        "for i in idxs:\n",
        "  forecast_models.append(train_models(i, trainings, gammas=gammas,divergence=False))\n",
        "\n",
        "# This cell takes a while to run depending on the number of datasets that you train on"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'df_activity' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m models = train_models_insee(\u001b[33m\"\u001b[39m\u001b[33mOBS_VALUE\u001b[39m\u001b[33m\"\u001b[39m,\u001b[43mdf_activity\u001b[49m, gammas=gammas,divergence=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "\u001b[31mNameError\u001b[39m: name 'df_activity' is not defined"
          ]
        }
      ],
      "source": [
        "models = train_models_insee(\"OBS_VALUE\",df_activity, gammas=gammas,divergence=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "Fphn1L_UPRIf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20\n",
            "torch.Size([100, 5, 1])\n",
            "torch.Size([100, 5, 1])\n",
            "100\n"
          ]
        }
      ],
      "source": [
        "\n",
        "results = eval_models_insee(models,\"OBS_VALUE\",df_activity)\n",
        "print(len(results[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MSE: 1.82 +- 2.29\n",
            "DTW: [2.48 2.52] +- [2.2  2.19]\n"
          ]
        }
      ],
      "source": [
        "#plot_forecasts_insee(results, \"OBS_VALUE\",df_activity,gammas)\n",
        "error_insee(results, \"OBS_VALUE\",df_activity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDdSuGclpJEV"
      },
      "outputs": [],
      "source": [
        "for i in range(len(idxs)):\n",
        "  plot_forecasts(results[i], idxs[i], tests,gammas)\n",
        "  error(results[i], idxs[i], tests)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fb2u_YakPRkq"
      },
      "source": [
        "### Soft DTW Divergence Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbJ85nH3PV5u"
      },
      "outputs": [],
      "source": [
        "forecast_models=[]\n",
        "for i in idxs:\n",
        "  forecast_models.append(train_models(i, trainings, gammas=gammas, divergence=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2BFhh6XjovL"
      },
      "outputs": [],
      "source": [
        "results=[]\n",
        "for i,idx in enumerate(idxs):\n",
        "  results.append(eval_models(forecast_models[i],idx,tests))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n25Pt_YrpRXG"
      },
      "outputs": [],
      "source": [
        "for i in range(len(idxs)):\n",
        "  plot_forecasts(results[i], idxs[i], tests,gammas)\n",
        "  error(results[i], idxs[i], tests)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "-rnKo-COcSoR",
        "7YvGtxrFcvrs",
        "S-YG5K3wdP_Z",
        "y2Dp6Kkogdkf",
        "4fS7HQJ_6kHa",
        "nyd0dEYev2oV",
        "TU00wQf-oLN4",
        "dkLnAOxaoQ4G",
        "yNsX2FjHoTB9",
        "PIfY4NvToU8t",
        "qT8A1pKtKbvg"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
